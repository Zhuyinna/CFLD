[2024-03-19 10:43:40 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 10:43:40 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 10:43:48 pose_transfer_train.py:298] preparing model...
[2024-03-19 10:43:51 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 10:44:03 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 10:44:03 pose_transfer_train.py:344] preparing optimizer...
[2024-03-19 10:44:03 pose_transfer_train.py:350] preparing accelerator...
[2024-03-19 10:44:06 pose_transfer_train.py:357] loading states from ./checkpoints/
[2024-03-19 10:46:14 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 10:46:14 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 10:46:23 pose_transfer_train.py:298] preparing model...
[2024-03-19 10:46:26 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 10:46:39 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 10:46:39 pose_transfer_train.py:344] preparing optimizer...
[2024-03-19 10:46:39 pose_transfer_train.py:350] preparing accelerator...
[2024-03-19 10:46:41 pose_transfer_train.py:357] loading states from ./checkpoints/
[2024-03-19 10:49:05 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 10:49:05 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 10:49:13 pose_transfer_train.py:298] preparing model...
[2024-03-19 10:49:16 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 10:49:28 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 10:49:28 pose_transfer_train.py:344] preparing optimizer...
[2024-03-19 10:49:28 pose_transfer_train.py:350] preparing accelerator...
[2024-03-19 10:49:31 pose_transfer_train.py:357] loading states from ./checkpoints/
[2024-03-19 10:49:31 pose_transfer_train.py:367] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 10:49:33 pose_transfer_train.py:370] _IncompatibleKeys(missing_keys=['module.model.conv_in.weight', 'module.model.conv_in.bias', 'module.model.time_embedding.linear_1.weight', 'module.model.time_embedding.linear_1.bias', 'module.model.time_embedding.linear_2.weight', 'module.model.time_embedding.linear_2.bias', 'module.model.down_blocks.0.attentions.0.norm.weight', 'module.model.down_blocks.0.attentions.0.norm.bias', 'module.model.down_blocks.0.attentions.0.proj_in.weight', 'module.model.down_blocks.0.attentions.0.proj_in.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.0.attentions.0.proj_out.weight', 'module.model.down_blocks.0.attentions.0.proj_out.bias', 'module.model.down_blocks.0.attentions.1.norm.weight', 'module.model.down_blocks.0.attentions.1.norm.bias', 'module.model.down_blocks.0.attentions.1.proj_in.weight', 'module.model.down_blocks.0.attentions.1.proj_in.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.0.attentions.1.proj_out.weight', 'module.model.down_blocks.0.attentions.1.proj_out.bias', 'module.model.down_blocks.0.resnets.0.norm1.weight', 'module.model.down_blocks.0.resnets.0.norm1.bias', 'module.model.down_blocks.0.resnets.0.conv1.weight', 'module.model.down_blocks.0.resnets.0.conv1.bias', 'module.model.down_blocks.0.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.0.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.0.resnets.0.norm2.weight', 'module.model.down_blocks.0.resnets.0.norm2.bias', 'module.model.down_blocks.0.resnets.0.conv2.weight', 'module.model.down_blocks.0.resnets.0.conv2.bias', 'module.model.down_blocks.0.resnets.1.norm1.weight', 'module.model.down_blocks.0.resnets.1.norm1.bias', 'module.model.down_blocks.0.resnets.1.conv1.weight', 'module.model.down_blocks.0.resnets.1.conv1.bias', 'module.model.down_blocks.0.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.0.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.0.resnets.1.norm2.weight', 'module.model.down_blocks.0.resnets.1.norm2.bias', 'module.model.down_blocks.0.resnets.1.conv2.weight', 'module.model.down_blocks.0.resnets.1.conv2.bias', 'module.model.down_blocks.0.downsamplers.0.conv.weight', 'module.model.down_blocks.0.downsamplers.0.conv.bias', 'module.model.down_blocks.1.attentions.0.norm.weight', 'module.model.down_blocks.1.attentions.0.norm.bias', 'module.model.down_blocks.1.attentions.0.proj_in.weight', 'module.model.down_blocks.1.attentions.0.proj_in.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.1.attentions.0.proj_out.weight', 'module.model.down_blocks.1.attentions.0.proj_out.bias', 'module.model.down_blocks.1.attentions.1.norm.weight', 'module.model.down_blocks.1.attentions.1.norm.bias', 'module.model.down_blocks.1.attentions.1.proj_in.weight', 'module.model.down_blocks.1.attentions.1.proj_in.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.1.attentions.1.proj_out.weight', 'module.model.down_blocks.1.attentions.1.proj_out.bias', 'module.model.down_blocks.1.resnets.0.norm1.weight', 'module.model.down_blocks.1.resnets.0.norm1.bias', 'module.model.down_blocks.1.resnets.0.conv1.weight', 'module.model.down_blocks.1.resnets.0.conv1.bias', 'module.model.down_blocks.1.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.1.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.1.resnets.0.norm2.weight', 'module.model.down_blocks.1.resnets.0.norm2.bias', 'module.model.down_blocks.1.resnets.0.conv2.weight', 'module.model.down_blocks.1.resnets.0.conv2.bias', 'module.model.down_blocks.1.resnets.0.conv_shortcut.weight', 'module.model.down_blocks.1.resnets.0.conv_shortcut.bias', 'module.model.down_blocks.1.resnets.1.norm1.weight', 'module.model.down_blocks.1.resnets.1.norm1.bias', 'module.model.down_blocks.1.resnets.1.conv1.weight', 'module.model.down_blocks.1.resnets.1.conv1.bias', 'module.model.down_blocks.1.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.1.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.1.resnets.1.norm2.weight', 'module.model.down_blocks.1.resnets.1.norm2.bias', 'module.model.down_blocks.1.resnets.1.conv2.weight', 'module.model.down_blocks.1.resnets.1.conv2.bias', 'module.model.down_blocks.1.downsamplers.0.conv.weight', 'module.model.down_blocks.1.downsamplers.0.conv.bias', 'module.model.down_blocks.2.attentions.0.norm.weight', 'module.model.down_blocks.2.attentions.0.norm.bias', 'module.model.down_blocks.2.attentions.0.proj_in.weight', 'module.model.down_blocks.2.attentions.0.proj_in.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.2.attentions.0.proj_out.weight', 'module.model.down_blocks.2.attentions.0.proj_out.bias', 'module.model.down_blocks.2.attentions.1.norm.weight', 'module.model.down_blocks.2.attentions.1.norm.bias', 'module.model.down_blocks.2.attentions.1.proj_in.weight', 'module.model.down_blocks.2.attentions.1.proj_in.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.2.attentions.1.proj_out.weight', 'module.model.down_blocks.2.attentions.1.proj_out.bias', 'module.model.down_blocks.2.resnets.0.norm1.weight', 'module.model.down_blocks.2.resnets.0.norm1.bias', 'module.model.down_blocks.2.resnets.0.conv1.weight', 'module.model.down_blocks.2.resnets.0.conv1.bias', 'module.model.down_blocks.2.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.2.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.2.resnets.0.norm2.weight', 'module.model.down_blocks.2.resnets.0.norm2.bias', 'module.model.down_blocks.2.resnets.0.conv2.weight', 'module.model.down_blocks.2.resnets.0.conv2.bias', 'module.model.down_blocks.2.resnets.0.conv_shortcut.weight', 'module.model.down_blocks.2.resnets.0.conv_shortcut.bias', 'module.model.down_blocks.2.resnets.1.norm1.weight', 'module.model.down_blocks.2.resnets.1.norm1.bias', 'module.model.down_blocks.2.resnets.1.conv1.weight', 'module.model.down_blocks.2.resnets.1.conv1.bias', 'module.model.down_blocks.2.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.2.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.2.resnets.1.norm2.weight', 'module.model.down_blocks.2.resnets.1.norm2.bias', 'module.model.down_blocks.2.resnets.1.conv2.weight', 'module.model.down_blocks.2.resnets.1.conv2.bias', 'module.model.down_blocks.2.downsamplers.0.conv.weight', 'module.model.down_blocks.2.downsamplers.0.conv.bias', 'module.model.down_blocks.3.resnets.0.norm1.weight', 'module.model.down_blocks.3.resnets.0.norm1.bias', 'module.model.down_blocks.3.resnets.0.conv1.weight', 'module.model.down_blocks.3.resnets.0.conv1.bias', 'module.model.down_blocks.3.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.3.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.3.resnets.0.norm2.weight', 'module.model.down_blocks.3.resnets.0.norm2.bias', 'module.model.down_blocks.3.resnets.0.conv2.weight', 'module.model.down_blocks.3.resnets.0.conv2.bias', 'module.model.down_blocks.3.resnets.1.norm1.weight', 'module.model.down_blocks.3.resnets.1.norm1.bias', 'module.model.down_blocks.3.resnets.1.conv1.weight', 'module.model.down_blocks.3.resnets.1.conv1.bias', 'module.model.down_blocks.3.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.3.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.3.resnets.1.norm2.weight', 'module.model.down_blocks.3.resnets.1.norm2.bias', 'module.model.down_blocks.3.resnets.1.conv2.weight', 'module.model.down_blocks.3.resnets.1.conv2.bias', 'module.model.up_blocks.0.resnets.0.norm1.weight', 'module.model.up_blocks.0.resnets.0.norm1.bias', 'module.model.up_blocks.0.resnets.0.conv1.weight', 'module.model.up_blocks.0.resnets.0.conv1.bias', 'module.model.up_blocks.0.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.0.norm2.weight', 'module.model.up_blocks.0.resnets.0.norm2.bias', 'module.model.up_blocks.0.resnets.0.conv2.weight', 'module.model.up_blocks.0.resnets.0.conv2.bias', 'module.model.up_blocks.0.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.0.resnets.1.norm1.weight', 'module.model.up_blocks.0.resnets.1.norm1.bias', 'module.model.up_blocks.0.resnets.1.conv1.weight', 'module.model.up_blocks.0.resnets.1.conv1.bias', 'module.model.up_blocks.0.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.1.norm2.weight', 'module.model.up_blocks.0.resnets.1.norm2.bias', 'module.model.up_blocks.0.resnets.1.conv2.weight', 'module.model.up_blocks.0.resnets.1.conv2.bias', 'module.model.up_blocks.0.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.0.resnets.2.norm1.weight', 'module.model.up_blocks.0.resnets.2.norm1.bias', 'module.model.up_blocks.0.resnets.2.conv1.weight', 'module.model.up_blocks.0.resnets.2.conv1.bias', 'module.model.up_blocks.0.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.2.norm2.weight', 'module.model.up_blocks.0.resnets.2.norm2.bias', 'module.model.up_blocks.0.resnets.2.conv2.weight', 'module.model.up_blocks.0.resnets.2.conv2.bias', 'module.model.up_blocks.0.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.0.upsamplers.0.conv.weight', 'module.model.up_blocks.0.upsamplers.0.conv.bias', 'module.model.up_blocks.1.attentions.0.norm.weight', 'module.model.up_blocks.1.attentions.0.norm.bias', 'module.model.up_blocks.1.attentions.0.proj_in.weight', 'module.model.up_blocks.1.attentions.0.proj_in.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.0.proj_out.weight', 'module.model.up_blocks.1.attentions.0.proj_out.bias', 'module.model.up_blocks.1.attentions.1.norm.weight', 'module.model.up_blocks.1.attentions.1.norm.bias', 'module.model.up_blocks.1.attentions.1.proj_in.weight', 'module.model.up_blocks.1.attentions.1.proj_in.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.1.proj_out.weight', 'module.model.up_blocks.1.attentions.1.proj_out.bias', 'module.model.up_blocks.1.attentions.2.norm.weight', 'module.model.up_blocks.1.attentions.2.norm.bias', 'module.model.up_blocks.1.attentions.2.proj_in.weight', 'module.model.up_blocks.1.attentions.2.proj_in.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.2.proj_out.weight', 'module.model.up_blocks.1.attentions.2.proj_out.bias', 'module.model.up_blocks.1.resnets.0.norm1.weight', 'module.model.up_blocks.1.resnets.0.norm1.bias', 'module.model.up_blocks.1.resnets.0.conv1.weight', 'module.model.up_blocks.1.resnets.0.conv1.bias', 'module.model.up_blocks.1.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.0.norm2.weight', 'module.model.up_blocks.1.resnets.0.norm2.bias', 'module.model.up_blocks.1.resnets.0.conv2.weight', 'module.model.up_blocks.1.resnets.0.conv2.bias', 'module.model.up_blocks.1.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.1.resnets.1.norm1.weight', 'module.model.up_blocks.1.resnets.1.norm1.bias', 'module.model.up_blocks.1.resnets.1.conv1.weight', 'module.model.up_blocks.1.resnets.1.conv1.bias', 'module.model.up_blocks.1.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.1.norm2.weight', 'module.model.up_blocks.1.resnets.1.norm2.bias', 'module.model.up_blocks.1.resnets.1.conv2.weight', 'module.model.up_blocks.1.resnets.1.conv2.bias', 'module.model.up_blocks.1.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.1.resnets.2.norm1.weight', 'module.model.up_blocks.1.resnets.2.norm1.bias', 'module.model.up_blocks.1.resnets.2.conv1.weight', 'module.model.up_blocks.1.resnets.2.conv1.bias', 'module.model.up_blocks.1.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.2.norm2.weight', 'module.model.up_blocks.1.resnets.2.norm2.bias', 'module.model.up_blocks.1.resnets.2.conv2.weight', 'module.model.up_blocks.1.resnets.2.conv2.bias', 'module.model.up_blocks.1.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.1.warpflows.0.norm.weight', 'module.model.up_blocks.1.warpflows.0.norm.bias', 'module.model.up_blocks.1.warpflows.0.proj_in.weight', 'module.model.up_blocks.1.warpflows.0.proj_in.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.0.proj_out.weight', 'module.model.up_blocks.1.warpflows.0.proj_out.bias', 'module.model.up_blocks.1.warpflows.1.norm.weight', 'module.model.up_blocks.1.warpflows.1.norm.bias', 'module.model.up_blocks.1.warpflows.1.proj_in.weight', 'module.model.up_blocks.1.warpflows.1.proj_in.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.1.proj_out.weight', 'module.model.up_blocks.1.warpflows.1.proj_out.bias', 'module.model.up_blocks.1.warpflows.2.norm.weight', 'module.model.up_blocks.1.warpflows.2.norm.bias', 'module.model.up_blocks.1.warpflows.2.proj_in.weight', 'module.model.up_blocks.1.warpflows.2.proj_in.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.2.proj_out.weight', 'module.model.up_blocks.1.warpflows.2.proj_out.bias', 'module.model.up_blocks.1.warpzc.0.weight', 'module.model.up_blocks.1.warpzc.0.bias', 'module.model.up_blocks.1.warpzc.1.weight', 'module.model.up_blocks.1.warpzc.1.bias', 'module.model.up_blocks.1.warpzc.2.weight', 'module.model.up_blocks.1.warpzc.2.bias', 'module.model.up_blocks.1.upsamplers.0.conv.weight', 'module.model.up_blocks.1.upsamplers.0.conv.bias', 'module.model.up_blocks.2.attentions.0.norm.weight', 'module.model.up_blocks.2.attentions.0.norm.bias', 'module.model.up_blocks.2.attentions.0.proj_in.weight', 'module.model.up_blocks.2.attentions.0.proj_in.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.0.proj_out.weight', 'module.model.up_blocks.2.attentions.0.proj_out.bias', 'module.model.up_blocks.2.attentions.1.norm.weight', 'module.model.up_blocks.2.attentions.1.norm.bias', 'module.model.up_blocks.2.attentions.1.proj_in.weight', 'module.model.up_blocks.2.attentions.1.proj_in.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.1.proj_out.weight', 'module.model.up_blocks.2.attentions.1.proj_out.bias', 'module.model.up_blocks.2.attentions.2.norm.weight', 'module.model.up_blocks.2.attentions.2.norm.bias', 'module.model.up_blocks.2.attentions.2.proj_in.weight', 'module.model.up_blocks.2.attentions.2.proj_in.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.2.proj_out.weight', 'module.model.up_blocks.2.attentions.2.proj_out.bias', 'module.model.up_blocks.2.resnets.0.norm1.weight', 'module.model.up_blocks.2.resnets.0.norm1.bias', 'module.model.up_blocks.2.resnets.0.conv1.weight', 'module.model.up_blocks.2.resnets.0.conv1.bias', 'module.model.up_blocks.2.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.0.norm2.weight', 'module.model.up_blocks.2.resnets.0.norm2.bias', 'module.model.up_blocks.2.resnets.0.conv2.weight', 'module.model.up_blocks.2.resnets.0.conv2.bias', 'module.model.up_blocks.2.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.2.resnets.1.norm1.weight', 'module.model.up_blocks.2.resnets.1.norm1.bias', 'module.model.up_blocks.2.resnets.1.conv1.weight', 'module.model.up_blocks.2.resnets.1.conv1.bias', 'module.model.up_blocks.2.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.1.norm2.weight', 'module.model.up_blocks.2.resnets.1.norm2.bias', 'module.model.up_blocks.2.resnets.1.conv2.weight', 'module.model.up_blocks.2.resnets.1.conv2.bias', 'module.model.up_blocks.2.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.2.resnets.2.norm1.weight', 'module.model.up_blocks.2.resnets.2.norm1.bias', 'module.model.up_blocks.2.resnets.2.conv1.weight', 'module.model.up_blocks.2.resnets.2.conv1.bias', 'module.model.up_blocks.2.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.2.norm2.weight', 'module.model.up_blocks.2.resnets.2.norm2.bias', 'module.model.up_blocks.2.resnets.2.conv2.weight', 'module.model.up_blocks.2.resnets.2.conv2.bias', 'module.model.up_blocks.2.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.2.warpflows.0.norm.weight', 'module.model.up_blocks.2.warpflows.0.norm.bias', 'module.model.up_blocks.2.warpflows.0.proj_in.weight', 'module.model.up_blocks.2.warpflows.0.proj_in.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.0.proj_out.weight', 'module.model.up_blocks.2.warpflows.0.proj_out.bias', 'module.model.up_blocks.2.warpflows.1.norm.weight', 'module.model.up_blocks.2.warpflows.1.norm.bias', 'module.model.up_blocks.2.warpflows.1.proj_in.weight', 'module.model.up_blocks.2.warpflows.1.proj_in.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.1.proj_out.weight', 'module.model.up_blocks.2.warpflows.1.proj_out.bias', 'module.model.up_blocks.2.warpflows.2.norm.weight', 'module.model.up_blocks.2.warpflows.2.norm.bias', 'module.model.up_blocks.2.warpflows.2.proj_in.weight', 'module.model.up_blocks.2.warpflows.2.proj_in.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.2.proj_out.weight', 'module.model.up_blocks.2.warpflows.2.proj_out.bias', 'module.model.up_blocks.2.warpzc.0.weight', 'module.model.up_blocks.2.warpzc.0.bias', 'module.model.up_blocks.2.warpzc.1.weight', 'module.model.up_blocks.2.warpzc.1.bias', 'module.model.up_blocks.2.warpzc.2.weight', 'module.model.up_blocks.2.warpzc.2.bias', 'module.model.up_blocks.2.upsamplers.0.conv.weight', 'module.model.up_blocks.2.upsamplers.0.conv.bias', 'module.model.up_blocks.3.attentions.0.norm.weight', 'module.model.up_blocks.3.attentions.0.norm.bias', 'module.model.up_blocks.3.attentions.0.proj_in.weight', 'module.model.up_blocks.3.attentions.0.proj_in.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.0.proj_out.weight', 'module.model.up_blocks.3.attentions.0.proj_out.bias', 'module.model.up_blocks.3.attentions.1.norm.weight', 'module.model.up_blocks.3.attentions.1.norm.bias', 'module.model.up_blocks.3.attentions.1.proj_in.weight', 'module.model.up_blocks.3.attentions.1.proj_in.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.1.proj_out.weight', 'module.model.up_blocks.3.attentions.1.proj_out.bias', 'module.model.up_blocks.3.attentions.2.norm.weight', 'module.model.up_blocks.3.attentions.2.norm.bias', 'module.model.up_blocks.3.attentions.2.proj_in.weight', 'module.model.up_blocks.3.attentions.2.proj_in.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.2.proj_out.weight', 'module.model.up_blocks.3.attentions.2.proj_out.bias', 'module.model.up_blocks.3.resnets.0.norm1.weight', 'module.model.up_blocks.3.resnets.0.norm1.bias', 'module.model.up_blocks.3.resnets.0.conv1.weight', 'module.model.up_blocks.3.resnets.0.conv1.bias', 'module.model.up_blocks.3.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.0.norm2.weight', 'module.model.up_blocks.3.resnets.0.norm2.bias', 'module.model.up_blocks.3.resnets.0.conv2.weight', 'module.model.up_blocks.3.resnets.0.conv2.bias', 'module.model.up_blocks.3.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.3.resnets.1.norm1.weight', 'module.model.up_blocks.3.resnets.1.norm1.bias', 'module.model.up_blocks.3.resnets.1.conv1.weight', 'module.model.up_blocks.3.resnets.1.conv1.bias', 'module.model.up_blocks.3.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.1.norm2.weight', 'module.model.up_blocks.3.resnets.1.norm2.bias', 'module.model.up_blocks.3.resnets.1.conv2.weight', 'module.model.up_blocks.3.resnets.1.conv2.bias', 'module.model.up_blocks.3.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.3.resnets.2.norm1.weight', 'module.model.up_blocks.3.resnets.2.norm1.bias', 'module.model.up_blocks.3.resnets.2.conv1.weight', 'module.model.up_blocks.3.resnets.2.conv1.bias', 'module.model.up_blocks.3.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.2.norm2.weight', 'module.model.up_blocks.3.resnets.2.norm2.bias', 'module.model.up_blocks.3.resnets.2.conv2.weight', 'module.model.up_blocks.3.resnets.2.conv2.bias', 'module.model.up_blocks.3.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.3.warpflows.0.norm.weight', 'module.model.up_blocks.3.warpflows.0.norm.bias', 'module.model.up_blocks.3.warpflows.0.proj_in.weight', 'module.model.up_blocks.3.warpflows.0.proj_in.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.0.proj_out.weight', 'module.model.up_blocks.3.warpflows.0.proj_out.bias', 'module.model.up_blocks.3.warpflows.1.norm.weight', 'module.model.up_blocks.3.warpflows.1.norm.bias', 'module.model.up_blocks.3.warpflows.1.proj_in.weight', 'module.model.up_blocks.3.warpflows.1.proj_in.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.1.proj_out.weight', 'module.model.up_blocks.3.warpflows.1.proj_out.bias', 'module.model.up_blocks.3.warpflows.2.norm.weight', 'module.model.up_blocks.3.warpflows.2.norm.bias', 'module.model.up_blocks.3.warpflows.2.proj_in.weight', 'module.model.up_blocks.3.warpflows.2.proj_in.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.2.proj_out.weight', 'module.model.up_blocks.3.warpflows.2.proj_out.bias', 'module.model.up_blocks.3.warpzc.0.weight', 'module.model.up_blocks.3.warpzc.0.bias', 'module.model.up_blocks.3.warpzc.1.weight', 'module.model.up_blocks.3.warpzc.1.bias', 'module.model.up_blocks.3.warpzc.2.weight', 'module.model.up_blocks.3.warpzc.2.bias', 'module.model.mid_block.attentions.0.norm.weight', 'module.model.mid_block.attentions.0.norm.bias', 'module.model.mid_block.attentions.0.proj_in.weight', 'module.model.mid_block.attentions.0.proj_in.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.mid_block.attentions.0.proj_out.weight', 'module.model.mid_block.attentions.0.proj_out.bias', 'module.model.mid_block.resnets.0.norm1.weight', 'module.model.mid_block.resnets.0.norm1.bias', 'module.model.mid_block.resnets.0.conv1.weight', 'module.model.mid_block.resnets.0.conv1.bias', 'module.model.mid_block.resnets.0.time_emb_proj.weight', 'module.model.mid_block.resnets.0.time_emb_proj.bias', 'module.model.mid_block.resnets.0.norm2.weight', 'module.model.mid_block.resnets.0.norm2.bias', 'module.model.mid_block.resnets.0.conv2.weight', 'module.model.mid_block.resnets.0.conv2.bias', 'module.model.mid_block.resnets.1.norm1.weight', 'module.model.mid_block.resnets.1.norm1.bias', 'module.model.mid_block.resnets.1.conv1.weight', 'module.model.mid_block.resnets.1.conv1.bias', 'module.model.mid_block.resnets.1.time_emb_proj.weight', 'module.model.mid_block.resnets.1.time_emb_proj.bias', 'module.model.mid_block.resnets.1.norm2.weight', 'module.model.mid_block.resnets.1.norm2.bias', 'module.model.mid_block.resnets.1.conv2.weight', 'module.model.mid_block.resnets.1.conv2.bias', 'module.model.conv_norm_out.weight', 'module.model.conv_norm_out.bias', 'module.model.conv_out.weight', 'module.model.conv_out.bias'], unexpected_keys=['model.conv_in.weight', 'model.conv_in.bias', 'model.time_embedding.linear_1.weight', 'model.time_embedding.linear_1.bias', 'model.time_embedding.linear_2.weight', 'model.time_embedding.linear_2.bias', 'model.down_blocks.0.attentions.0.norm.weight', 'model.down_blocks.0.attentions.0.norm.bias', 'model.down_blocks.0.attentions.0.proj_in.weight', 'model.down_blocks.0.attentions.0.proj_in.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.0.attentions.0.proj_out.weight', 'model.down_blocks.0.attentions.0.proj_out.bias', 'model.down_blocks.0.attentions.1.norm.weight', 'model.down_blocks.0.attentions.1.norm.bias', 'model.down_blocks.0.attentions.1.proj_in.weight', 'model.down_blocks.0.attentions.1.proj_in.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.0.attentions.1.proj_out.weight', 'model.down_blocks.0.attentions.1.proj_out.bias', 'model.down_blocks.0.resnets.0.norm1.weight', 'model.down_blocks.0.resnets.0.norm1.bias', 'model.down_blocks.0.resnets.0.conv1.weight', 'model.down_blocks.0.resnets.0.conv1.bias', 'model.down_blocks.0.resnets.0.time_emb_proj.weight', 'model.down_blocks.0.resnets.0.time_emb_proj.bias', 'model.down_blocks.0.resnets.0.norm2.weight', 'model.down_blocks.0.resnets.0.norm2.bias', 'model.down_blocks.0.resnets.0.conv2.weight', 'model.down_blocks.0.resnets.0.conv2.bias', 'model.down_blocks.0.resnets.1.norm1.weight', 'model.down_blocks.0.resnets.1.norm1.bias', 'model.down_blocks.0.resnets.1.conv1.weight', 'model.down_blocks.0.resnets.1.conv1.bias', 'model.down_blocks.0.resnets.1.time_emb_proj.weight', 'model.down_blocks.0.resnets.1.time_emb_proj.bias', 'model.down_blocks.0.resnets.1.norm2.weight', 'model.down_blocks.0.resnets.1.norm2.bias', 'model.down_blocks.0.resnets.1.conv2.weight', 'model.down_blocks.0.resnets.1.conv2.bias', 'model.down_blocks.0.downsamplers.0.conv.weight', 'model.down_blocks.0.downsamplers.0.conv.bias', 'model.down_blocks.1.attentions.0.norm.weight', 'model.down_blocks.1.attentions.0.norm.bias', 'model.down_blocks.1.attentions.0.proj_in.weight', 'model.down_blocks.1.attentions.0.proj_in.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.1.attentions.0.proj_out.weight', 'model.down_blocks.1.attentions.0.proj_out.bias', 'model.down_blocks.1.attentions.1.norm.weight', 'model.down_blocks.1.attentions.1.norm.bias', 'model.down_blocks.1.attentions.1.proj_in.weight', 'model.down_blocks.1.attentions.1.proj_in.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.1.attentions.1.proj_out.weight', 'model.down_blocks.1.attentions.1.proj_out.bias', 'model.down_blocks.1.resnets.0.norm1.weight', 'model.down_blocks.1.resnets.0.norm1.bias', 'model.down_blocks.1.resnets.0.conv1.weight', 'model.down_blocks.1.resnets.0.conv1.bias', 'model.down_blocks.1.resnets.0.time_emb_proj.weight', 'model.down_blocks.1.resnets.0.time_emb_proj.bias', 'model.down_blocks.1.resnets.0.norm2.weight', 'model.down_blocks.1.resnets.0.norm2.bias', 'model.down_blocks.1.resnets.0.conv2.weight', 'model.down_blocks.1.resnets.0.conv2.bias', 'model.down_blocks.1.resnets.0.conv_shortcut.weight', 'model.down_blocks.1.resnets.0.conv_shortcut.bias', 'model.down_blocks.1.resnets.1.norm1.weight', 'model.down_blocks.1.resnets.1.norm1.bias', 'model.down_blocks.1.resnets.1.conv1.weight', 'model.down_blocks.1.resnets.1.conv1.bias', 'model.down_blocks.1.resnets.1.time_emb_proj.weight', 'model.down_blocks.1.resnets.1.time_emb_proj.bias', 'model.down_blocks.1.resnets.1.norm2.weight', 'model.down_blocks.1.resnets.1.norm2.bias', 'model.down_blocks.1.resnets.1.conv2.weight', 'model.down_blocks.1.resnets.1.conv2.bias', 'model.down_blocks.1.downsamplers.0.conv.weight', 'model.down_blocks.1.downsamplers.0.conv.bias', 'model.down_blocks.2.attentions.0.norm.weight', 'model.down_blocks.2.attentions.0.norm.bias', 'model.down_blocks.2.attentions.0.proj_in.weight', 'model.down_blocks.2.attentions.0.proj_in.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.2.attentions.0.proj_out.weight', 'model.down_blocks.2.attentions.0.proj_out.bias', 'model.down_blocks.2.attentions.1.norm.weight', 'model.down_blocks.2.attentions.1.norm.bias', 'model.down_blocks.2.attentions.1.proj_in.weight', 'model.down_blocks.2.attentions.1.proj_in.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.2.attentions.1.proj_out.weight', 'model.down_blocks.2.attentions.1.proj_out.bias', 'model.down_blocks.2.resnets.0.norm1.weight', 'model.down_blocks.2.resnets.0.norm1.bias', 'model.down_blocks.2.resnets.0.conv1.weight', 'model.down_blocks.2.resnets.0.conv1.bias', 'model.down_blocks.2.resnets.0.time_emb_proj.weight', 'model.down_blocks.2.resnets.0.time_emb_proj.bias', 'model.down_blocks.2.resnets.0.norm2.weight', 'model.down_blocks.2.resnets.0.norm2.bias', 'model.down_blocks.2.resnets.0.conv2.weight', 'model.down_blocks.2.resnets.0.conv2.bias', 'model.down_blocks.2.resnets.0.conv_shortcut.weight', 'model.down_blocks.2.resnets.0.conv_shortcut.bias', 'model.down_blocks.2.resnets.1.norm1.weight', 'model.down_blocks.2.resnets.1.norm1.bias', 'model.down_blocks.2.resnets.1.conv1.weight', 'model.down_blocks.2.resnets.1.conv1.bias', 'model.down_blocks.2.resnets.1.time_emb_proj.weight', 'model.down_blocks.2.resnets.1.time_emb_proj.bias', 'model.down_blocks.2.resnets.1.norm2.weight', 'model.down_blocks.2.resnets.1.norm2.bias', 'model.down_blocks.2.resnets.1.conv2.weight', 'model.down_blocks.2.resnets.1.conv2.bias', 'model.down_blocks.2.downsamplers.0.conv.weight', 'model.down_blocks.2.downsamplers.0.conv.bias', 'model.down_blocks.3.resnets.0.norm1.weight', 'model.down_blocks.3.resnets.0.norm1.bias', 'model.down_blocks.3.resnets.0.conv1.weight', 'model.down_blocks.3.resnets.0.conv1.bias', 'model.down_blocks.3.resnets.0.time_emb_proj.weight', 'model.down_blocks.3.resnets.0.time_emb_proj.bias', 'model.down_blocks.3.resnets.0.norm2.weight', 'model.down_blocks.3.resnets.0.norm2.bias', 'model.down_blocks.3.resnets.0.conv2.weight', 'model.down_blocks.3.resnets.0.conv2.bias', 'model.down_blocks.3.resnets.1.norm1.weight', 'model.down_blocks.3.resnets.1.norm1.bias', 'model.down_blocks.3.resnets.1.conv1.weight', 'model.down_blocks.3.resnets.1.conv1.bias', 'model.down_blocks.3.resnets.1.time_emb_proj.weight', 'model.down_blocks.3.resnets.1.time_emb_proj.bias', 'model.down_blocks.3.resnets.1.norm2.weight', 'model.down_blocks.3.resnets.1.norm2.bias', 'model.down_blocks.3.resnets.1.conv2.weight', 'model.down_blocks.3.resnets.1.conv2.bias', 'model.up_blocks.0.resnets.0.norm1.weight', 'model.up_blocks.0.resnets.0.norm1.bias', 'model.up_blocks.0.resnets.0.conv1.weight', 'model.up_blocks.0.resnets.0.conv1.bias', 'model.up_blocks.0.resnets.0.time_emb_proj.weight', 'model.up_blocks.0.resnets.0.time_emb_proj.bias', 'model.up_blocks.0.resnets.0.norm2.weight', 'model.up_blocks.0.resnets.0.norm2.bias', 'model.up_blocks.0.resnets.0.conv2.weight', 'model.up_blocks.0.resnets.0.conv2.bias', 'model.up_blocks.0.resnets.0.conv_shortcut.weight', 'model.up_blocks.0.resnets.0.conv_shortcut.bias', 'model.up_blocks.0.resnets.1.norm1.weight', 'model.up_blocks.0.resnets.1.norm1.bias', 'model.up_blocks.0.resnets.1.conv1.weight', 'model.up_blocks.0.resnets.1.conv1.bias', 'model.up_blocks.0.resnets.1.time_emb_proj.weight', 'model.up_blocks.0.resnets.1.time_emb_proj.bias', 'model.up_blocks.0.resnets.1.norm2.weight', 'model.up_blocks.0.resnets.1.norm2.bias', 'model.up_blocks.0.resnets.1.conv2.weight', 'model.up_blocks.0.resnets.1.conv2.bias', 'model.up_blocks.0.resnets.1.conv_shortcut.weight', 'model.up_blocks.0.resnets.1.conv_shortcut.bias', 'model.up_blocks.0.resnets.2.norm1.weight', 'model.up_blocks.0.resnets.2.norm1.bias', 'model.up_blocks.0.resnets.2.conv1.weight', 'model.up_blocks.0.resnets.2.conv1.bias', 'model.up_blocks.0.resnets.2.time_emb_proj.weight', 'model.up_blocks.0.resnets.2.time_emb_proj.bias', 'model.up_blocks.0.resnets.2.norm2.weight', 'model.up_blocks.0.resnets.2.norm2.bias', 'model.up_blocks.0.resnets.2.conv2.weight', 'model.up_blocks.0.resnets.2.conv2.bias', 'model.up_blocks.0.resnets.2.conv_shortcut.weight', 'model.up_blocks.0.resnets.2.conv_shortcut.bias', 'model.up_blocks.0.upsamplers.0.conv.weight', 'model.up_blocks.0.upsamplers.0.conv.bias', 'model.up_blocks.1.attentions.0.norm.weight', 'model.up_blocks.1.attentions.0.norm.bias', 'model.up_blocks.1.attentions.0.proj_in.weight', 'model.up_blocks.1.attentions.0.proj_in.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.0.proj_out.weight', 'model.up_blocks.1.attentions.0.proj_out.bias', 'model.up_blocks.1.attentions.1.norm.weight', 'model.up_blocks.1.attentions.1.norm.bias', 'model.up_blocks.1.attentions.1.proj_in.weight', 'model.up_blocks.1.attentions.1.proj_in.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.1.proj_out.weight', 'model.up_blocks.1.attentions.1.proj_out.bias', 'model.up_blocks.1.attentions.2.norm.weight', 'model.up_blocks.1.attentions.2.norm.bias', 'model.up_blocks.1.attentions.2.proj_in.weight', 'model.up_blocks.1.attentions.2.proj_in.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.2.proj_out.weight', 'model.up_blocks.1.attentions.2.proj_out.bias', 'model.up_blocks.1.resnets.0.norm1.weight', 'model.up_blocks.1.resnets.0.norm1.bias', 'model.up_blocks.1.resnets.0.conv1.weight', 'model.up_blocks.1.resnets.0.conv1.bias', 'model.up_blocks.1.resnets.0.time_emb_proj.weight', 'model.up_blocks.1.resnets.0.time_emb_proj.bias', 'model.up_blocks.1.resnets.0.norm2.weight', 'model.up_blocks.1.resnets.0.norm2.bias', 'model.up_blocks.1.resnets.0.conv2.weight', 'model.up_blocks.1.resnets.0.conv2.bias', 'model.up_blocks.1.resnets.0.conv_shortcut.weight', 'model.up_blocks.1.resnets.0.conv_shortcut.bias', 'model.up_blocks.1.resnets.1.norm1.weight', 'model.up_blocks.1.resnets.1.norm1.bias', 'model.up_blocks.1.resnets.1.conv1.weight', 'model.up_blocks.1.resnets.1.conv1.bias', 'model.up_blocks.1.resnets.1.time_emb_proj.weight', 'model.up_blocks.1.resnets.1.time_emb_proj.bias', 'model.up_blocks.1.resnets.1.norm2.weight', 'model.up_blocks.1.resnets.1.norm2.bias', 'model.up_blocks.1.resnets.1.conv2.weight', 'model.up_blocks.1.resnets.1.conv2.bias', 'model.up_blocks.1.resnets.1.conv_shortcut.weight', 'model.up_blocks.1.resnets.1.conv_shortcut.bias', 'model.up_blocks.1.resnets.2.norm1.weight', 'model.up_blocks.1.resnets.2.norm1.bias', 'model.up_blocks.1.resnets.2.conv1.weight', 'model.up_blocks.1.resnets.2.conv1.bias', 'model.up_blocks.1.resnets.2.time_emb_proj.weight', 'model.up_blocks.1.resnets.2.time_emb_proj.bias', 'model.up_blocks.1.resnets.2.norm2.weight', 'model.up_blocks.1.resnets.2.norm2.bias', 'model.up_blocks.1.resnets.2.conv2.weight', 'model.up_blocks.1.resnets.2.conv2.bias', 'model.up_blocks.1.resnets.2.conv_shortcut.weight', 'model.up_blocks.1.resnets.2.conv_shortcut.bias', 'model.up_blocks.1.upsamplers.0.conv.weight', 'model.up_blocks.1.upsamplers.0.conv.bias', 'model.up_blocks.2.attentions.0.norm.weight', 'model.up_blocks.2.attentions.0.norm.bias', 'model.up_blocks.2.attentions.0.proj_in.weight', 'model.up_blocks.2.attentions.0.proj_in.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.0.proj_out.weight', 'model.up_blocks.2.attentions.0.proj_out.bias', 'model.up_blocks.2.attentions.1.norm.weight', 'model.up_blocks.2.attentions.1.norm.bias', 'model.up_blocks.2.attentions.1.proj_in.weight', 'model.up_blocks.2.attentions.1.proj_in.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.1.proj_out.weight', 'model.up_blocks.2.attentions.1.proj_out.bias', 'model.up_blocks.2.attentions.2.norm.weight', 'model.up_blocks.2.attentions.2.norm.bias', 'model.up_blocks.2.attentions.2.proj_in.weight', 'model.up_blocks.2.attentions.2.proj_in.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.2.proj_out.weight', 'model.up_blocks.2.attentions.2.proj_out.bias', 'model.up_blocks.2.resnets.0.norm1.weight', 'model.up_blocks.2.resnets.0.norm1.bias', 'model.up_blocks.2.resnets.0.conv1.weight', 'model.up_blocks.2.resnets.0.conv1.bias', 'model.up_blocks.2.resnets.0.time_emb_proj.weight', 'model.up_blocks.2.resnets.0.time_emb_proj.bias', 'model.up_blocks.2.resnets.0.norm2.weight', 'model.up_blocks.2.resnets.0.norm2.bias', 'model.up_blocks.2.resnets.0.conv2.weight', 'model.up_blocks.2.resnets.0.conv2.bias', 'model.up_blocks.2.resnets.0.conv_shortcut.weight', 'model.up_blocks.2.resnets.0.conv_shortcut.bias', 'model.up_blocks.2.resnets.1.norm1.weight', 'model.up_blocks.2.resnets.1.norm1.bias', 'model.up_blocks.2.resnets.1.conv1.weight', 'model.up_blocks.2.resnets.1.conv1.bias', 'model.up_blocks.2.resnets.1.time_emb_proj.weight', 'model.up_blocks.2.resnets.1.time_emb_proj.bias', 'model.up_blocks.2.resnets.1.norm2.weight', 'model.up_blocks.2.resnets.1.norm2.bias', 'model.up_blocks.2.resnets.1.conv2.weight', 'model.up_blocks.2.resnets.1.conv2.bias', 'model.up_blocks.2.resnets.1.conv_shortcut.weight', 'model.up_blocks.2.resnets.1.conv_shortcut.bias', 'model.up_blocks.2.resnets.2.norm1.weight', 'model.up_blocks.2.resnets.2.norm1.bias', 'model.up_blocks.2.resnets.2.conv1.weight', 'model.up_blocks.2.resnets.2.conv1.bias', 'model.up_blocks.2.resnets.2.time_emb_proj.weight', 'model.up_blocks.2.resnets.2.time_emb_proj.bias', 'model.up_blocks.2.resnets.2.norm2.weight', 'model.up_blocks.2.resnets.2.norm2.bias', 'model.up_blocks.2.resnets.2.conv2.weight', 'model.up_blocks.2.resnets.2.conv2.bias', 'model.up_blocks.2.resnets.2.conv_shortcut.weight', 'model.up_blocks.2.resnets.2.conv_shortcut.bias', 'model.up_blocks.2.upsamplers.0.conv.weight', 'model.up_blocks.2.upsamplers.0.conv.bias', 'model.up_blocks.3.attentions.0.norm.weight', 'model.up_blocks.3.attentions.0.norm.bias', 'model.up_blocks.3.attentions.0.proj_in.weight', 'model.up_blocks.3.attentions.0.proj_in.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.0.proj_out.weight', 'model.up_blocks.3.attentions.0.proj_out.bias', 'model.up_blocks.3.attentions.1.norm.weight', 'model.up_blocks.3.attentions.1.norm.bias', 'model.up_blocks.3.attentions.1.proj_in.weight', 'model.up_blocks.3.attentions.1.proj_in.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.1.proj_out.weight', 'model.up_blocks.3.attentions.1.proj_out.bias', 'model.up_blocks.3.attentions.2.norm.weight', 'model.up_blocks.3.attentions.2.norm.bias', 'model.up_blocks.3.attentions.2.proj_in.weight', 'model.up_blocks.3.attentions.2.proj_in.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.2.proj_out.weight', 'model.up_blocks.3.attentions.2.proj_out.bias', 'model.up_blocks.3.resnets.0.norm1.weight', 'model.up_blocks.3.resnets.0.norm1.bias', 'model.up_blocks.3.resnets.0.conv1.weight', 'model.up_blocks.3.resnets.0.conv1.bias', 'model.up_blocks.3.resnets.0.time_emb_proj.weight', 'model.up_blocks.3.resnets.0.time_emb_proj.bias', 'model.up_blocks.3.resnets.0.norm2.weight', 'model.up_blocks.3.resnets.0.norm2.bias', 'model.up_blocks.3.resnets.0.conv2.weight', 'model.up_blocks.3.resnets.0.conv2.bias', 'model.up_blocks.3.resnets.0.conv_shortcut.weight', 'model.up_blocks.3.resnets.0.conv_shortcut.bias', 'model.up_blocks.3.resnets.1.norm1.weight', 'model.up_blocks.3.resnets.1.norm1.bias', 'model.up_blocks.3.resnets.1.conv1.weight', 'model.up_blocks.3.resnets.1.conv1.bias', 'model.up_blocks.3.resnets.1.time_emb_proj.weight', 'model.up_blocks.3.resnets.1.time_emb_proj.bias', 'model.up_blocks.3.resnets.1.norm2.weight', 'model.up_blocks.3.resnets.1.norm2.bias', 'model.up_blocks.3.resnets.1.conv2.weight', 'model.up_blocks.3.resnets.1.conv2.bias', 'model.up_blocks.3.resnets.1.conv_shortcut.weight', 'model.up_blocks.3.resnets.1.conv_shortcut.bias', 'model.up_blocks.3.resnets.2.norm1.weight', 'model.up_blocks.3.resnets.2.norm1.bias', 'model.up_blocks.3.resnets.2.conv1.weight', 'model.up_blocks.3.resnets.2.conv1.bias', 'model.up_blocks.3.resnets.2.time_emb_proj.weight', 'model.up_blocks.3.resnets.2.time_emb_proj.bias', 'model.up_blocks.3.resnets.2.norm2.weight', 'model.up_blocks.3.resnets.2.norm2.bias', 'model.up_blocks.3.resnets.2.conv2.weight', 'model.up_blocks.3.resnets.2.conv2.bias', 'model.up_blocks.3.resnets.2.conv_shortcut.weight', 'model.up_blocks.3.resnets.2.conv_shortcut.bias', 'model.mid_block.attentions.0.norm.weight', 'model.mid_block.attentions.0.norm.bias', 'model.mid_block.attentions.0.proj_in.weight', 'model.mid_block.attentions.0.proj_in.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.mid_block.attentions.0.proj_out.weight', 'model.mid_block.attentions.0.proj_out.bias', 'model.mid_block.resnets.0.norm1.weight', 'model.mid_block.resnets.0.norm1.bias', 'model.mid_block.resnets.0.conv1.weight', 'model.mid_block.resnets.0.conv1.bias', 'model.mid_block.resnets.0.time_emb_proj.weight', 'model.mid_block.resnets.0.time_emb_proj.bias', 'model.mid_block.resnets.0.norm2.weight', 'model.mid_block.resnets.0.norm2.bias', 'model.mid_block.resnets.0.conv2.weight', 'model.mid_block.resnets.0.conv2.bias', 'model.mid_block.resnets.1.norm1.weight', 'model.mid_block.resnets.1.norm1.bias', 'model.mid_block.resnets.1.conv1.weight', 'model.mid_block.resnets.1.conv1.bias', 'model.mid_block.resnets.1.time_emb_proj.weight', 'model.mid_block.resnets.1.time_emb_proj.bias', 'model.mid_block.resnets.1.norm2.weight', 'model.mid_block.resnets.1.norm2.bias', 'model.mid_block.resnets.1.conv2.weight', 'model.mid_block.resnets.1.conv2.bias', 'model.conv_norm_out.weight', 'model.conv_norm_out.bias', 'model.conv_out.weight', 'model.conv_out.bias'])
[2024-03-19 10:49:33 pose_transfer_train.py:376] preparing lr scheduler...
[2024-03-19 10:49:33 pose_transfer_train.py:382] start training...
[2024-03-19 10:49:33 pose_transfer_train.py:391] epoch 1 start
[2024-03-19 10:50:52 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 10:50:52 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 10:51:01 pose_transfer_train.py:298] preparing model...
[2024-03-19 10:51:04 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 10:51:16 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 10:51:16 pose_transfer_train.py:344] preparing optimizer...
[2024-03-19 10:51:16 pose_transfer_train.py:350] preparing accelerator...
[2024-03-19 10:51:18 pose_transfer_train.py:357] loading states from ./checkpoints/
[2024-03-19 10:51:19 pose_transfer_train.py:367] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 10:51:21 pose_transfer_train.py:370] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 10:51:21 pose_transfer_train.py:376] preparing lr scheduler...
[2024-03-19 10:51:21 pose_transfer_train.py:382] start training...
[2024-03-19 10:51:21 pose_transfer_train.py:391] epoch 1 start
[2024-03-19 10:53:58 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 10:53:58 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 10:54:07 pose_transfer_train.py:298] preparing model...
[2024-03-19 10:54:10 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 10:54:22 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 10:54:22 pose_transfer_train.py:344] preparing optimizer...
[2024-03-19 10:54:22 pose_transfer_train.py:350] preparing accelerator...
[2024-03-19 10:54:25 pose_transfer_train.py:357] loading states from ./checkpoints/
[2024-03-19 10:54:25 pose_transfer_train.py:367] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 10:54:28 pose_transfer_train.py:370] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 10:54:28 pose_transfer_train.py:376] preparing lr scheduler...
[2024-03-19 10:54:28 pose_transfer_train.py:382] start training...
[2024-03-19 10:54:28 pose_transfer_train.py:391] epoch 1 start
[2024-03-19 11:00:52 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:00:52 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:01:01 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:01:04 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:01:16 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:02:02 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:02:02 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:02:11 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:02:14 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:02:26 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:02:26 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 11:02:26 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 11:02:28 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 11:02:29 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 11:02:32 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 11:02:32 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 11:02:32 pose_transfer_train.py:394] start training...
[2024-03-19 11:02:32 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 11:19:55 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:19:55 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:20:04 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:20:07 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:20:19 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:20:19 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 11:20:19 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 11:20:21 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 11:20:22 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 11:20:25 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 11:20:25 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 11:20:25 pose_transfer_train.py:394] start training...
[2024-03-19 11:20:25 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 11:24:33 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:24:33 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:24:42 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:24:45 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:24:58 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:24:58 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 11:24:58 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 11:25:00 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 11:25:01 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 11:25:03 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['module.model.conv_in.weight', 'module.model.conv_in.bias', 'module.model.time_embedding.linear_1.weight', 'module.model.time_embedding.linear_1.bias', 'module.model.time_embedding.linear_2.weight', 'module.model.time_embedding.linear_2.bias', 'module.model.down_blocks.0.attentions.0.norm.weight', 'module.model.down_blocks.0.attentions.0.norm.bias', 'module.model.down_blocks.0.attentions.0.proj_in.weight', 'module.model.down_blocks.0.attentions.0.proj_in.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.0.attentions.0.proj_out.weight', 'module.model.down_blocks.0.attentions.0.proj_out.bias', 'module.model.down_blocks.0.attentions.1.norm.weight', 'module.model.down_blocks.0.attentions.1.norm.bias', 'module.model.down_blocks.0.attentions.1.proj_in.weight', 'module.model.down_blocks.0.attentions.1.proj_in.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.0.attentions.1.proj_out.weight', 'module.model.down_blocks.0.attentions.1.proj_out.bias', 'module.model.down_blocks.0.resnets.0.norm1.weight', 'module.model.down_blocks.0.resnets.0.norm1.bias', 'module.model.down_blocks.0.resnets.0.conv1.weight', 'module.model.down_blocks.0.resnets.0.conv1.bias', 'module.model.down_blocks.0.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.0.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.0.resnets.0.norm2.weight', 'module.model.down_blocks.0.resnets.0.norm2.bias', 'module.model.down_blocks.0.resnets.0.conv2.weight', 'module.model.down_blocks.0.resnets.0.conv2.bias', 'module.model.down_blocks.0.resnets.1.norm1.weight', 'module.model.down_blocks.0.resnets.1.norm1.bias', 'module.model.down_blocks.0.resnets.1.conv1.weight', 'module.model.down_blocks.0.resnets.1.conv1.bias', 'module.model.down_blocks.0.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.0.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.0.resnets.1.norm2.weight', 'module.model.down_blocks.0.resnets.1.norm2.bias', 'module.model.down_blocks.0.resnets.1.conv2.weight', 'module.model.down_blocks.0.resnets.1.conv2.bias', 'module.model.down_blocks.0.downsamplers.0.conv.weight', 'module.model.down_blocks.0.downsamplers.0.conv.bias', 'module.model.down_blocks.1.attentions.0.norm.weight', 'module.model.down_blocks.1.attentions.0.norm.bias', 'module.model.down_blocks.1.attentions.0.proj_in.weight', 'module.model.down_blocks.1.attentions.0.proj_in.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.1.attentions.0.proj_out.weight', 'module.model.down_blocks.1.attentions.0.proj_out.bias', 'module.model.down_blocks.1.attentions.1.norm.weight', 'module.model.down_blocks.1.attentions.1.norm.bias', 'module.model.down_blocks.1.attentions.1.proj_in.weight', 'module.model.down_blocks.1.attentions.1.proj_in.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.1.attentions.1.proj_out.weight', 'module.model.down_blocks.1.attentions.1.proj_out.bias', 'module.model.down_blocks.1.resnets.0.norm1.weight', 'module.model.down_blocks.1.resnets.0.norm1.bias', 'module.model.down_blocks.1.resnets.0.conv1.weight', 'module.model.down_blocks.1.resnets.0.conv1.bias', 'module.model.down_blocks.1.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.1.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.1.resnets.0.norm2.weight', 'module.model.down_blocks.1.resnets.0.norm2.bias', 'module.model.down_blocks.1.resnets.0.conv2.weight', 'module.model.down_blocks.1.resnets.0.conv2.bias', 'module.model.down_blocks.1.resnets.0.conv_shortcut.weight', 'module.model.down_blocks.1.resnets.0.conv_shortcut.bias', 'module.model.down_blocks.1.resnets.1.norm1.weight', 'module.model.down_blocks.1.resnets.1.norm1.bias', 'module.model.down_blocks.1.resnets.1.conv1.weight', 'module.model.down_blocks.1.resnets.1.conv1.bias', 'module.model.down_blocks.1.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.1.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.1.resnets.1.norm2.weight', 'module.model.down_blocks.1.resnets.1.norm2.bias', 'module.model.down_blocks.1.resnets.1.conv2.weight', 'module.model.down_blocks.1.resnets.1.conv2.bias', 'module.model.down_blocks.1.downsamplers.0.conv.weight', 'module.model.down_blocks.1.downsamplers.0.conv.bias', 'module.model.down_blocks.2.attentions.0.norm.weight', 'module.model.down_blocks.2.attentions.0.norm.bias', 'module.model.down_blocks.2.attentions.0.proj_in.weight', 'module.model.down_blocks.2.attentions.0.proj_in.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.2.attentions.0.proj_out.weight', 'module.model.down_blocks.2.attentions.0.proj_out.bias', 'module.model.down_blocks.2.attentions.1.norm.weight', 'module.model.down_blocks.2.attentions.1.norm.bias', 'module.model.down_blocks.2.attentions.1.proj_in.weight', 'module.model.down_blocks.2.attentions.1.proj_in.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.2.attentions.1.proj_out.weight', 'module.model.down_blocks.2.attentions.1.proj_out.bias', 'module.model.down_blocks.2.resnets.0.norm1.weight', 'module.model.down_blocks.2.resnets.0.norm1.bias', 'module.model.down_blocks.2.resnets.0.conv1.weight', 'module.model.down_blocks.2.resnets.0.conv1.bias', 'module.model.down_blocks.2.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.2.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.2.resnets.0.norm2.weight', 'module.model.down_blocks.2.resnets.0.norm2.bias', 'module.model.down_blocks.2.resnets.0.conv2.weight', 'module.model.down_blocks.2.resnets.0.conv2.bias', 'module.model.down_blocks.2.resnets.0.conv_shortcut.weight', 'module.model.down_blocks.2.resnets.0.conv_shortcut.bias', 'module.model.down_blocks.2.resnets.1.norm1.weight', 'module.model.down_blocks.2.resnets.1.norm1.bias', 'module.model.down_blocks.2.resnets.1.conv1.weight', 'module.model.down_blocks.2.resnets.1.conv1.bias', 'module.model.down_blocks.2.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.2.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.2.resnets.1.norm2.weight', 'module.model.down_blocks.2.resnets.1.norm2.bias', 'module.model.down_blocks.2.resnets.1.conv2.weight', 'module.model.down_blocks.2.resnets.1.conv2.bias', 'module.model.down_blocks.2.downsamplers.0.conv.weight', 'module.model.down_blocks.2.downsamplers.0.conv.bias', 'module.model.down_blocks.3.resnets.0.norm1.weight', 'module.model.down_blocks.3.resnets.0.norm1.bias', 'module.model.down_blocks.3.resnets.0.conv1.weight', 'module.model.down_blocks.3.resnets.0.conv1.bias', 'module.model.down_blocks.3.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.3.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.3.resnets.0.norm2.weight', 'module.model.down_blocks.3.resnets.0.norm2.bias', 'module.model.down_blocks.3.resnets.0.conv2.weight', 'module.model.down_blocks.3.resnets.0.conv2.bias', 'module.model.down_blocks.3.resnets.1.norm1.weight', 'module.model.down_blocks.3.resnets.1.norm1.bias', 'module.model.down_blocks.3.resnets.1.conv1.weight', 'module.model.down_blocks.3.resnets.1.conv1.bias', 'module.model.down_blocks.3.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.3.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.3.resnets.1.norm2.weight', 'module.model.down_blocks.3.resnets.1.norm2.bias', 'module.model.down_blocks.3.resnets.1.conv2.weight', 'module.model.down_blocks.3.resnets.1.conv2.bias', 'module.model.up_blocks.0.resnets.0.norm1.weight', 'module.model.up_blocks.0.resnets.0.norm1.bias', 'module.model.up_blocks.0.resnets.0.conv1.weight', 'module.model.up_blocks.0.resnets.0.conv1.bias', 'module.model.up_blocks.0.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.0.norm2.weight', 'module.model.up_blocks.0.resnets.0.norm2.bias', 'module.model.up_blocks.0.resnets.0.conv2.weight', 'module.model.up_blocks.0.resnets.0.conv2.bias', 'module.model.up_blocks.0.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.0.resnets.1.norm1.weight', 'module.model.up_blocks.0.resnets.1.norm1.bias', 'module.model.up_blocks.0.resnets.1.conv1.weight', 'module.model.up_blocks.0.resnets.1.conv1.bias', 'module.model.up_blocks.0.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.1.norm2.weight', 'module.model.up_blocks.0.resnets.1.norm2.bias', 'module.model.up_blocks.0.resnets.1.conv2.weight', 'module.model.up_blocks.0.resnets.1.conv2.bias', 'module.model.up_blocks.0.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.0.resnets.2.norm1.weight', 'module.model.up_blocks.0.resnets.2.norm1.bias', 'module.model.up_blocks.0.resnets.2.conv1.weight', 'module.model.up_blocks.0.resnets.2.conv1.bias', 'module.model.up_blocks.0.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.2.norm2.weight', 'module.model.up_blocks.0.resnets.2.norm2.bias', 'module.model.up_blocks.0.resnets.2.conv2.weight', 'module.model.up_blocks.0.resnets.2.conv2.bias', 'module.model.up_blocks.0.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.0.upsamplers.0.conv.weight', 'module.model.up_blocks.0.upsamplers.0.conv.bias', 'module.model.up_blocks.1.attentions.0.norm.weight', 'module.model.up_blocks.1.attentions.0.norm.bias', 'module.model.up_blocks.1.attentions.0.proj_in.weight', 'module.model.up_blocks.1.attentions.0.proj_in.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.0.proj_out.weight', 'module.model.up_blocks.1.attentions.0.proj_out.bias', 'module.model.up_blocks.1.attentions.1.norm.weight', 'module.model.up_blocks.1.attentions.1.norm.bias', 'module.model.up_blocks.1.attentions.1.proj_in.weight', 'module.model.up_blocks.1.attentions.1.proj_in.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.1.proj_out.weight', 'module.model.up_blocks.1.attentions.1.proj_out.bias', 'module.model.up_blocks.1.attentions.2.norm.weight', 'module.model.up_blocks.1.attentions.2.norm.bias', 'module.model.up_blocks.1.attentions.2.proj_in.weight', 'module.model.up_blocks.1.attentions.2.proj_in.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.2.proj_out.weight', 'module.model.up_blocks.1.attentions.2.proj_out.bias', 'module.model.up_blocks.1.resnets.0.norm1.weight', 'module.model.up_blocks.1.resnets.0.norm1.bias', 'module.model.up_blocks.1.resnets.0.conv1.weight', 'module.model.up_blocks.1.resnets.0.conv1.bias', 'module.model.up_blocks.1.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.0.norm2.weight', 'module.model.up_blocks.1.resnets.0.norm2.bias', 'module.model.up_blocks.1.resnets.0.conv2.weight', 'module.model.up_blocks.1.resnets.0.conv2.bias', 'module.model.up_blocks.1.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.1.resnets.1.norm1.weight', 'module.model.up_blocks.1.resnets.1.norm1.bias', 'module.model.up_blocks.1.resnets.1.conv1.weight', 'module.model.up_blocks.1.resnets.1.conv1.bias', 'module.model.up_blocks.1.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.1.norm2.weight', 'module.model.up_blocks.1.resnets.1.norm2.bias', 'module.model.up_blocks.1.resnets.1.conv2.weight', 'module.model.up_blocks.1.resnets.1.conv2.bias', 'module.model.up_blocks.1.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.1.resnets.2.norm1.weight', 'module.model.up_blocks.1.resnets.2.norm1.bias', 'module.model.up_blocks.1.resnets.2.conv1.weight', 'module.model.up_blocks.1.resnets.2.conv1.bias', 'module.model.up_blocks.1.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.2.norm2.weight', 'module.model.up_blocks.1.resnets.2.norm2.bias', 'module.model.up_blocks.1.resnets.2.conv2.weight', 'module.model.up_blocks.1.resnets.2.conv2.bias', 'module.model.up_blocks.1.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.1.warpflows.0.norm.weight', 'module.model.up_blocks.1.warpflows.0.norm.bias', 'module.model.up_blocks.1.warpflows.0.proj_in.weight', 'module.model.up_blocks.1.warpflows.0.proj_in.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.0.proj_out.weight', 'module.model.up_blocks.1.warpflows.0.proj_out.bias', 'module.model.up_blocks.1.warpflows.1.norm.weight', 'module.model.up_blocks.1.warpflows.1.norm.bias', 'module.model.up_blocks.1.warpflows.1.proj_in.weight', 'module.model.up_blocks.1.warpflows.1.proj_in.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.1.proj_out.weight', 'module.model.up_blocks.1.warpflows.1.proj_out.bias', 'module.model.up_blocks.1.warpflows.2.norm.weight', 'module.model.up_blocks.1.warpflows.2.norm.bias', 'module.model.up_blocks.1.warpflows.2.proj_in.weight', 'module.model.up_blocks.1.warpflows.2.proj_in.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.2.proj_out.weight', 'module.model.up_blocks.1.warpflows.2.proj_out.bias', 'module.model.up_blocks.1.warpzc.0.weight', 'module.model.up_blocks.1.warpzc.0.bias', 'module.model.up_blocks.1.warpzc.1.weight', 'module.model.up_blocks.1.warpzc.1.bias', 'module.model.up_blocks.1.warpzc.2.weight', 'module.model.up_blocks.1.warpzc.2.bias', 'module.model.up_blocks.1.upsamplers.0.conv.weight', 'module.model.up_blocks.1.upsamplers.0.conv.bias', 'module.model.up_blocks.2.attentions.0.norm.weight', 'module.model.up_blocks.2.attentions.0.norm.bias', 'module.model.up_blocks.2.attentions.0.proj_in.weight', 'module.model.up_blocks.2.attentions.0.proj_in.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.0.proj_out.weight', 'module.model.up_blocks.2.attentions.0.proj_out.bias', 'module.model.up_blocks.2.attentions.1.norm.weight', 'module.model.up_blocks.2.attentions.1.norm.bias', 'module.model.up_blocks.2.attentions.1.proj_in.weight', 'module.model.up_blocks.2.attentions.1.proj_in.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.1.proj_out.weight', 'module.model.up_blocks.2.attentions.1.proj_out.bias', 'module.model.up_blocks.2.attentions.2.norm.weight', 'module.model.up_blocks.2.attentions.2.norm.bias', 'module.model.up_blocks.2.attentions.2.proj_in.weight', 'module.model.up_blocks.2.attentions.2.proj_in.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.2.proj_out.weight', 'module.model.up_blocks.2.attentions.2.proj_out.bias', 'module.model.up_blocks.2.resnets.0.norm1.weight', 'module.model.up_blocks.2.resnets.0.norm1.bias', 'module.model.up_blocks.2.resnets.0.conv1.weight', 'module.model.up_blocks.2.resnets.0.conv1.bias', 'module.model.up_blocks.2.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.0.norm2.weight', 'module.model.up_blocks.2.resnets.0.norm2.bias', 'module.model.up_blocks.2.resnets.0.conv2.weight', 'module.model.up_blocks.2.resnets.0.conv2.bias', 'module.model.up_blocks.2.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.2.resnets.1.norm1.weight', 'module.model.up_blocks.2.resnets.1.norm1.bias', 'module.model.up_blocks.2.resnets.1.conv1.weight', 'module.model.up_blocks.2.resnets.1.conv1.bias', 'module.model.up_blocks.2.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.1.norm2.weight', 'module.model.up_blocks.2.resnets.1.norm2.bias', 'module.model.up_blocks.2.resnets.1.conv2.weight', 'module.model.up_blocks.2.resnets.1.conv2.bias', 'module.model.up_blocks.2.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.2.resnets.2.norm1.weight', 'module.model.up_blocks.2.resnets.2.norm1.bias', 'module.model.up_blocks.2.resnets.2.conv1.weight', 'module.model.up_blocks.2.resnets.2.conv1.bias', 'module.model.up_blocks.2.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.2.norm2.weight', 'module.model.up_blocks.2.resnets.2.norm2.bias', 'module.model.up_blocks.2.resnets.2.conv2.weight', 'module.model.up_blocks.2.resnets.2.conv2.bias', 'module.model.up_blocks.2.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.2.warpflows.0.norm.weight', 'module.model.up_blocks.2.warpflows.0.norm.bias', 'module.model.up_blocks.2.warpflows.0.proj_in.weight', 'module.model.up_blocks.2.warpflows.0.proj_in.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.0.proj_out.weight', 'module.model.up_blocks.2.warpflows.0.proj_out.bias', 'module.model.up_blocks.2.warpflows.1.norm.weight', 'module.model.up_blocks.2.warpflows.1.norm.bias', 'module.model.up_blocks.2.warpflows.1.proj_in.weight', 'module.model.up_blocks.2.warpflows.1.proj_in.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.1.proj_out.weight', 'module.model.up_blocks.2.warpflows.1.proj_out.bias', 'module.model.up_blocks.2.warpflows.2.norm.weight', 'module.model.up_blocks.2.warpflows.2.norm.bias', 'module.model.up_blocks.2.warpflows.2.proj_in.weight', 'module.model.up_blocks.2.warpflows.2.proj_in.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.2.proj_out.weight', 'module.model.up_blocks.2.warpflows.2.proj_out.bias', 'module.model.up_blocks.2.warpzc.0.weight', 'module.model.up_blocks.2.warpzc.0.bias', 'module.model.up_blocks.2.warpzc.1.weight', 'module.model.up_blocks.2.warpzc.1.bias', 'module.model.up_blocks.2.warpzc.2.weight', 'module.model.up_blocks.2.warpzc.2.bias', 'module.model.up_blocks.2.upsamplers.0.conv.weight', 'module.model.up_blocks.2.upsamplers.0.conv.bias', 'module.model.up_blocks.3.attentions.0.norm.weight', 'module.model.up_blocks.3.attentions.0.norm.bias', 'module.model.up_blocks.3.attentions.0.proj_in.weight', 'module.model.up_blocks.3.attentions.0.proj_in.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.0.proj_out.weight', 'module.model.up_blocks.3.attentions.0.proj_out.bias', 'module.model.up_blocks.3.attentions.1.norm.weight', 'module.model.up_blocks.3.attentions.1.norm.bias', 'module.model.up_blocks.3.attentions.1.proj_in.weight', 'module.model.up_blocks.3.attentions.1.proj_in.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.1.proj_out.weight', 'module.model.up_blocks.3.attentions.1.proj_out.bias', 'module.model.up_blocks.3.attentions.2.norm.weight', 'module.model.up_blocks.3.attentions.2.norm.bias', 'module.model.up_blocks.3.attentions.2.proj_in.weight', 'module.model.up_blocks.3.attentions.2.proj_in.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.2.proj_out.weight', 'module.model.up_blocks.3.attentions.2.proj_out.bias', 'module.model.up_blocks.3.resnets.0.norm1.weight', 'module.model.up_blocks.3.resnets.0.norm1.bias', 'module.model.up_blocks.3.resnets.0.conv1.weight', 'module.model.up_blocks.3.resnets.0.conv1.bias', 'module.model.up_blocks.3.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.0.norm2.weight', 'module.model.up_blocks.3.resnets.0.norm2.bias', 'module.model.up_blocks.3.resnets.0.conv2.weight', 'module.model.up_blocks.3.resnets.0.conv2.bias', 'module.model.up_blocks.3.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.3.resnets.1.norm1.weight', 'module.model.up_blocks.3.resnets.1.norm1.bias', 'module.model.up_blocks.3.resnets.1.conv1.weight', 'module.model.up_blocks.3.resnets.1.conv1.bias', 'module.model.up_blocks.3.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.1.norm2.weight', 'module.model.up_blocks.3.resnets.1.norm2.bias', 'module.model.up_blocks.3.resnets.1.conv2.weight', 'module.model.up_blocks.3.resnets.1.conv2.bias', 'module.model.up_blocks.3.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.3.resnets.2.norm1.weight', 'module.model.up_blocks.3.resnets.2.norm1.bias', 'module.model.up_blocks.3.resnets.2.conv1.weight', 'module.model.up_blocks.3.resnets.2.conv1.bias', 'module.model.up_blocks.3.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.2.norm2.weight', 'module.model.up_blocks.3.resnets.2.norm2.bias', 'module.model.up_blocks.3.resnets.2.conv2.weight', 'module.model.up_blocks.3.resnets.2.conv2.bias', 'module.model.up_blocks.3.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.3.warpflows.0.norm.weight', 'module.model.up_blocks.3.warpflows.0.norm.bias', 'module.model.up_blocks.3.warpflows.0.proj_in.weight', 'module.model.up_blocks.3.warpflows.0.proj_in.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.0.proj_out.weight', 'module.model.up_blocks.3.warpflows.0.proj_out.bias', 'module.model.up_blocks.3.warpflows.1.norm.weight', 'module.model.up_blocks.3.warpflows.1.norm.bias', 'module.model.up_blocks.3.warpflows.1.proj_in.weight', 'module.model.up_blocks.3.warpflows.1.proj_in.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.1.proj_out.weight', 'module.model.up_blocks.3.warpflows.1.proj_out.bias', 'module.model.up_blocks.3.warpflows.2.norm.weight', 'module.model.up_blocks.3.warpflows.2.norm.bias', 'module.model.up_blocks.3.warpflows.2.proj_in.weight', 'module.model.up_blocks.3.warpflows.2.proj_in.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.2.proj_out.weight', 'module.model.up_blocks.3.warpflows.2.proj_out.bias', 'module.model.up_blocks.3.warpzc.0.weight', 'module.model.up_blocks.3.warpzc.0.bias', 'module.model.up_blocks.3.warpzc.1.weight', 'module.model.up_blocks.3.warpzc.1.bias', 'module.model.up_blocks.3.warpzc.2.weight', 'module.model.up_blocks.3.warpzc.2.bias', 'module.model.mid_block.attentions.0.norm.weight', 'module.model.mid_block.attentions.0.norm.bias', 'module.model.mid_block.attentions.0.proj_in.weight', 'module.model.mid_block.attentions.0.proj_in.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.mid_block.attentions.0.proj_out.weight', 'module.model.mid_block.attentions.0.proj_out.bias', 'module.model.mid_block.resnets.0.norm1.weight', 'module.model.mid_block.resnets.0.norm1.bias', 'module.model.mid_block.resnets.0.conv1.weight', 'module.model.mid_block.resnets.0.conv1.bias', 'module.model.mid_block.resnets.0.time_emb_proj.weight', 'module.model.mid_block.resnets.0.time_emb_proj.bias', 'module.model.mid_block.resnets.0.norm2.weight', 'module.model.mid_block.resnets.0.norm2.bias', 'module.model.mid_block.resnets.0.conv2.weight', 'module.model.mid_block.resnets.0.conv2.bias', 'module.model.mid_block.resnets.1.norm1.weight', 'module.model.mid_block.resnets.1.norm1.bias', 'module.model.mid_block.resnets.1.conv1.weight', 'module.model.mid_block.resnets.1.conv1.bias', 'module.model.mid_block.resnets.1.time_emb_proj.weight', 'module.model.mid_block.resnets.1.time_emb_proj.bias', 'module.model.mid_block.resnets.1.norm2.weight', 'module.model.mid_block.resnets.1.norm2.bias', 'module.model.mid_block.resnets.1.conv2.weight', 'module.model.mid_block.resnets.1.conv2.bias', 'module.model.conv_norm_out.weight', 'module.model.conv_norm_out.bias', 'module.model.conv_out.weight', 'module.model.conv_out.bias'], unexpected_keys=['model.conv_in.weight', 'model.conv_in.bias', 'model.time_embedding.linear_1.weight', 'model.time_embedding.linear_1.bias', 'model.time_embedding.linear_2.weight', 'model.time_embedding.linear_2.bias', 'model.down_blocks.0.attentions.0.norm.weight', 'model.down_blocks.0.attentions.0.norm.bias', 'model.down_blocks.0.attentions.0.proj_in.weight', 'model.down_blocks.0.attentions.0.proj_in.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.0.attentions.0.proj_out.weight', 'model.down_blocks.0.attentions.0.proj_out.bias', 'model.down_blocks.0.attentions.1.norm.weight', 'model.down_blocks.0.attentions.1.norm.bias', 'model.down_blocks.0.attentions.1.proj_in.weight', 'model.down_blocks.0.attentions.1.proj_in.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.0.attentions.1.proj_out.weight', 'model.down_blocks.0.attentions.1.proj_out.bias', 'model.down_blocks.0.resnets.0.norm1.weight', 'model.down_blocks.0.resnets.0.norm1.bias', 'model.down_blocks.0.resnets.0.conv1.weight', 'model.down_blocks.0.resnets.0.conv1.bias', 'model.down_blocks.0.resnets.0.time_emb_proj.weight', 'model.down_blocks.0.resnets.0.time_emb_proj.bias', 'model.down_blocks.0.resnets.0.norm2.weight', 'model.down_blocks.0.resnets.0.norm2.bias', 'model.down_blocks.0.resnets.0.conv2.weight', 'model.down_blocks.0.resnets.0.conv2.bias', 'model.down_blocks.0.resnets.1.norm1.weight', 'model.down_blocks.0.resnets.1.norm1.bias', 'model.down_blocks.0.resnets.1.conv1.weight', 'model.down_blocks.0.resnets.1.conv1.bias', 'model.down_blocks.0.resnets.1.time_emb_proj.weight', 'model.down_blocks.0.resnets.1.time_emb_proj.bias', 'model.down_blocks.0.resnets.1.norm2.weight', 'model.down_blocks.0.resnets.1.norm2.bias', 'model.down_blocks.0.resnets.1.conv2.weight', 'model.down_blocks.0.resnets.1.conv2.bias', 'model.down_blocks.0.downsamplers.0.conv.weight', 'model.down_blocks.0.downsamplers.0.conv.bias', 'model.down_blocks.1.attentions.0.norm.weight', 'model.down_blocks.1.attentions.0.norm.bias', 'model.down_blocks.1.attentions.0.proj_in.weight', 'model.down_blocks.1.attentions.0.proj_in.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.1.attentions.0.proj_out.weight', 'model.down_blocks.1.attentions.0.proj_out.bias', 'model.down_blocks.1.attentions.1.norm.weight', 'model.down_blocks.1.attentions.1.norm.bias', 'model.down_blocks.1.attentions.1.proj_in.weight', 'model.down_blocks.1.attentions.1.proj_in.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.1.attentions.1.proj_out.weight', 'model.down_blocks.1.attentions.1.proj_out.bias', 'model.down_blocks.1.resnets.0.norm1.weight', 'model.down_blocks.1.resnets.0.norm1.bias', 'model.down_blocks.1.resnets.0.conv1.weight', 'model.down_blocks.1.resnets.0.conv1.bias', 'model.down_blocks.1.resnets.0.time_emb_proj.weight', 'model.down_blocks.1.resnets.0.time_emb_proj.bias', 'model.down_blocks.1.resnets.0.norm2.weight', 'model.down_blocks.1.resnets.0.norm2.bias', 'model.down_blocks.1.resnets.0.conv2.weight', 'model.down_blocks.1.resnets.0.conv2.bias', 'model.down_blocks.1.resnets.0.conv_shortcut.weight', 'model.down_blocks.1.resnets.0.conv_shortcut.bias', 'model.down_blocks.1.resnets.1.norm1.weight', 'model.down_blocks.1.resnets.1.norm1.bias', 'model.down_blocks.1.resnets.1.conv1.weight', 'model.down_blocks.1.resnets.1.conv1.bias', 'model.down_blocks.1.resnets.1.time_emb_proj.weight', 'model.down_blocks.1.resnets.1.time_emb_proj.bias', 'model.down_blocks.1.resnets.1.norm2.weight', 'model.down_blocks.1.resnets.1.norm2.bias', 'model.down_blocks.1.resnets.1.conv2.weight', 'model.down_blocks.1.resnets.1.conv2.bias', 'model.down_blocks.1.downsamplers.0.conv.weight', 'model.down_blocks.1.downsamplers.0.conv.bias', 'model.down_blocks.2.attentions.0.norm.weight', 'model.down_blocks.2.attentions.0.norm.bias', 'model.down_blocks.2.attentions.0.proj_in.weight', 'model.down_blocks.2.attentions.0.proj_in.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.2.attentions.0.proj_out.weight', 'model.down_blocks.2.attentions.0.proj_out.bias', 'model.down_blocks.2.attentions.1.norm.weight', 'model.down_blocks.2.attentions.1.norm.bias', 'model.down_blocks.2.attentions.1.proj_in.weight', 'model.down_blocks.2.attentions.1.proj_in.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.2.attentions.1.proj_out.weight', 'model.down_blocks.2.attentions.1.proj_out.bias', 'model.down_blocks.2.resnets.0.norm1.weight', 'model.down_blocks.2.resnets.0.norm1.bias', 'model.down_blocks.2.resnets.0.conv1.weight', 'model.down_blocks.2.resnets.0.conv1.bias', 'model.down_blocks.2.resnets.0.time_emb_proj.weight', 'model.down_blocks.2.resnets.0.time_emb_proj.bias', 'model.down_blocks.2.resnets.0.norm2.weight', 'model.down_blocks.2.resnets.0.norm2.bias', 'model.down_blocks.2.resnets.0.conv2.weight', 'model.down_blocks.2.resnets.0.conv2.bias', 'model.down_blocks.2.resnets.0.conv_shortcut.weight', 'model.down_blocks.2.resnets.0.conv_shortcut.bias', 'model.down_blocks.2.resnets.1.norm1.weight', 'model.down_blocks.2.resnets.1.norm1.bias', 'model.down_blocks.2.resnets.1.conv1.weight', 'model.down_blocks.2.resnets.1.conv1.bias', 'model.down_blocks.2.resnets.1.time_emb_proj.weight', 'model.down_blocks.2.resnets.1.time_emb_proj.bias', 'model.down_blocks.2.resnets.1.norm2.weight', 'model.down_blocks.2.resnets.1.norm2.bias', 'model.down_blocks.2.resnets.1.conv2.weight', 'model.down_blocks.2.resnets.1.conv2.bias', 'model.down_blocks.2.downsamplers.0.conv.weight', 'model.down_blocks.2.downsamplers.0.conv.bias', 'model.down_blocks.3.resnets.0.norm1.weight', 'model.down_blocks.3.resnets.0.norm1.bias', 'model.down_blocks.3.resnets.0.conv1.weight', 'model.down_blocks.3.resnets.0.conv1.bias', 'model.down_blocks.3.resnets.0.time_emb_proj.weight', 'model.down_blocks.3.resnets.0.time_emb_proj.bias', 'model.down_blocks.3.resnets.0.norm2.weight', 'model.down_blocks.3.resnets.0.norm2.bias', 'model.down_blocks.3.resnets.0.conv2.weight', 'model.down_blocks.3.resnets.0.conv2.bias', 'model.down_blocks.3.resnets.1.norm1.weight', 'model.down_blocks.3.resnets.1.norm1.bias', 'model.down_blocks.3.resnets.1.conv1.weight', 'model.down_blocks.3.resnets.1.conv1.bias', 'model.down_blocks.3.resnets.1.time_emb_proj.weight', 'model.down_blocks.3.resnets.1.time_emb_proj.bias', 'model.down_blocks.3.resnets.1.norm2.weight', 'model.down_blocks.3.resnets.1.norm2.bias', 'model.down_blocks.3.resnets.1.conv2.weight', 'model.down_blocks.3.resnets.1.conv2.bias', 'model.up_blocks.0.resnets.0.norm1.weight', 'model.up_blocks.0.resnets.0.norm1.bias', 'model.up_blocks.0.resnets.0.conv1.weight', 'model.up_blocks.0.resnets.0.conv1.bias', 'model.up_blocks.0.resnets.0.time_emb_proj.weight', 'model.up_blocks.0.resnets.0.time_emb_proj.bias', 'model.up_blocks.0.resnets.0.norm2.weight', 'model.up_blocks.0.resnets.0.norm2.bias', 'model.up_blocks.0.resnets.0.conv2.weight', 'model.up_blocks.0.resnets.0.conv2.bias', 'model.up_blocks.0.resnets.0.conv_shortcut.weight', 'model.up_blocks.0.resnets.0.conv_shortcut.bias', 'model.up_blocks.0.resnets.1.norm1.weight', 'model.up_blocks.0.resnets.1.norm1.bias', 'model.up_blocks.0.resnets.1.conv1.weight', 'model.up_blocks.0.resnets.1.conv1.bias', 'model.up_blocks.0.resnets.1.time_emb_proj.weight', 'model.up_blocks.0.resnets.1.time_emb_proj.bias', 'model.up_blocks.0.resnets.1.norm2.weight', 'model.up_blocks.0.resnets.1.norm2.bias', 'model.up_blocks.0.resnets.1.conv2.weight', 'model.up_blocks.0.resnets.1.conv2.bias', 'model.up_blocks.0.resnets.1.conv_shortcut.weight', 'model.up_blocks.0.resnets.1.conv_shortcut.bias', 'model.up_blocks.0.resnets.2.norm1.weight', 'model.up_blocks.0.resnets.2.norm1.bias', 'model.up_blocks.0.resnets.2.conv1.weight', 'model.up_blocks.0.resnets.2.conv1.bias', 'model.up_blocks.0.resnets.2.time_emb_proj.weight', 'model.up_blocks.0.resnets.2.time_emb_proj.bias', 'model.up_blocks.0.resnets.2.norm2.weight', 'model.up_blocks.0.resnets.2.norm2.bias', 'model.up_blocks.0.resnets.2.conv2.weight', 'model.up_blocks.0.resnets.2.conv2.bias', 'model.up_blocks.0.resnets.2.conv_shortcut.weight', 'model.up_blocks.0.resnets.2.conv_shortcut.bias', 'model.up_blocks.0.upsamplers.0.conv.weight', 'model.up_blocks.0.upsamplers.0.conv.bias', 'model.up_blocks.1.attentions.0.norm.weight', 'model.up_blocks.1.attentions.0.norm.bias', 'model.up_blocks.1.attentions.0.proj_in.weight', 'model.up_blocks.1.attentions.0.proj_in.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.0.proj_out.weight', 'model.up_blocks.1.attentions.0.proj_out.bias', 'model.up_blocks.1.attentions.1.norm.weight', 'model.up_blocks.1.attentions.1.norm.bias', 'model.up_blocks.1.attentions.1.proj_in.weight', 'model.up_blocks.1.attentions.1.proj_in.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.1.proj_out.weight', 'model.up_blocks.1.attentions.1.proj_out.bias', 'model.up_blocks.1.attentions.2.norm.weight', 'model.up_blocks.1.attentions.2.norm.bias', 'model.up_blocks.1.attentions.2.proj_in.weight', 'model.up_blocks.1.attentions.2.proj_in.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.2.proj_out.weight', 'model.up_blocks.1.attentions.2.proj_out.bias', 'model.up_blocks.1.resnets.0.norm1.weight', 'model.up_blocks.1.resnets.0.norm1.bias', 'model.up_blocks.1.resnets.0.conv1.weight', 'model.up_blocks.1.resnets.0.conv1.bias', 'model.up_blocks.1.resnets.0.time_emb_proj.weight', 'model.up_blocks.1.resnets.0.time_emb_proj.bias', 'model.up_blocks.1.resnets.0.norm2.weight', 'model.up_blocks.1.resnets.0.norm2.bias', 'model.up_blocks.1.resnets.0.conv2.weight', 'model.up_blocks.1.resnets.0.conv2.bias', 'model.up_blocks.1.resnets.0.conv_shortcut.weight', 'model.up_blocks.1.resnets.0.conv_shortcut.bias', 'model.up_blocks.1.resnets.1.norm1.weight', 'model.up_blocks.1.resnets.1.norm1.bias', 'model.up_blocks.1.resnets.1.conv1.weight', 'model.up_blocks.1.resnets.1.conv1.bias', 'model.up_blocks.1.resnets.1.time_emb_proj.weight', 'model.up_blocks.1.resnets.1.time_emb_proj.bias', 'model.up_blocks.1.resnets.1.norm2.weight', 'model.up_blocks.1.resnets.1.norm2.bias', 'model.up_blocks.1.resnets.1.conv2.weight', 'model.up_blocks.1.resnets.1.conv2.bias', 'model.up_blocks.1.resnets.1.conv_shortcut.weight', 'model.up_blocks.1.resnets.1.conv_shortcut.bias', 'model.up_blocks.1.resnets.2.norm1.weight', 'model.up_blocks.1.resnets.2.norm1.bias', 'model.up_blocks.1.resnets.2.conv1.weight', 'model.up_blocks.1.resnets.2.conv1.bias', 'model.up_blocks.1.resnets.2.time_emb_proj.weight', 'model.up_blocks.1.resnets.2.time_emb_proj.bias', 'model.up_blocks.1.resnets.2.norm2.weight', 'model.up_blocks.1.resnets.2.norm2.bias', 'model.up_blocks.1.resnets.2.conv2.weight', 'model.up_blocks.1.resnets.2.conv2.bias', 'model.up_blocks.1.resnets.2.conv_shortcut.weight', 'model.up_blocks.1.resnets.2.conv_shortcut.bias', 'model.up_blocks.1.upsamplers.0.conv.weight', 'model.up_blocks.1.upsamplers.0.conv.bias', 'model.up_blocks.2.attentions.0.norm.weight', 'model.up_blocks.2.attentions.0.norm.bias', 'model.up_blocks.2.attentions.0.proj_in.weight', 'model.up_blocks.2.attentions.0.proj_in.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.0.proj_out.weight', 'model.up_blocks.2.attentions.0.proj_out.bias', 'model.up_blocks.2.attentions.1.norm.weight', 'model.up_blocks.2.attentions.1.norm.bias', 'model.up_blocks.2.attentions.1.proj_in.weight', 'model.up_blocks.2.attentions.1.proj_in.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.1.proj_out.weight', 'model.up_blocks.2.attentions.1.proj_out.bias', 'model.up_blocks.2.attentions.2.norm.weight', 'model.up_blocks.2.attentions.2.norm.bias', 'model.up_blocks.2.attentions.2.proj_in.weight', 'model.up_blocks.2.attentions.2.proj_in.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.2.proj_out.weight', 'model.up_blocks.2.attentions.2.proj_out.bias', 'model.up_blocks.2.resnets.0.norm1.weight', 'model.up_blocks.2.resnets.0.norm1.bias', 'model.up_blocks.2.resnets.0.conv1.weight', 'model.up_blocks.2.resnets.0.conv1.bias', 'model.up_blocks.2.resnets.0.time_emb_proj.weight', 'model.up_blocks.2.resnets.0.time_emb_proj.bias', 'model.up_blocks.2.resnets.0.norm2.weight', 'model.up_blocks.2.resnets.0.norm2.bias', 'model.up_blocks.2.resnets.0.conv2.weight', 'model.up_blocks.2.resnets.0.conv2.bias', 'model.up_blocks.2.resnets.0.conv_shortcut.weight', 'model.up_blocks.2.resnets.0.conv_shortcut.bias', 'model.up_blocks.2.resnets.1.norm1.weight', 'model.up_blocks.2.resnets.1.norm1.bias', 'model.up_blocks.2.resnets.1.conv1.weight', 'model.up_blocks.2.resnets.1.conv1.bias', 'model.up_blocks.2.resnets.1.time_emb_proj.weight', 'model.up_blocks.2.resnets.1.time_emb_proj.bias', 'model.up_blocks.2.resnets.1.norm2.weight', 'model.up_blocks.2.resnets.1.norm2.bias', 'model.up_blocks.2.resnets.1.conv2.weight', 'model.up_blocks.2.resnets.1.conv2.bias', 'model.up_blocks.2.resnets.1.conv_shortcut.weight', 'model.up_blocks.2.resnets.1.conv_shortcut.bias', 'model.up_blocks.2.resnets.2.norm1.weight', 'model.up_blocks.2.resnets.2.norm1.bias', 'model.up_blocks.2.resnets.2.conv1.weight', 'model.up_blocks.2.resnets.2.conv1.bias', 'model.up_blocks.2.resnets.2.time_emb_proj.weight', 'model.up_blocks.2.resnets.2.time_emb_proj.bias', 'model.up_blocks.2.resnets.2.norm2.weight', 'model.up_blocks.2.resnets.2.norm2.bias', 'model.up_blocks.2.resnets.2.conv2.weight', 'model.up_blocks.2.resnets.2.conv2.bias', 'model.up_blocks.2.resnets.2.conv_shortcut.weight', 'model.up_blocks.2.resnets.2.conv_shortcut.bias', 'model.up_blocks.2.upsamplers.0.conv.weight', 'model.up_blocks.2.upsamplers.0.conv.bias', 'model.up_blocks.3.attentions.0.norm.weight', 'model.up_blocks.3.attentions.0.norm.bias', 'model.up_blocks.3.attentions.0.proj_in.weight', 'model.up_blocks.3.attentions.0.proj_in.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.0.proj_out.weight', 'model.up_blocks.3.attentions.0.proj_out.bias', 'model.up_blocks.3.attentions.1.norm.weight', 'model.up_blocks.3.attentions.1.norm.bias', 'model.up_blocks.3.attentions.1.proj_in.weight', 'model.up_blocks.3.attentions.1.proj_in.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.1.proj_out.weight', 'model.up_blocks.3.attentions.1.proj_out.bias', 'model.up_blocks.3.attentions.2.norm.weight', 'model.up_blocks.3.attentions.2.norm.bias', 'model.up_blocks.3.attentions.2.proj_in.weight', 'model.up_blocks.3.attentions.2.proj_in.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.2.proj_out.weight', 'model.up_blocks.3.attentions.2.proj_out.bias', 'model.up_blocks.3.resnets.0.norm1.weight', 'model.up_blocks.3.resnets.0.norm1.bias', 'model.up_blocks.3.resnets.0.conv1.weight', 'model.up_blocks.3.resnets.0.conv1.bias', 'model.up_blocks.3.resnets.0.time_emb_proj.weight', 'model.up_blocks.3.resnets.0.time_emb_proj.bias', 'model.up_blocks.3.resnets.0.norm2.weight', 'model.up_blocks.3.resnets.0.norm2.bias', 'model.up_blocks.3.resnets.0.conv2.weight', 'model.up_blocks.3.resnets.0.conv2.bias', 'model.up_blocks.3.resnets.0.conv_shortcut.weight', 'model.up_blocks.3.resnets.0.conv_shortcut.bias', 'model.up_blocks.3.resnets.1.norm1.weight', 'model.up_blocks.3.resnets.1.norm1.bias', 'model.up_blocks.3.resnets.1.conv1.weight', 'model.up_blocks.3.resnets.1.conv1.bias', 'model.up_blocks.3.resnets.1.time_emb_proj.weight', 'model.up_blocks.3.resnets.1.time_emb_proj.bias', 'model.up_blocks.3.resnets.1.norm2.weight', 'model.up_blocks.3.resnets.1.norm2.bias', 'model.up_blocks.3.resnets.1.conv2.weight', 'model.up_blocks.3.resnets.1.conv2.bias', 'model.up_blocks.3.resnets.1.conv_shortcut.weight', 'model.up_blocks.3.resnets.1.conv_shortcut.bias', 'model.up_blocks.3.resnets.2.norm1.weight', 'model.up_blocks.3.resnets.2.norm1.bias', 'model.up_blocks.3.resnets.2.conv1.weight', 'model.up_blocks.3.resnets.2.conv1.bias', 'model.up_blocks.3.resnets.2.time_emb_proj.weight', 'model.up_blocks.3.resnets.2.time_emb_proj.bias', 'model.up_blocks.3.resnets.2.norm2.weight', 'model.up_blocks.3.resnets.2.norm2.bias', 'model.up_blocks.3.resnets.2.conv2.weight', 'model.up_blocks.3.resnets.2.conv2.bias', 'model.up_blocks.3.resnets.2.conv_shortcut.weight', 'model.up_blocks.3.resnets.2.conv_shortcut.bias', 'model.mid_block.attentions.0.norm.weight', 'model.mid_block.attentions.0.norm.bias', 'model.mid_block.attentions.0.proj_in.weight', 'model.mid_block.attentions.0.proj_in.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.mid_block.attentions.0.proj_out.weight', 'model.mid_block.attentions.0.proj_out.bias', 'model.mid_block.resnets.0.norm1.weight', 'model.mid_block.resnets.0.norm1.bias', 'model.mid_block.resnets.0.conv1.weight', 'model.mid_block.resnets.0.conv1.bias', 'model.mid_block.resnets.0.time_emb_proj.weight', 'model.mid_block.resnets.0.time_emb_proj.bias', 'model.mid_block.resnets.0.norm2.weight', 'model.mid_block.resnets.0.norm2.bias', 'model.mid_block.resnets.0.conv2.weight', 'model.mid_block.resnets.0.conv2.bias', 'model.mid_block.resnets.1.norm1.weight', 'model.mid_block.resnets.1.norm1.bias', 'model.mid_block.resnets.1.conv1.weight', 'model.mid_block.resnets.1.conv1.bias', 'model.mid_block.resnets.1.time_emb_proj.weight', 'model.mid_block.resnets.1.time_emb_proj.bias', 'model.mid_block.resnets.1.norm2.weight', 'model.mid_block.resnets.1.norm2.bias', 'model.mid_block.resnets.1.conv2.weight', 'model.mid_block.resnets.1.conv2.bias', 'model.conv_norm_out.weight', 'model.conv_norm_out.bias', 'model.conv_out.weight', 'model.conv_out.bias'])
[2024-03-19 11:25:03 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 11:25:03 pose_transfer_train.py:394] start training...
[2024-03-19 11:25:03 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 11:30:06 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:30:06 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:30:15 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:30:18 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:30:30 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:30:30 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 11:30:30 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 11:30:32 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 11:30:33 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 11:30:36 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 11:30:36 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 11:30:36 pose_transfer_train.py:394] start training...
[2024-03-19 11:30:36 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 11:34:35 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:34:35 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:34:44 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:34:47 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:34:59 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:34:59 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 11:34:59 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 11:35:01 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 11:35:02 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 11:35:05 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 11:35:05 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 11:35:05 pose_transfer_train.py:394] start training...
[2024-03-19 11:35:05 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 11:44:07 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:44:07 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:44:17 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:44:20 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:44:32 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:44:32 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 11:44:32 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 11:44:35 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 11:44:35 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 11:44:38 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 11:44:38 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 11:44:38 pose_transfer_train.py:394] start training...
[2024-03-19 11:44:38 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 11:57:12 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:57:12 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:57:20 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:57:23 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:57:35 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:57:35 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 11:57:35 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 11:57:38 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 11:57:39 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 11:57:42 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 11:57:42 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 11:57:42 pose_transfer_train.py:394] start training...
[2024-03-19 11:57:42 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 11:59:29 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 11:59:29 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 11:59:38 pose_transfer_train.py:298] preparing model...
[2024-03-19 11:59:41 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 11:59:53 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 11:59:53 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 11:59:53 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 11:59:56 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 11:59:57 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 11:59:59 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 11:59:59 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 11:59:59 pose_transfer_train.py:394] start training...
[2024-03-19 11:59:59 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 12:05:44 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 12:05:44 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 12:05:53 pose_transfer_train.py:298] preparing model...
[2024-03-19 12:05:56 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 12:06:08 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 12:06:08 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 12:06:08 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 12:06:11 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 12:06:12 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 12:06:15 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 12:06:15 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 12:06:15 pose_transfer_train.py:394] start training...
[2024-03-19 12:06:15 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 12:07:36 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 12:07:36 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 12:07:45 pose_transfer_train.py:298] preparing model...
[2024-03-19 12:07:48 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 12:08:00 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 12:08:00 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 12:08:00 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 12:08:03 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 12:08:03 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 12:08:06 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 12:08:06 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 12:08:06 pose_transfer_train.py:394] start training...
[2024-03-19 12:08:06 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 12:13:45 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 12:13:45 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 12:13:54 pose_transfer_train.py:298] preparing model...
[2024-03-19 12:13:57 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 12:14:10 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 12:14:10 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 12:14:10 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 12:14:12 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 12:14:13 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 12:14:16 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 12:14:16 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 12:14:16 pose_transfer_train.py:394] start training...
[2024-03-19 12:14:16 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 12:20:04 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 12:20:04 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 12:20:14 pose_transfer_train.py:298] preparing model...
[2024-03-19 12:20:17 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 12:20:29 pose_transfer_train.py:342] number of trainable parameters: 302650616
[2024-03-19 12:20:29 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 12:20:29 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 12:20:32 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 12:20:32 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 12:20:35 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 12:20:35 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 12:20:35 pose_transfer_train.py:394] start training...
[2024-03-19 12:20:35 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 13:12:58 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 13:12:58 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 13:13:06 pose_transfer_train.py:298] preparing model...
[2024-03-19 13:13:09 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 13:13:22 pose_transfer_train.py:342] number of trainable parameters: 334412856
[2024-03-19 13:13:22 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 13:13:22 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 13:13:24 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 13:13:24 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 13:13:27 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 13:13:27 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 13:13:27 pose_transfer_train.py:394] start training...
[2024-03-19 13:13:27 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 13:22:48 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 13:22:48 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 13:22:57 pose_transfer_train.py:298] preparing model...
[2024-03-19 13:23:00 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 13:23:14 pose_transfer_train.py:342] number of trainable parameters: 334412856
[2024-03-19 13:23:14 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 13:23:14 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 13:23:16 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 13:23:17 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 13:23:20 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 13:23:20 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 13:23:20 pose_transfer_train.py:394] start training...
[2024-03-19 13:23:20 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 13:49:37 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 13:49:37 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 13:49:45 pose_transfer_train.py:298] preparing model...
[2024-03-19 13:49:48 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 13:50:00 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 13:50:00 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 13:50:00 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 13:50:03 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 13:50:04 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 13:50:06 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 13:50:06 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 13:50:06 pose_transfer_train.py:394] start training...
[2024-03-19 13:50:06 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 13:52:07 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 13:52:07 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 13:52:16 pose_transfer_train.py:298] preparing model...
[2024-03-19 13:52:19 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 13:52:31 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 13:52:31 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 13:52:31 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 13:52:33 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 13:52:34 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 13:52:36 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 13:52:36 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 13:52:36 pose_transfer_train.py:394] start training...
[2024-03-19 13:52:36 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 13:58:38 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 13:58:38 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 13:58:46 pose_transfer_train.py:298] preparing model...
[2024-03-19 13:58:49 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 13:59:01 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 13:59:01 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 13:59:01 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 13:59:04 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 13:59:05 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 13:59:08 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 13:59:08 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 13:59:08 pose_transfer_train.py:394] start training...
[2024-03-19 13:59:08 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 14:16:20 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 14:16:20 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 14:16:29 pose_transfer_train.py:298] preparing model...
[2024-03-19 14:16:32 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 14:16:44 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 14:16:44 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 14:16:44 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 14:16:46 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 14:16:47 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 14:16:50 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 14:16:50 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 14:16:50 pose_transfer_train.py:394] start training...
[2024-03-19 14:16:50 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 17:24:48 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 17:24:48 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 17:24:57 pose_transfer_train.py:298] preparing model...
[2024-03-19 17:25:00 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 17:25:12 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 17:25:12 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 17:25:12 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 17:25:14 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 17:25:15 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 17:25:17 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 17:25:17 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 17:25:17 pose_transfer_train.py:394] start training...
[2024-03-19 17:25:17 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 18:09:36 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 18:09:36 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 18:09:45 pose_transfer_train.py:298] preparing model...
[2024-03-19 18:09:48 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 18:10:00 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 18:10:00 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 18:10:00 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 18:10:02 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 18:10:03 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 18:10:06 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 18:10:06 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 18:10:06 pose_transfer_train.py:394] start training...
[2024-03-19 18:10:06 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 18:14:36 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 18:14:36 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 18:14:44 pose_transfer_train.py:298] preparing model...
[2024-03-19 18:14:47 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 18:14:59 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 18:14:59 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 18:14:59 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 18:15:02 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 18:15:02 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 18:15:05 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 18:15:05 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 18:15:05 pose_transfer_train.py:394] start training...
[2024-03-19 18:15:05 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 18:17:33 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 18:17:33 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 18:17:42 pose_transfer_train.py:298] preparing model...
[2024-03-19 18:17:45 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 18:17:57 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 18:17:57 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 18:17:57 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 18:17:59 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 18:18:00 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 18:18:03 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 18:18:03 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 18:18:03 pose_transfer_train.py:394] start training...
[2024-03-19 18:18:03 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 18:21:05 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 18:21:05 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 18:21:14 pose_transfer_train.py:298] preparing model...
[2024-03-19 18:21:17 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 18:21:30 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 18:21:30 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 18:21:30 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 18:21:32 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 18:21:32 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 18:21:35 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 18:21:35 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 18:21:35 pose_transfer_train.py:394] start training...
[2024-03-19 18:21:35 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 18:24:37 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 18:24:37 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 18:24:46 pose_transfer_train.py:298] preparing model...
[2024-03-19 18:24:49 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 18:25:01 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 18:25:01 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 18:25:01 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 18:25:04 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 18:25:04 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 18:25:07 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 18:25:07 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 18:25:07 pose_transfer_train.py:394] start training...
[2024-03-19 18:25:07 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 18:26:37 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 18:26:37 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 18:26:46 pose_transfer_train.py:298] preparing model...
[2024-03-19 18:26:49 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 18:27:01 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 18:27:01 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 18:27:01 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 18:27:03 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 18:27:04 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 18:27:07 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 18:27:07 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 18:27:07 pose_transfer_train.py:394] start training...
[2024-03-19 18:27:07 pose_transfer_train.py:403] epoch 1 start
[2024-03-19 18:36:23 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-19 18:36:23 pose_transfer_train.py:275] preparing datasets...
[2024-03-19 18:36:32 pose_transfer_train.py:298] preparing model...
[2024-03-19 18:36:35 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-19 18:36:47 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-19 18:36:47 pose_transfer_train.py:356] preparing optimizer...
[2024-03-19 18:36:47 pose_transfer_train.py:362] preparing accelerator...
[2024-03-19 18:36:49 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-19 18:36:50 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-19 18:36:52 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-19 18:36:52 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-19 18:36:52 pose_transfer_train.py:394] start training...
[2024-03-19 18:36:52 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 01:11:12 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 01:11:12 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 01:11:21 pose_transfer_train.py:298] preparing model...
[2024-03-20 01:11:24 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 01:11:36 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-20 01:11:36 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 01:11:36 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 01:11:38 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 01:11:38 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 01:11:41 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 01:11:41 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 01:11:41 pose_transfer_train.py:394] start training...
[2024-03-20 01:11:41 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 01:26:37 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 01:26:37 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 01:26:45 pose_transfer_train.py:298] preparing model...
[2024-03-20 01:26:49 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 01:27:00 pose_transfer_train.py:342] number of trainable parameters: 299063736
[2024-03-20 01:27:00 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 01:27:00 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 01:27:02 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 01:27:03 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 01:27:06 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 01:27:06 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 01:27:06 pose_transfer_train.py:394] start training...
[2024-03-20 01:27:06 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 04:28:16 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 04:28:16 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 04:28:26 pose_transfer_train.py:298] preparing model...
[2024-03-20 04:28:29 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 04:28:41 pose_transfer_train.py:342] number of trainable parameters: 213327096
[2024-03-20 04:28:41 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 04:28:41 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 04:28:43 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 04:28:43 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 04:28:46 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 04:28:46 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 04:28:46 pose_transfer_train.py:394] start training...
[2024-03-20 04:28:46 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 04:34:21 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 04:34:21 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 04:34:31 pose_transfer_train.py:298] preparing model...
[2024-03-20 04:34:35 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 04:34:47 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 04:34:47 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 04:34:47 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 04:34:49 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 04:34:50 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 04:34:53 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 04:34:53 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 04:34:53 pose_transfer_train.py:394] start training...
[2024-03-20 04:34:53 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 05:02:46 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 05:02:46 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 05:02:57 pose_transfer_train.py:298] preparing model...
[2024-03-20 05:03:03 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 05:03:16 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 05:03:17 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 05:03:17 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 05:03:19 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 05:03:20 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 05:03:23 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 05:03:23 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 05:03:23 pose_transfer_train.py:394] start training...
[2024-03-20 05:03:23 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 05:06:39 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 05:06:39 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 05:06:49 pose_transfer_train.py:298] preparing model...
[2024-03-20 05:06:52 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 05:07:03 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 05:07:03 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 05:07:03 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 05:07:05 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 05:07:06 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 05:07:09 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 05:07:09 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 05:07:09 pose_transfer_train.py:394] start training...
[2024-03-20 05:07:09 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 05:14:07 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 05:14:07 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 05:14:18 pose_transfer_train.py:298] preparing model...
[2024-03-20 05:14:23 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 05:14:36 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 05:14:36 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 05:14:36 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 05:14:39 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 05:14:39 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 05:14:42 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 05:14:42 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 05:14:42 pose_transfer_train.py:394] start training...
[2024-03-20 05:14:42 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 05:17:26 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 05:17:26 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 05:17:35 pose_transfer_train.py:298] preparing model...
[2024-03-20 05:17:39 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 05:17:50 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 05:17:50 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 05:17:50 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 05:17:55 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 05:17:56 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 05:17:59 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 05:17:59 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 05:17:59 pose_transfer_train.py:394] start training...
[2024-03-20 05:17:59 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 05:44:40 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 05:44:40 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 05:44:49 pose_transfer_train.py:298] preparing model...
[2024-03-20 05:44:52 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 05:45:04 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 05:45:04 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 05:45:04 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 05:45:06 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 05:45:06 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 05:45:10 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 05:45:10 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 05:45:10 pose_transfer_train.py:394] start training...
[2024-03-20 05:45:10 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 06:18:33 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 06:18:33 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 06:18:45 pose_transfer_train.py:298] preparing model...
[2024-03-20 06:18:49 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 06:19:01 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 06:19:01 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 06:19:01 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 06:19:04 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 06:19:05 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 06:19:11 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 06:19:11 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 06:19:11 pose_transfer_train.py:394] start training...
[2024-03-20 06:19:11 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 06:21:50 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 06:21:50 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 06:22:00 pose_transfer_train.py:298] preparing model...
[2024-03-20 06:22:04 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 06:22:15 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 06:22:15 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 06:22:15 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 06:22:17 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 06:22:18 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 06:22:21 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 06:22:21 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 06:22:21 pose_transfer_train.py:394] start training...
[2024-03-20 06:22:21 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 06:34:37 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 06:34:37 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 06:34:47 pose_transfer_train.py:298] preparing model...
[2024-03-20 06:34:51 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 06:35:03 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 06:35:03 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 06:35:03 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 06:35:07 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 06:35:08 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 06:35:11 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 06:35:11 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 06:35:11 pose_transfer_train.py:394] start training...
[2024-03-20 06:35:11 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 07:05:07 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:05:07 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:05:17 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:05:20 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:05:32 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 07:05:32 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 07:05:32 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 07:05:36 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 07:05:37 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 07:05:40 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 07:05:40 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 07:05:40 pose_transfer_train.py:394] start training...
[2024-03-20 07:05:40 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 07:09:39 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:09:39 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:09:48 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:09:52 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:10:04 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 07:10:04 pose_transfer_train.py:356] preparing optimizer...
[2024-03-20 07:10:04 pose_transfer_train.py:362] preparing accelerator...
[2024-03-20 07:10:07 pose_transfer_train.py:369] loading states from ./checkpoints/
[2024-03-20 07:10:07 pose_transfer_train.py:379] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 07:10:10 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 07:10:10 pose_transfer_train.py:388] preparing lr scheduler...
[2024-03-20 07:10:10 pose_transfer_train.py:394] start training...
[2024-03-20 07:10:10 pose_transfer_train.py:403] epoch 1 start
[2024-03-20 07:14:39 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:14:39 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:14:49 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:14:52 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:15:05 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 07:15:05 pose_transfer_train.py:361] preparing optimizer...
[2024-03-20 07:15:05 pose_transfer_train.py:367] preparing accelerator...
[2024-03-20 07:15:08 pose_transfer_train.py:374] loading states from ./checkpoints/
[2024-03-20 07:15:09 pose_transfer_train.py:384] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 07:15:12 pose_transfer_train.py:387] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 07:15:12 pose_transfer_train.py:393] preparing lr scheduler...
[2024-03-20 07:15:12 pose_transfer_train.py:399] start training...
[2024-03-20 07:15:12 pose_transfer_train.py:408] epoch 1 start
[2024-03-20 07:21:02 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:21:02 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:21:12 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:21:15 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:22:49 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:22:49 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:23:01 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:23:05 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:23:20 pose_transfer_train.py:342] number of trainable parameters: 213329208
[2024-03-20 07:23:20 pose_transfer_train.py:361] preparing optimizer...
[2024-03-20 07:23:20 pose_transfer_train.py:367] preparing accelerator...
[2024-03-20 07:23:22 pose_transfer_train.py:374] loading states from ./checkpoints/
[2024-03-20 07:23:23 pose_transfer_train.py:384] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 07:23:26 pose_transfer_train.py:387] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 07:23:26 pose_transfer_train.py:393] preparing lr scheduler...
[2024-03-20 07:23:26 pose_transfer_train.py:399] start training...
[2024-03-20 07:23:26 pose_transfer_train.py:408] epoch 1 start
[2024-03-20 07:29:59 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:29:59 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:30:09 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:30:13 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:32:29 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:32:29 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:32:38 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:32:42 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:32:57 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 07:32:57 pose_transfer_train.py:363] preparing optimizer...
[2024-03-20 07:32:57 pose_transfer_train.py:369] preparing accelerator...
[2024-03-20 07:32:57 pose_transfer_train.py:376] loading states from ./checkpoints/
[2024-03-20 07:32:58 pose_transfer_train.py:386] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 07:33:01 pose_transfer_train.py:389] _IncompatibleKeys(missing_keys=['module.model.conv_in.weight', 'module.model.conv_in.bias', 'module.model.time_embedding.linear_1.weight', 'module.model.time_embedding.linear_1.bias', 'module.model.time_embedding.linear_2.weight', 'module.model.time_embedding.linear_2.bias', 'module.model.down_blocks.0.attentions.0.norm.weight', 'module.model.down_blocks.0.attentions.0.norm.bias', 'module.model.down_blocks.0.attentions.0.proj_in.weight', 'module.model.down_blocks.0.attentions.0.proj_in.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.0.attentions.0.proj_out.weight', 'module.model.down_blocks.0.attentions.0.proj_out.bias', 'module.model.down_blocks.0.attentions.1.norm.weight', 'module.model.down_blocks.0.attentions.1.norm.bias', 'module.model.down_blocks.0.attentions.1.proj_in.weight', 'module.model.down_blocks.0.attentions.1.proj_in.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.0.attentions.1.proj_out.weight', 'module.model.down_blocks.0.attentions.1.proj_out.bias', 'module.model.down_blocks.0.resnets.0.norm1.weight', 'module.model.down_blocks.0.resnets.0.norm1.bias', 'module.model.down_blocks.0.resnets.0.conv1.weight', 'module.model.down_blocks.0.resnets.0.conv1.bias', 'module.model.down_blocks.0.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.0.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.0.resnets.0.norm2.weight', 'module.model.down_blocks.0.resnets.0.norm2.bias', 'module.model.down_blocks.0.resnets.0.conv2.weight', 'module.model.down_blocks.0.resnets.0.conv2.bias', 'module.model.down_blocks.0.resnets.1.norm1.weight', 'module.model.down_blocks.0.resnets.1.norm1.bias', 'module.model.down_blocks.0.resnets.1.conv1.weight', 'module.model.down_blocks.0.resnets.1.conv1.bias', 'module.model.down_blocks.0.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.0.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.0.resnets.1.norm2.weight', 'module.model.down_blocks.0.resnets.1.norm2.bias', 'module.model.down_blocks.0.resnets.1.conv2.weight', 'module.model.down_blocks.0.resnets.1.conv2.bias', 'module.model.down_blocks.0.downsamplers.0.conv.weight', 'module.model.down_blocks.0.downsamplers.0.conv.bias', 'module.model.down_blocks.1.attentions.0.norm.weight', 'module.model.down_blocks.1.attentions.0.norm.bias', 'module.model.down_blocks.1.attentions.0.proj_in.weight', 'module.model.down_blocks.1.attentions.0.proj_in.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.1.attentions.0.proj_out.weight', 'module.model.down_blocks.1.attentions.0.proj_out.bias', 'module.model.down_blocks.1.attentions.1.norm.weight', 'module.model.down_blocks.1.attentions.1.norm.bias', 'module.model.down_blocks.1.attentions.1.proj_in.weight', 'module.model.down_blocks.1.attentions.1.proj_in.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.1.attentions.1.proj_out.weight', 'module.model.down_blocks.1.attentions.1.proj_out.bias', 'module.model.down_blocks.1.resnets.0.norm1.weight', 'module.model.down_blocks.1.resnets.0.norm1.bias', 'module.model.down_blocks.1.resnets.0.conv1.weight', 'module.model.down_blocks.1.resnets.0.conv1.bias', 'module.model.down_blocks.1.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.1.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.1.resnets.0.norm2.weight', 'module.model.down_blocks.1.resnets.0.norm2.bias', 'module.model.down_blocks.1.resnets.0.conv2.weight', 'module.model.down_blocks.1.resnets.0.conv2.bias', 'module.model.down_blocks.1.resnets.0.conv_shortcut.weight', 'module.model.down_blocks.1.resnets.0.conv_shortcut.bias', 'module.model.down_blocks.1.resnets.1.norm1.weight', 'module.model.down_blocks.1.resnets.1.norm1.bias', 'module.model.down_blocks.1.resnets.1.conv1.weight', 'module.model.down_blocks.1.resnets.1.conv1.bias', 'module.model.down_blocks.1.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.1.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.1.resnets.1.norm2.weight', 'module.model.down_blocks.1.resnets.1.norm2.bias', 'module.model.down_blocks.1.resnets.1.conv2.weight', 'module.model.down_blocks.1.resnets.1.conv2.bias', 'module.model.down_blocks.1.downsamplers.0.conv.weight', 'module.model.down_blocks.1.downsamplers.0.conv.bias', 'module.model.down_blocks.2.attentions.0.norm.weight', 'module.model.down_blocks.2.attentions.0.norm.bias', 'module.model.down_blocks.2.attentions.0.proj_in.weight', 'module.model.down_blocks.2.attentions.0.proj_in.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.2.attentions.0.proj_out.weight', 'module.model.down_blocks.2.attentions.0.proj_out.bias', 'module.model.down_blocks.2.attentions.1.norm.weight', 'module.model.down_blocks.2.attentions.1.norm.bias', 'module.model.down_blocks.2.attentions.1.proj_in.weight', 'module.model.down_blocks.2.attentions.1.proj_in.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.2.attentions.1.proj_out.weight', 'module.model.down_blocks.2.attentions.1.proj_out.bias', 'module.model.down_blocks.2.resnets.0.norm1.weight', 'module.model.down_blocks.2.resnets.0.norm1.bias', 'module.model.down_blocks.2.resnets.0.conv1.weight', 'module.model.down_blocks.2.resnets.0.conv1.bias', 'module.model.down_blocks.2.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.2.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.2.resnets.0.norm2.weight', 'module.model.down_blocks.2.resnets.0.norm2.bias', 'module.model.down_blocks.2.resnets.0.conv2.weight', 'module.model.down_blocks.2.resnets.0.conv2.bias', 'module.model.down_blocks.2.resnets.0.conv_shortcut.weight', 'module.model.down_blocks.2.resnets.0.conv_shortcut.bias', 'module.model.down_blocks.2.resnets.1.norm1.weight', 'module.model.down_blocks.2.resnets.1.norm1.bias', 'module.model.down_blocks.2.resnets.1.conv1.weight', 'module.model.down_blocks.2.resnets.1.conv1.bias', 'module.model.down_blocks.2.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.2.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.2.resnets.1.norm2.weight', 'module.model.down_blocks.2.resnets.1.norm2.bias', 'module.model.down_blocks.2.resnets.1.conv2.weight', 'module.model.down_blocks.2.resnets.1.conv2.bias', 'module.model.down_blocks.2.downsamplers.0.conv.weight', 'module.model.down_blocks.2.downsamplers.0.conv.bias', 'module.model.down_blocks.3.resnets.0.norm1.weight', 'module.model.down_blocks.3.resnets.0.norm1.bias', 'module.model.down_blocks.3.resnets.0.conv1.weight', 'module.model.down_blocks.3.resnets.0.conv1.bias', 'module.model.down_blocks.3.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.3.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.3.resnets.0.norm2.weight', 'module.model.down_blocks.3.resnets.0.norm2.bias', 'module.model.down_blocks.3.resnets.0.conv2.weight', 'module.model.down_blocks.3.resnets.0.conv2.bias', 'module.model.down_blocks.3.resnets.1.norm1.weight', 'module.model.down_blocks.3.resnets.1.norm1.bias', 'module.model.down_blocks.3.resnets.1.conv1.weight', 'module.model.down_blocks.3.resnets.1.conv1.bias', 'module.model.down_blocks.3.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.3.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.3.resnets.1.norm2.weight', 'module.model.down_blocks.3.resnets.1.norm2.bias', 'module.model.down_blocks.3.resnets.1.conv2.weight', 'module.model.down_blocks.3.resnets.1.conv2.bias', 'module.model.up_blocks.0.resnets.0.norm1.weight', 'module.model.up_blocks.0.resnets.0.norm1.bias', 'module.model.up_blocks.0.resnets.0.conv1.weight', 'module.model.up_blocks.0.resnets.0.conv1.bias', 'module.model.up_blocks.0.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.0.norm2.weight', 'module.model.up_blocks.0.resnets.0.norm2.bias', 'module.model.up_blocks.0.resnets.0.conv2.weight', 'module.model.up_blocks.0.resnets.0.conv2.bias', 'module.model.up_blocks.0.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.0.resnets.1.norm1.weight', 'module.model.up_blocks.0.resnets.1.norm1.bias', 'module.model.up_blocks.0.resnets.1.conv1.weight', 'module.model.up_blocks.0.resnets.1.conv1.bias', 'module.model.up_blocks.0.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.1.norm2.weight', 'module.model.up_blocks.0.resnets.1.norm2.bias', 'module.model.up_blocks.0.resnets.1.conv2.weight', 'module.model.up_blocks.0.resnets.1.conv2.bias', 'module.model.up_blocks.0.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.0.resnets.2.norm1.weight', 'module.model.up_blocks.0.resnets.2.norm1.bias', 'module.model.up_blocks.0.resnets.2.conv1.weight', 'module.model.up_blocks.0.resnets.2.conv1.bias', 'module.model.up_blocks.0.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.2.norm2.weight', 'module.model.up_blocks.0.resnets.2.norm2.bias', 'module.model.up_blocks.0.resnets.2.conv2.weight', 'module.model.up_blocks.0.resnets.2.conv2.bias', 'module.model.up_blocks.0.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.0.upsamplers.0.conv.weight', 'module.model.up_blocks.0.upsamplers.0.conv.bias', 'module.model.up_blocks.1.attentions.0.norm.weight', 'module.model.up_blocks.1.attentions.0.norm.bias', 'module.model.up_blocks.1.attentions.0.proj_in.weight', 'module.model.up_blocks.1.attentions.0.proj_in.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.0.proj_out.weight', 'module.model.up_blocks.1.attentions.0.proj_out.bias', 'module.model.up_blocks.1.attentions.1.norm.weight', 'module.model.up_blocks.1.attentions.1.norm.bias', 'module.model.up_blocks.1.attentions.1.proj_in.weight', 'module.model.up_blocks.1.attentions.1.proj_in.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.1.proj_out.weight', 'module.model.up_blocks.1.attentions.1.proj_out.bias', 'module.model.up_blocks.1.attentions.2.norm.weight', 'module.model.up_blocks.1.attentions.2.norm.bias', 'module.model.up_blocks.1.attentions.2.proj_in.weight', 'module.model.up_blocks.1.attentions.2.proj_in.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.2.proj_out.weight', 'module.model.up_blocks.1.attentions.2.proj_out.bias', 'module.model.up_blocks.1.resnets.0.norm1.weight', 'module.model.up_blocks.1.resnets.0.norm1.bias', 'module.model.up_blocks.1.resnets.0.conv1.weight', 'module.model.up_blocks.1.resnets.0.conv1.bias', 'module.model.up_blocks.1.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.0.norm2.weight', 'module.model.up_blocks.1.resnets.0.norm2.bias', 'module.model.up_blocks.1.resnets.0.conv2.weight', 'module.model.up_blocks.1.resnets.0.conv2.bias', 'module.model.up_blocks.1.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.1.resnets.1.norm1.weight', 'module.model.up_blocks.1.resnets.1.norm1.bias', 'module.model.up_blocks.1.resnets.1.conv1.weight', 'module.model.up_blocks.1.resnets.1.conv1.bias', 'module.model.up_blocks.1.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.1.norm2.weight', 'module.model.up_blocks.1.resnets.1.norm2.bias', 'module.model.up_blocks.1.resnets.1.conv2.weight', 'module.model.up_blocks.1.resnets.1.conv2.bias', 'module.model.up_blocks.1.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.1.resnets.2.norm1.weight', 'module.model.up_blocks.1.resnets.2.norm1.bias', 'module.model.up_blocks.1.resnets.2.conv1.weight', 'module.model.up_blocks.1.resnets.2.conv1.bias', 'module.model.up_blocks.1.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.2.norm2.weight', 'module.model.up_blocks.1.resnets.2.norm2.bias', 'module.model.up_blocks.1.resnets.2.conv2.weight', 'module.model.up_blocks.1.resnets.2.conv2.bias', 'module.model.up_blocks.1.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.1.warpflows.0.norm.weight', 'module.model.up_blocks.1.warpflows.0.norm.bias', 'module.model.up_blocks.1.warpflows.0.proj_in.weight', 'module.model.up_blocks.1.warpflows.0.proj_in.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.0.proj_out.weight', 'module.model.up_blocks.1.warpflows.0.proj_out.bias', 'module.model.up_blocks.1.warpflows.1.norm.weight', 'module.model.up_blocks.1.warpflows.1.norm.bias', 'module.model.up_blocks.1.warpflows.1.proj_in.weight', 'module.model.up_blocks.1.warpflows.1.proj_in.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.1.proj_out.weight', 'module.model.up_blocks.1.warpflows.1.proj_out.bias', 'module.model.up_blocks.1.warpflows.2.norm.weight', 'module.model.up_blocks.1.warpflows.2.norm.bias', 'module.model.up_blocks.1.warpflows.2.proj_in.weight', 'module.model.up_blocks.1.warpflows.2.proj_in.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.2.proj_out.weight', 'module.model.up_blocks.1.warpflows.2.proj_out.bias', 'module.model.up_blocks.1.warpzc.0.weight', 'module.model.up_blocks.1.warpzc.0.bias', 'module.model.up_blocks.1.warpzc.1.weight', 'module.model.up_blocks.1.warpzc.1.bias', 'module.model.up_blocks.1.warpzc.2.weight', 'module.model.up_blocks.1.warpzc.2.bias', 'module.model.up_blocks.1.upsamplers.0.conv.weight', 'module.model.up_blocks.1.upsamplers.0.conv.bias', 'module.model.up_blocks.2.attentions.0.norm.weight', 'module.model.up_blocks.2.attentions.0.norm.bias', 'module.model.up_blocks.2.attentions.0.proj_in.weight', 'module.model.up_blocks.2.attentions.0.proj_in.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.0.proj_out.weight', 'module.model.up_blocks.2.attentions.0.proj_out.bias', 'module.model.up_blocks.2.attentions.1.norm.weight', 'module.model.up_blocks.2.attentions.1.norm.bias', 'module.model.up_blocks.2.attentions.1.proj_in.weight', 'module.model.up_blocks.2.attentions.1.proj_in.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.1.proj_out.weight', 'module.model.up_blocks.2.attentions.1.proj_out.bias', 'module.model.up_blocks.2.attentions.2.norm.weight', 'module.model.up_blocks.2.attentions.2.norm.bias', 'module.model.up_blocks.2.attentions.2.proj_in.weight', 'module.model.up_blocks.2.attentions.2.proj_in.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.2.proj_out.weight', 'module.model.up_blocks.2.attentions.2.proj_out.bias', 'module.model.up_blocks.2.resnets.0.norm1.weight', 'module.model.up_blocks.2.resnets.0.norm1.bias', 'module.model.up_blocks.2.resnets.0.conv1.weight', 'module.model.up_blocks.2.resnets.0.conv1.bias', 'module.model.up_blocks.2.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.0.norm2.weight', 'module.model.up_blocks.2.resnets.0.norm2.bias', 'module.model.up_blocks.2.resnets.0.conv2.weight', 'module.model.up_blocks.2.resnets.0.conv2.bias', 'module.model.up_blocks.2.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.2.resnets.1.norm1.weight', 'module.model.up_blocks.2.resnets.1.norm1.bias', 'module.model.up_blocks.2.resnets.1.conv1.weight', 'module.model.up_blocks.2.resnets.1.conv1.bias', 'module.model.up_blocks.2.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.1.norm2.weight', 'module.model.up_blocks.2.resnets.1.norm2.bias', 'module.model.up_blocks.2.resnets.1.conv2.weight', 'module.model.up_blocks.2.resnets.1.conv2.bias', 'module.model.up_blocks.2.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.2.resnets.2.norm1.weight', 'module.model.up_blocks.2.resnets.2.norm1.bias', 'module.model.up_blocks.2.resnets.2.conv1.weight', 'module.model.up_blocks.2.resnets.2.conv1.bias', 'module.model.up_blocks.2.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.2.norm2.weight', 'module.model.up_blocks.2.resnets.2.norm2.bias', 'module.model.up_blocks.2.resnets.2.conv2.weight', 'module.model.up_blocks.2.resnets.2.conv2.bias', 'module.model.up_blocks.2.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.2.warpflows.0.norm.weight', 'module.model.up_blocks.2.warpflows.0.norm.bias', 'module.model.up_blocks.2.warpflows.0.proj_in.weight', 'module.model.up_blocks.2.warpflows.0.proj_in.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.0.proj_out.weight', 'module.model.up_blocks.2.warpflows.0.proj_out.bias', 'module.model.up_blocks.2.warpflows.1.norm.weight', 'module.model.up_blocks.2.warpflows.1.norm.bias', 'module.model.up_blocks.2.warpflows.1.proj_in.weight', 'module.model.up_blocks.2.warpflows.1.proj_in.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.1.proj_out.weight', 'module.model.up_blocks.2.warpflows.1.proj_out.bias', 'module.model.up_blocks.2.warpflows.2.norm.weight', 'module.model.up_blocks.2.warpflows.2.norm.bias', 'module.model.up_blocks.2.warpflows.2.proj_in.weight', 'module.model.up_blocks.2.warpflows.2.proj_in.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.2.proj_out.weight', 'module.model.up_blocks.2.warpflows.2.proj_out.bias', 'module.model.up_blocks.2.warpzc.0.weight', 'module.model.up_blocks.2.warpzc.0.bias', 'module.model.up_blocks.2.warpzc.1.weight', 'module.model.up_blocks.2.warpzc.1.bias', 'module.model.up_blocks.2.warpzc.2.weight', 'module.model.up_blocks.2.warpzc.2.bias', 'module.model.up_blocks.2.upsamplers.0.conv.weight', 'module.model.up_blocks.2.upsamplers.0.conv.bias', 'module.model.up_blocks.3.attentions.0.norm.weight', 'module.model.up_blocks.3.attentions.0.norm.bias', 'module.model.up_blocks.3.attentions.0.proj_in.weight', 'module.model.up_blocks.3.attentions.0.proj_in.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.0.proj_out.weight', 'module.model.up_blocks.3.attentions.0.proj_out.bias', 'module.model.up_blocks.3.attentions.1.norm.weight', 'module.model.up_blocks.3.attentions.1.norm.bias', 'module.model.up_blocks.3.attentions.1.proj_in.weight', 'module.model.up_blocks.3.attentions.1.proj_in.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.1.proj_out.weight', 'module.model.up_blocks.3.attentions.1.proj_out.bias', 'module.model.up_blocks.3.attentions.2.norm.weight', 'module.model.up_blocks.3.attentions.2.norm.bias', 'module.model.up_blocks.3.attentions.2.proj_in.weight', 'module.model.up_blocks.3.attentions.2.proj_in.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.2.proj_out.weight', 'module.model.up_blocks.3.attentions.2.proj_out.bias', 'module.model.up_blocks.3.resnets.0.norm1.weight', 'module.model.up_blocks.3.resnets.0.norm1.bias', 'module.model.up_blocks.3.resnets.0.conv1.weight', 'module.model.up_blocks.3.resnets.0.conv1.bias', 'module.model.up_blocks.3.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.0.norm2.weight', 'module.model.up_blocks.3.resnets.0.norm2.bias', 'module.model.up_blocks.3.resnets.0.conv2.weight', 'module.model.up_blocks.3.resnets.0.conv2.bias', 'module.model.up_blocks.3.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.3.resnets.1.norm1.weight', 'module.model.up_blocks.3.resnets.1.norm1.bias', 'module.model.up_blocks.3.resnets.1.conv1.weight', 'module.model.up_blocks.3.resnets.1.conv1.bias', 'module.model.up_blocks.3.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.1.norm2.weight', 'module.model.up_blocks.3.resnets.1.norm2.bias', 'module.model.up_blocks.3.resnets.1.conv2.weight', 'module.model.up_blocks.3.resnets.1.conv2.bias', 'module.model.up_blocks.3.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.3.resnets.2.norm1.weight', 'module.model.up_blocks.3.resnets.2.norm1.bias', 'module.model.up_blocks.3.resnets.2.conv1.weight', 'module.model.up_blocks.3.resnets.2.conv1.bias', 'module.model.up_blocks.3.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.2.norm2.weight', 'module.model.up_blocks.3.resnets.2.norm2.bias', 'module.model.up_blocks.3.resnets.2.conv2.weight', 'module.model.up_blocks.3.resnets.2.conv2.bias', 'module.model.up_blocks.3.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.3.warpflows.0.norm.weight', 'module.model.up_blocks.3.warpflows.0.norm.bias', 'module.model.up_blocks.3.warpflows.0.proj_in.weight', 'module.model.up_blocks.3.warpflows.0.proj_in.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.0.proj_out.weight', 'module.model.up_blocks.3.warpflows.0.proj_out.bias', 'module.model.up_blocks.3.warpflows.1.norm.weight', 'module.model.up_blocks.3.warpflows.1.norm.bias', 'module.model.up_blocks.3.warpflows.1.proj_in.weight', 'module.model.up_blocks.3.warpflows.1.proj_in.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.1.proj_out.weight', 'module.model.up_blocks.3.warpflows.1.proj_out.bias', 'module.model.up_blocks.3.warpflows.2.norm.weight', 'module.model.up_blocks.3.warpflows.2.norm.bias', 'module.model.up_blocks.3.warpflows.2.proj_in.weight', 'module.model.up_blocks.3.warpflows.2.proj_in.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.2.proj_out.weight', 'module.model.up_blocks.3.warpflows.2.proj_out.bias', 'module.model.up_blocks.3.warpzc.0.weight', 'module.model.up_blocks.3.warpzc.0.bias', 'module.model.up_blocks.3.warpzc.1.weight', 'module.model.up_blocks.3.warpzc.1.bias', 'module.model.up_blocks.3.warpzc.2.weight', 'module.model.up_blocks.3.warpzc.2.bias', 'module.model.mid_block.attentions.0.norm.weight', 'module.model.mid_block.attentions.0.norm.bias', 'module.model.mid_block.attentions.0.proj_in.weight', 'module.model.mid_block.attentions.0.proj_in.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.mid_block.attentions.0.proj_out.weight', 'module.model.mid_block.attentions.0.proj_out.bias', 'module.model.mid_block.resnets.0.norm1.weight', 'module.model.mid_block.resnets.0.norm1.bias', 'module.model.mid_block.resnets.0.conv1.weight', 'module.model.mid_block.resnets.0.conv1.bias', 'module.model.mid_block.resnets.0.time_emb_proj.weight', 'module.model.mid_block.resnets.0.time_emb_proj.bias', 'module.model.mid_block.resnets.0.norm2.weight', 'module.model.mid_block.resnets.0.norm2.bias', 'module.model.mid_block.resnets.0.conv2.weight', 'module.model.mid_block.resnets.0.conv2.bias', 'module.model.mid_block.resnets.1.norm1.weight', 'module.model.mid_block.resnets.1.norm1.bias', 'module.model.mid_block.resnets.1.conv1.weight', 'module.model.mid_block.resnets.1.conv1.bias', 'module.model.mid_block.resnets.1.time_emb_proj.weight', 'module.model.mid_block.resnets.1.time_emb_proj.bias', 'module.model.mid_block.resnets.1.norm2.weight', 'module.model.mid_block.resnets.1.norm2.bias', 'module.model.mid_block.resnets.1.conv2.weight', 'module.model.mid_block.resnets.1.conv2.bias', 'module.model.conv_norm_out.weight', 'module.model.conv_norm_out.bias', 'module.model.conv_out.weight', 'module.model.conv_out.bias'], unexpected_keys=['model.conv_in.weight', 'model.conv_in.bias', 'model.time_embedding.linear_1.weight', 'model.time_embedding.linear_1.bias', 'model.time_embedding.linear_2.weight', 'model.time_embedding.linear_2.bias', 'model.down_blocks.0.attentions.0.norm.weight', 'model.down_blocks.0.attentions.0.norm.bias', 'model.down_blocks.0.attentions.0.proj_in.weight', 'model.down_blocks.0.attentions.0.proj_in.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.0.attentions.0.proj_out.weight', 'model.down_blocks.0.attentions.0.proj_out.bias', 'model.down_blocks.0.attentions.1.norm.weight', 'model.down_blocks.0.attentions.1.norm.bias', 'model.down_blocks.0.attentions.1.proj_in.weight', 'model.down_blocks.0.attentions.1.proj_in.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.0.attentions.1.proj_out.weight', 'model.down_blocks.0.attentions.1.proj_out.bias', 'model.down_blocks.0.resnets.0.norm1.weight', 'model.down_blocks.0.resnets.0.norm1.bias', 'model.down_blocks.0.resnets.0.conv1.weight', 'model.down_blocks.0.resnets.0.conv1.bias', 'model.down_blocks.0.resnets.0.time_emb_proj.weight', 'model.down_blocks.0.resnets.0.time_emb_proj.bias', 'model.down_blocks.0.resnets.0.norm2.weight', 'model.down_blocks.0.resnets.0.norm2.bias', 'model.down_blocks.0.resnets.0.conv2.weight', 'model.down_blocks.0.resnets.0.conv2.bias', 'model.down_blocks.0.resnets.1.norm1.weight', 'model.down_blocks.0.resnets.1.norm1.bias', 'model.down_blocks.0.resnets.1.conv1.weight', 'model.down_blocks.0.resnets.1.conv1.bias', 'model.down_blocks.0.resnets.1.time_emb_proj.weight', 'model.down_blocks.0.resnets.1.time_emb_proj.bias', 'model.down_blocks.0.resnets.1.norm2.weight', 'model.down_blocks.0.resnets.1.norm2.bias', 'model.down_blocks.0.resnets.1.conv2.weight', 'model.down_blocks.0.resnets.1.conv2.bias', 'model.down_blocks.0.downsamplers.0.conv.weight', 'model.down_blocks.0.downsamplers.0.conv.bias', 'model.down_blocks.1.attentions.0.norm.weight', 'model.down_blocks.1.attentions.0.norm.bias', 'model.down_blocks.1.attentions.0.proj_in.weight', 'model.down_blocks.1.attentions.0.proj_in.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.1.attentions.0.proj_out.weight', 'model.down_blocks.1.attentions.0.proj_out.bias', 'model.down_blocks.1.attentions.1.norm.weight', 'model.down_blocks.1.attentions.1.norm.bias', 'model.down_blocks.1.attentions.1.proj_in.weight', 'model.down_blocks.1.attentions.1.proj_in.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.1.attentions.1.proj_out.weight', 'model.down_blocks.1.attentions.1.proj_out.bias', 'model.down_blocks.1.resnets.0.norm1.weight', 'model.down_blocks.1.resnets.0.norm1.bias', 'model.down_blocks.1.resnets.0.conv1.weight', 'model.down_blocks.1.resnets.0.conv1.bias', 'model.down_blocks.1.resnets.0.time_emb_proj.weight', 'model.down_blocks.1.resnets.0.time_emb_proj.bias', 'model.down_blocks.1.resnets.0.norm2.weight', 'model.down_blocks.1.resnets.0.norm2.bias', 'model.down_blocks.1.resnets.0.conv2.weight', 'model.down_blocks.1.resnets.0.conv2.bias', 'model.down_blocks.1.resnets.0.conv_shortcut.weight', 'model.down_blocks.1.resnets.0.conv_shortcut.bias', 'model.down_blocks.1.resnets.1.norm1.weight', 'model.down_blocks.1.resnets.1.norm1.bias', 'model.down_blocks.1.resnets.1.conv1.weight', 'model.down_blocks.1.resnets.1.conv1.bias', 'model.down_blocks.1.resnets.1.time_emb_proj.weight', 'model.down_blocks.1.resnets.1.time_emb_proj.bias', 'model.down_blocks.1.resnets.1.norm2.weight', 'model.down_blocks.1.resnets.1.norm2.bias', 'model.down_blocks.1.resnets.1.conv2.weight', 'model.down_blocks.1.resnets.1.conv2.bias', 'model.down_blocks.1.downsamplers.0.conv.weight', 'model.down_blocks.1.downsamplers.0.conv.bias', 'model.down_blocks.2.attentions.0.norm.weight', 'model.down_blocks.2.attentions.0.norm.bias', 'model.down_blocks.2.attentions.0.proj_in.weight', 'model.down_blocks.2.attentions.0.proj_in.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.2.attentions.0.proj_out.weight', 'model.down_blocks.2.attentions.0.proj_out.bias', 'model.down_blocks.2.attentions.1.norm.weight', 'model.down_blocks.2.attentions.1.norm.bias', 'model.down_blocks.2.attentions.1.proj_in.weight', 'model.down_blocks.2.attentions.1.proj_in.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.2.attentions.1.proj_out.weight', 'model.down_blocks.2.attentions.1.proj_out.bias', 'model.down_blocks.2.resnets.0.norm1.weight', 'model.down_blocks.2.resnets.0.norm1.bias', 'model.down_blocks.2.resnets.0.conv1.weight', 'model.down_blocks.2.resnets.0.conv1.bias', 'model.down_blocks.2.resnets.0.time_emb_proj.weight', 'model.down_blocks.2.resnets.0.time_emb_proj.bias', 'model.down_blocks.2.resnets.0.norm2.weight', 'model.down_blocks.2.resnets.0.norm2.bias', 'model.down_blocks.2.resnets.0.conv2.weight', 'model.down_blocks.2.resnets.0.conv2.bias', 'model.down_blocks.2.resnets.0.conv_shortcut.weight', 'model.down_blocks.2.resnets.0.conv_shortcut.bias', 'model.down_blocks.2.resnets.1.norm1.weight', 'model.down_blocks.2.resnets.1.norm1.bias', 'model.down_blocks.2.resnets.1.conv1.weight', 'model.down_blocks.2.resnets.1.conv1.bias', 'model.down_blocks.2.resnets.1.time_emb_proj.weight', 'model.down_blocks.2.resnets.1.time_emb_proj.bias', 'model.down_blocks.2.resnets.1.norm2.weight', 'model.down_blocks.2.resnets.1.norm2.bias', 'model.down_blocks.2.resnets.1.conv2.weight', 'model.down_blocks.2.resnets.1.conv2.bias', 'model.down_blocks.2.downsamplers.0.conv.weight', 'model.down_blocks.2.downsamplers.0.conv.bias', 'model.down_blocks.3.resnets.0.norm1.weight', 'model.down_blocks.3.resnets.0.norm1.bias', 'model.down_blocks.3.resnets.0.conv1.weight', 'model.down_blocks.3.resnets.0.conv1.bias', 'model.down_blocks.3.resnets.0.time_emb_proj.weight', 'model.down_blocks.3.resnets.0.time_emb_proj.bias', 'model.down_blocks.3.resnets.0.norm2.weight', 'model.down_blocks.3.resnets.0.norm2.bias', 'model.down_blocks.3.resnets.0.conv2.weight', 'model.down_blocks.3.resnets.0.conv2.bias', 'model.down_blocks.3.resnets.1.norm1.weight', 'model.down_blocks.3.resnets.1.norm1.bias', 'model.down_blocks.3.resnets.1.conv1.weight', 'model.down_blocks.3.resnets.1.conv1.bias', 'model.down_blocks.3.resnets.1.time_emb_proj.weight', 'model.down_blocks.3.resnets.1.time_emb_proj.bias', 'model.down_blocks.3.resnets.1.norm2.weight', 'model.down_blocks.3.resnets.1.norm2.bias', 'model.down_blocks.3.resnets.1.conv2.weight', 'model.down_blocks.3.resnets.1.conv2.bias', 'model.up_blocks.0.resnets.0.norm1.weight', 'model.up_blocks.0.resnets.0.norm1.bias', 'model.up_blocks.0.resnets.0.conv1.weight', 'model.up_blocks.0.resnets.0.conv1.bias', 'model.up_blocks.0.resnets.0.time_emb_proj.weight', 'model.up_blocks.0.resnets.0.time_emb_proj.bias', 'model.up_blocks.0.resnets.0.norm2.weight', 'model.up_blocks.0.resnets.0.norm2.bias', 'model.up_blocks.0.resnets.0.conv2.weight', 'model.up_blocks.0.resnets.0.conv2.bias', 'model.up_blocks.0.resnets.0.conv_shortcut.weight', 'model.up_blocks.0.resnets.0.conv_shortcut.bias', 'model.up_blocks.0.resnets.1.norm1.weight', 'model.up_blocks.0.resnets.1.norm1.bias', 'model.up_blocks.0.resnets.1.conv1.weight', 'model.up_blocks.0.resnets.1.conv1.bias', 'model.up_blocks.0.resnets.1.time_emb_proj.weight', 'model.up_blocks.0.resnets.1.time_emb_proj.bias', 'model.up_blocks.0.resnets.1.norm2.weight', 'model.up_blocks.0.resnets.1.norm2.bias', 'model.up_blocks.0.resnets.1.conv2.weight', 'model.up_blocks.0.resnets.1.conv2.bias', 'model.up_blocks.0.resnets.1.conv_shortcut.weight', 'model.up_blocks.0.resnets.1.conv_shortcut.bias', 'model.up_blocks.0.resnets.2.norm1.weight', 'model.up_blocks.0.resnets.2.norm1.bias', 'model.up_blocks.0.resnets.2.conv1.weight', 'model.up_blocks.0.resnets.2.conv1.bias', 'model.up_blocks.0.resnets.2.time_emb_proj.weight', 'model.up_blocks.0.resnets.2.time_emb_proj.bias', 'model.up_blocks.0.resnets.2.norm2.weight', 'model.up_blocks.0.resnets.2.norm2.bias', 'model.up_blocks.0.resnets.2.conv2.weight', 'model.up_blocks.0.resnets.2.conv2.bias', 'model.up_blocks.0.resnets.2.conv_shortcut.weight', 'model.up_blocks.0.resnets.2.conv_shortcut.bias', 'model.up_blocks.0.upsamplers.0.conv.weight', 'model.up_blocks.0.upsamplers.0.conv.bias', 'model.up_blocks.1.attentions.0.norm.weight', 'model.up_blocks.1.attentions.0.norm.bias', 'model.up_blocks.1.attentions.0.proj_in.weight', 'model.up_blocks.1.attentions.0.proj_in.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.0.proj_out.weight', 'model.up_blocks.1.attentions.0.proj_out.bias', 'model.up_blocks.1.attentions.1.norm.weight', 'model.up_blocks.1.attentions.1.norm.bias', 'model.up_blocks.1.attentions.1.proj_in.weight', 'model.up_blocks.1.attentions.1.proj_in.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.1.proj_out.weight', 'model.up_blocks.1.attentions.1.proj_out.bias', 'model.up_blocks.1.attentions.2.norm.weight', 'model.up_blocks.1.attentions.2.norm.bias', 'model.up_blocks.1.attentions.2.proj_in.weight', 'model.up_blocks.1.attentions.2.proj_in.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.2.proj_out.weight', 'model.up_blocks.1.attentions.2.proj_out.bias', 'model.up_blocks.1.resnets.0.norm1.weight', 'model.up_blocks.1.resnets.0.norm1.bias', 'model.up_blocks.1.resnets.0.conv1.weight', 'model.up_blocks.1.resnets.0.conv1.bias', 'model.up_blocks.1.resnets.0.time_emb_proj.weight', 'model.up_blocks.1.resnets.0.time_emb_proj.bias', 'model.up_blocks.1.resnets.0.norm2.weight', 'model.up_blocks.1.resnets.0.norm2.bias', 'model.up_blocks.1.resnets.0.conv2.weight', 'model.up_blocks.1.resnets.0.conv2.bias', 'model.up_blocks.1.resnets.0.conv_shortcut.weight', 'model.up_blocks.1.resnets.0.conv_shortcut.bias', 'model.up_blocks.1.resnets.1.norm1.weight', 'model.up_blocks.1.resnets.1.norm1.bias', 'model.up_blocks.1.resnets.1.conv1.weight', 'model.up_blocks.1.resnets.1.conv1.bias', 'model.up_blocks.1.resnets.1.time_emb_proj.weight', 'model.up_blocks.1.resnets.1.time_emb_proj.bias', 'model.up_blocks.1.resnets.1.norm2.weight', 'model.up_blocks.1.resnets.1.norm2.bias', 'model.up_blocks.1.resnets.1.conv2.weight', 'model.up_blocks.1.resnets.1.conv2.bias', 'model.up_blocks.1.resnets.1.conv_shortcut.weight', 'model.up_blocks.1.resnets.1.conv_shortcut.bias', 'model.up_blocks.1.resnets.2.norm1.weight', 'model.up_blocks.1.resnets.2.norm1.bias', 'model.up_blocks.1.resnets.2.conv1.weight', 'model.up_blocks.1.resnets.2.conv1.bias', 'model.up_blocks.1.resnets.2.time_emb_proj.weight', 'model.up_blocks.1.resnets.2.time_emb_proj.bias', 'model.up_blocks.1.resnets.2.norm2.weight', 'model.up_blocks.1.resnets.2.norm2.bias', 'model.up_blocks.1.resnets.2.conv2.weight', 'model.up_blocks.1.resnets.2.conv2.bias', 'model.up_blocks.1.resnets.2.conv_shortcut.weight', 'model.up_blocks.1.resnets.2.conv_shortcut.bias', 'model.up_blocks.1.upsamplers.0.conv.weight', 'model.up_blocks.1.upsamplers.0.conv.bias', 'model.up_blocks.2.attentions.0.norm.weight', 'model.up_blocks.2.attentions.0.norm.bias', 'model.up_blocks.2.attentions.0.proj_in.weight', 'model.up_blocks.2.attentions.0.proj_in.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.0.proj_out.weight', 'model.up_blocks.2.attentions.0.proj_out.bias', 'model.up_blocks.2.attentions.1.norm.weight', 'model.up_blocks.2.attentions.1.norm.bias', 'model.up_blocks.2.attentions.1.proj_in.weight', 'model.up_blocks.2.attentions.1.proj_in.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.1.proj_out.weight', 'model.up_blocks.2.attentions.1.proj_out.bias', 'model.up_blocks.2.attentions.2.norm.weight', 'model.up_blocks.2.attentions.2.norm.bias', 'model.up_blocks.2.attentions.2.proj_in.weight', 'model.up_blocks.2.attentions.2.proj_in.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.2.proj_out.weight', 'model.up_blocks.2.attentions.2.proj_out.bias', 'model.up_blocks.2.resnets.0.norm1.weight', 'model.up_blocks.2.resnets.0.norm1.bias', 'model.up_blocks.2.resnets.0.conv1.weight', 'model.up_blocks.2.resnets.0.conv1.bias', 'model.up_blocks.2.resnets.0.time_emb_proj.weight', 'model.up_blocks.2.resnets.0.time_emb_proj.bias', 'model.up_blocks.2.resnets.0.norm2.weight', 'model.up_blocks.2.resnets.0.norm2.bias', 'model.up_blocks.2.resnets.0.conv2.weight', 'model.up_blocks.2.resnets.0.conv2.bias', 'model.up_blocks.2.resnets.0.conv_shortcut.weight', 'model.up_blocks.2.resnets.0.conv_shortcut.bias', 'model.up_blocks.2.resnets.1.norm1.weight', 'model.up_blocks.2.resnets.1.norm1.bias', 'model.up_blocks.2.resnets.1.conv1.weight', 'model.up_blocks.2.resnets.1.conv1.bias', 'model.up_blocks.2.resnets.1.time_emb_proj.weight', 'model.up_blocks.2.resnets.1.time_emb_proj.bias', 'model.up_blocks.2.resnets.1.norm2.weight', 'model.up_blocks.2.resnets.1.norm2.bias', 'model.up_blocks.2.resnets.1.conv2.weight', 'model.up_blocks.2.resnets.1.conv2.bias', 'model.up_blocks.2.resnets.1.conv_shortcut.weight', 'model.up_blocks.2.resnets.1.conv_shortcut.bias', 'model.up_blocks.2.resnets.2.norm1.weight', 'model.up_blocks.2.resnets.2.norm1.bias', 'model.up_blocks.2.resnets.2.conv1.weight', 'model.up_blocks.2.resnets.2.conv1.bias', 'model.up_blocks.2.resnets.2.time_emb_proj.weight', 'model.up_blocks.2.resnets.2.time_emb_proj.bias', 'model.up_blocks.2.resnets.2.norm2.weight', 'model.up_blocks.2.resnets.2.norm2.bias', 'model.up_blocks.2.resnets.2.conv2.weight', 'model.up_blocks.2.resnets.2.conv2.bias', 'model.up_blocks.2.resnets.2.conv_shortcut.weight', 'model.up_blocks.2.resnets.2.conv_shortcut.bias', 'model.up_blocks.2.upsamplers.0.conv.weight', 'model.up_blocks.2.upsamplers.0.conv.bias', 'model.up_blocks.3.attentions.0.norm.weight', 'model.up_blocks.3.attentions.0.norm.bias', 'model.up_blocks.3.attentions.0.proj_in.weight', 'model.up_blocks.3.attentions.0.proj_in.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.0.proj_out.weight', 'model.up_blocks.3.attentions.0.proj_out.bias', 'model.up_blocks.3.attentions.1.norm.weight', 'model.up_blocks.3.attentions.1.norm.bias', 'model.up_blocks.3.attentions.1.proj_in.weight', 'model.up_blocks.3.attentions.1.proj_in.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.1.proj_out.weight', 'model.up_blocks.3.attentions.1.proj_out.bias', 'model.up_blocks.3.attentions.2.norm.weight', 'model.up_blocks.3.attentions.2.norm.bias', 'model.up_blocks.3.attentions.2.proj_in.weight', 'model.up_blocks.3.attentions.2.proj_in.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.2.proj_out.weight', 'model.up_blocks.3.attentions.2.proj_out.bias', 'model.up_blocks.3.resnets.0.norm1.weight', 'model.up_blocks.3.resnets.0.norm1.bias', 'model.up_blocks.3.resnets.0.conv1.weight', 'model.up_blocks.3.resnets.0.conv1.bias', 'model.up_blocks.3.resnets.0.time_emb_proj.weight', 'model.up_blocks.3.resnets.0.time_emb_proj.bias', 'model.up_blocks.3.resnets.0.norm2.weight', 'model.up_blocks.3.resnets.0.norm2.bias', 'model.up_blocks.3.resnets.0.conv2.weight', 'model.up_blocks.3.resnets.0.conv2.bias', 'model.up_blocks.3.resnets.0.conv_shortcut.weight', 'model.up_blocks.3.resnets.0.conv_shortcut.bias', 'model.up_blocks.3.resnets.1.norm1.weight', 'model.up_blocks.3.resnets.1.norm1.bias', 'model.up_blocks.3.resnets.1.conv1.weight', 'model.up_blocks.3.resnets.1.conv1.bias', 'model.up_blocks.3.resnets.1.time_emb_proj.weight', 'model.up_blocks.3.resnets.1.time_emb_proj.bias', 'model.up_blocks.3.resnets.1.norm2.weight', 'model.up_blocks.3.resnets.1.norm2.bias', 'model.up_blocks.3.resnets.1.conv2.weight', 'model.up_blocks.3.resnets.1.conv2.bias', 'model.up_blocks.3.resnets.1.conv_shortcut.weight', 'model.up_blocks.3.resnets.1.conv_shortcut.bias', 'model.up_blocks.3.resnets.2.norm1.weight', 'model.up_blocks.3.resnets.2.norm1.bias', 'model.up_blocks.3.resnets.2.conv1.weight', 'model.up_blocks.3.resnets.2.conv1.bias', 'model.up_blocks.3.resnets.2.time_emb_proj.weight', 'model.up_blocks.3.resnets.2.time_emb_proj.bias', 'model.up_blocks.3.resnets.2.norm2.weight', 'model.up_blocks.3.resnets.2.norm2.bias', 'model.up_blocks.3.resnets.2.conv2.weight', 'model.up_blocks.3.resnets.2.conv2.bias', 'model.up_blocks.3.resnets.2.conv_shortcut.weight', 'model.up_blocks.3.resnets.2.conv_shortcut.bias', 'model.mid_block.attentions.0.norm.weight', 'model.mid_block.attentions.0.norm.bias', 'model.mid_block.attentions.0.proj_in.weight', 'model.mid_block.attentions.0.proj_in.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.mid_block.attentions.0.proj_out.weight', 'model.mid_block.attentions.0.proj_out.bias', 'model.mid_block.resnets.0.norm1.weight', 'model.mid_block.resnets.0.norm1.bias', 'model.mid_block.resnets.0.conv1.weight', 'model.mid_block.resnets.0.conv1.bias', 'model.mid_block.resnets.0.time_emb_proj.weight', 'model.mid_block.resnets.0.time_emb_proj.bias', 'model.mid_block.resnets.0.norm2.weight', 'model.mid_block.resnets.0.norm2.bias', 'model.mid_block.resnets.0.conv2.weight', 'model.mid_block.resnets.0.conv2.bias', 'model.mid_block.resnets.1.norm1.weight', 'model.mid_block.resnets.1.norm1.bias', 'model.mid_block.resnets.1.conv1.weight', 'model.mid_block.resnets.1.conv1.bias', 'model.mid_block.resnets.1.time_emb_proj.weight', 'model.mid_block.resnets.1.time_emb_proj.bias', 'model.mid_block.resnets.1.norm2.weight', 'model.mid_block.resnets.1.norm2.bias', 'model.mid_block.resnets.1.conv2.weight', 'model.mid_block.resnets.1.conv2.bias', 'model.conv_norm_out.weight', 'model.conv_norm_out.bias', 'model.conv_out.weight', 'model.conv_out.bias'])
[2024-03-20 07:33:01 pose_transfer_train.py:395] preparing lr scheduler...
[2024-03-20 07:33:01 pose_transfer_train.py:401] start training...
[2024-03-20 07:33:01 pose_transfer_train.py:410] epoch 1 start
[2024-03-20 07:34:19 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:34:19 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:34:29 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:34:33 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:34:50 pose_transfer_train.py:345] number of trainable parameters: 213329208
[2024-03-20 07:34:50 pose_transfer_train.py:364] preparing optimizer...
[2024-03-20 07:34:50 pose_transfer_train.py:370] preparing accelerator...
[2024-03-20 07:34:50 pose_transfer_train.py:377] loading states from ./checkpoints/
[2024-03-20 07:34:51 pose_transfer_train.py:387] _IncompatibleKeys(missing_keys=['module.learnable_vector', 'module.image_encoder_g.vision_model.embeddings.class_embedding', 'module.image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'module.image_encoder_g.vision_model.embeddings.position_embedding.weight', 'module.image_encoder_g.vision_model.pre_layrnorm.weight', 'module.image_encoder_g.vision_model.pre_layrnorm.bias', 'module.image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'module.image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'module.image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'module.image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'module.image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'module.image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'module.image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'module.image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'module.image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'module.image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'module.image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'module.image_encoder_g.vision_model.post_layernorm.weight', 'module.image_encoder_g.vision_model.post_layernorm.bias', 'module.image_encoder_g.visual_projection.weight', 'module.backbone.patch_embed.proj.weight', 'module.backbone.patch_embed.proj.bias', 'module.backbone.patch_embed.norm.weight', 'module.backbone.patch_embed.norm.bias', 'module.backbone.layers.0.blocks.0.norm1.weight', 'module.backbone.layers.0.blocks.0.norm1.bias', 'module.backbone.layers.0.blocks.0.attn.relative_position_bias_table', 'module.backbone.layers.0.blocks.0.attn.relative_position_index', 'module.backbone.layers.0.blocks.0.attn.qkv.weight', 'module.backbone.layers.0.blocks.0.attn.qkv.bias', 'module.backbone.layers.0.blocks.0.attn.proj.weight', 'module.backbone.layers.0.blocks.0.attn.proj.bias', 'module.backbone.layers.0.blocks.0.norm2.weight', 'module.backbone.layers.0.blocks.0.norm2.bias', 'module.backbone.layers.0.blocks.0.mlp.fc1.weight', 'module.backbone.layers.0.blocks.0.mlp.fc1.bias', 'module.backbone.layers.0.blocks.0.mlp.fc2.weight', 'module.backbone.layers.0.blocks.0.mlp.fc2.bias', 'module.backbone.layers.0.blocks.1.attn_mask', 'module.backbone.layers.0.blocks.1.norm1.weight', 'module.backbone.layers.0.blocks.1.norm1.bias', 'module.backbone.layers.0.blocks.1.attn.relative_position_bias_table', 'module.backbone.layers.0.blocks.1.attn.relative_position_index', 'module.backbone.layers.0.blocks.1.attn.qkv.weight', 'module.backbone.layers.0.blocks.1.attn.qkv.bias', 'module.backbone.layers.0.blocks.1.attn.proj.weight', 'module.backbone.layers.0.blocks.1.attn.proj.bias', 'module.backbone.layers.0.blocks.1.norm2.weight', 'module.backbone.layers.0.blocks.1.norm2.bias', 'module.backbone.layers.0.blocks.1.mlp.fc1.weight', 'module.backbone.layers.0.blocks.1.mlp.fc1.bias', 'module.backbone.layers.0.blocks.1.mlp.fc2.weight', 'module.backbone.layers.0.blocks.1.mlp.fc2.bias', 'module.backbone.layers.0.downsample.reduction.weight', 'module.backbone.layers.0.downsample.norm.weight', 'module.backbone.layers.0.downsample.norm.bias', 'module.backbone.layers.1.blocks.0.norm1.weight', 'module.backbone.layers.1.blocks.0.norm1.bias', 'module.backbone.layers.1.blocks.0.attn.relative_position_bias_table', 'module.backbone.layers.1.blocks.0.attn.relative_position_index', 'module.backbone.layers.1.blocks.0.attn.qkv.weight', 'module.backbone.layers.1.blocks.0.attn.qkv.bias', 'module.backbone.layers.1.blocks.0.attn.proj.weight', 'module.backbone.layers.1.blocks.0.attn.proj.bias', 'module.backbone.layers.1.blocks.0.norm2.weight', 'module.backbone.layers.1.blocks.0.norm2.bias', 'module.backbone.layers.1.blocks.0.mlp.fc1.weight', 'module.backbone.layers.1.blocks.0.mlp.fc1.bias', 'module.backbone.layers.1.blocks.0.mlp.fc2.weight', 'module.backbone.layers.1.blocks.0.mlp.fc2.bias', 'module.backbone.layers.1.blocks.1.attn_mask', 'module.backbone.layers.1.blocks.1.norm1.weight', 'module.backbone.layers.1.blocks.1.norm1.bias', 'module.backbone.layers.1.blocks.1.attn.relative_position_bias_table', 'module.backbone.layers.1.blocks.1.attn.relative_position_index', 'module.backbone.layers.1.blocks.1.attn.qkv.weight', 'module.backbone.layers.1.blocks.1.attn.qkv.bias', 'module.backbone.layers.1.blocks.1.attn.proj.weight', 'module.backbone.layers.1.blocks.1.attn.proj.bias', 'module.backbone.layers.1.blocks.1.norm2.weight', 'module.backbone.layers.1.blocks.1.norm2.bias', 'module.backbone.layers.1.blocks.1.mlp.fc1.weight', 'module.backbone.layers.1.blocks.1.mlp.fc1.bias', 'module.backbone.layers.1.blocks.1.mlp.fc2.weight', 'module.backbone.layers.1.blocks.1.mlp.fc2.bias', 'module.backbone.layers.1.downsample.reduction.weight', 'module.backbone.layers.1.downsample.norm.weight', 'module.backbone.layers.1.downsample.norm.bias', 'module.backbone.layers.2.blocks.0.norm1.weight', 'module.backbone.layers.2.blocks.0.norm1.bias', 'module.backbone.layers.2.blocks.0.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.0.attn.relative_position_index', 'module.backbone.layers.2.blocks.0.attn.qkv.weight', 'module.backbone.layers.2.blocks.0.attn.qkv.bias', 'module.backbone.layers.2.blocks.0.attn.proj.weight', 'module.backbone.layers.2.blocks.0.attn.proj.bias', 'module.backbone.layers.2.blocks.0.norm2.weight', 'module.backbone.layers.2.blocks.0.norm2.bias', 'module.backbone.layers.2.blocks.0.mlp.fc1.weight', 'module.backbone.layers.2.blocks.0.mlp.fc1.bias', 'module.backbone.layers.2.blocks.0.mlp.fc2.weight', 'module.backbone.layers.2.blocks.0.mlp.fc2.bias', 'module.backbone.layers.2.blocks.1.norm1.weight', 'module.backbone.layers.2.blocks.1.norm1.bias', 'module.backbone.layers.2.blocks.1.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.1.attn.relative_position_index', 'module.backbone.layers.2.blocks.1.attn.qkv.weight', 'module.backbone.layers.2.blocks.1.attn.qkv.bias', 'module.backbone.layers.2.blocks.1.attn.proj.weight', 'module.backbone.layers.2.blocks.1.attn.proj.bias', 'module.backbone.layers.2.blocks.1.norm2.weight', 'module.backbone.layers.2.blocks.1.norm2.bias', 'module.backbone.layers.2.blocks.1.mlp.fc1.weight', 'module.backbone.layers.2.blocks.1.mlp.fc1.bias', 'module.backbone.layers.2.blocks.1.mlp.fc2.weight', 'module.backbone.layers.2.blocks.1.mlp.fc2.bias', 'module.backbone.layers.2.blocks.2.norm1.weight', 'module.backbone.layers.2.blocks.2.norm1.bias', 'module.backbone.layers.2.blocks.2.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.2.attn.relative_position_index', 'module.backbone.layers.2.blocks.2.attn.qkv.weight', 'module.backbone.layers.2.blocks.2.attn.qkv.bias', 'module.backbone.layers.2.blocks.2.attn.proj.weight', 'module.backbone.layers.2.blocks.2.attn.proj.bias', 'module.backbone.layers.2.blocks.2.norm2.weight', 'module.backbone.layers.2.blocks.2.norm2.bias', 'module.backbone.layers.2.blocks.2.mlp.fc1.weight', 'module.backbone.layers.2.blocks.2.mlp.fc1.bias', 'module.backbone.layers.2.blocks.2.mlp.fc2.weight', 'module.backbone.layers.2.blocks.2.mlp.fc2.bias', 'module.backbone.layers.2.blocks.3.norm1.weight', 'module.backbone.layers.2.blocks.3.norm1.bias', 'module.backbone.layers.2.blocks.3.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.3.attn.relative_position_index', 'module.backbone.layers.2.blocks.3.attn.qkv.weight', 'module.backbone.layers.2.blocks.3.attn.qkv.bias', 'module.backbone.layers.2.blocks.3.attn.proj.weight', 'module.backbone.layers.2.blocks.3.attn.proj.bias', 'module.backbone.layers.2.blocks.3.norm2.weight', 'module.backbone.layers.2.blocks.3.norm2.bias', 'module.backbone.layers.2.blocks.3.mlp.fc1.weight', 'module.backbone.layers.2.blocks.3.mlp.fc1.bias', 'module.backbone.layers.2.blocks.3.mlp.fc2.weight', 'module.backbone.layers.2.blocks.3.mlp.fc2.bias', 'module.backbone.layers.2.blocks.4.norm1.weight', 'module.backbone.layers.2.blocks.4.norm1.bias', 'module.backbone.layers.2.blocks.4.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.4.attn.relative_position_index', 'module.backbone.layers.2.blocks.4.attn.qkv.weight', 'module.backbone.layers.2.blocks.4.attn.qkv.bias', 'module.backbone.layers.2.blocks.4.attn.proj.weight', 'module.backbone.layers.2.blocks.4.attn.proj.bias', 'module.backbone.layers.2.blocks.4.norm2.weight', 'module.backbone.layers.2.blocks.4.norm2.bias', 'module.backbone.layers.2.blocks.4.mlp.fc1.weight', 'module.backbone.layers.2.blocks.4.mlp.fc1.bias', 'module.backbone.layers.2.blocks.4.mlp.fc2.weight', 'module.backbone.layers.2.blocks.4.mlp.fc2.bias', 'module.backbone.layers.2.blocks.5.norm1.weight', 'module.backbone.layers.2.blocks.5.norm1.bias', 'module.backbone.layers.2.blocks.5.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.5.attn.relative_position_index', 'module.backbone.layers.2.blocks.5.attn.qkv.weight', 'module.backbone.layers.2.blocks.5.attn.qkv.bias', 'module.backbone.layers.2.blocks.5.attn.proj.weight', 'module.backbone.layers.2.blocks.5.attn.proj.bias', 'module.backbone.layers.2.blocks.5.norm2.weight', 'module.backbone.layers.2.blocks.5.norm2.bias', 'module.backbone.layers.2.blocks.5.mlp.fc1.weight', 'module.backbone.layers.2.blocks.5.mlp.fc1.bias', 'module.backbone.layers.2.blocks.5.mlp.fc2.weight', 'module.backbone.layers.2.blocks.5.mlp.fc2.bias', 'module.backbone.layers.2.blocks.6.norm1.weight', 'module.backbone.layers.2.blocks.6.norm1.bias', 'module.backbone.layers.2.blocks.6.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.6.attn.relative_position_index', 'module.backbone.layers.2.blocks.6.attn.qkv.weight', 'module.backbone.layers.2.blocks.6.attn.qkv.bias', 'module.backbone.layers.2.blocks.6.attn.proj.weight', 'module.backbone.layers.2.blocks.6.attn.proj.bias', 'module.backbone.layers.2.blocks.6.norm2.weight', 'module.backbone.layers.2.blocks.6.norm2.bias', 'module.backbone.layers.2.blocks.6.mlp.fc1.weight', 'module.backbone.layers.2.blocks.6.mlp.fc1.bias', 'module.backbone.layers.2.blocks.6.mlp.fc2.weight', 'module.backbone.layers.2.blocks.6.mlp.fc2.bias', 'module.backbone.layers.2.blocks.7.norm1.weight', 'module.backbone.layers.2.blocks.7.norm1.bias', 'module.backbone.layers.2.blocks.7.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.7.attn.relative_position_index', 'module.backbone.layers.2.blocks.7.attn.qkv.weight', 'module.backbone.layers.2.blocks.7.attn.qkv.bias', 'module.backbone.layers.2.blocks.7.attn.proj.weight', 'module.backbone.layers.2.blocks.7.attn.proj.bias', 'module.backbone.layers.2.blocks.7.norm2.weight', 'module.backbone.layers.2.blocks.7.norm2.bias', 'module.backbone.layers.2.blocks.7.mlp.fc1.weight', 'module.backbone.layers.2.blocks.7.mlp.fc1.bias', 'module.backbone.layers.2.blocks.7.mlp.fc2.weight', 'module.backbone.layers.2.blocks.7.mlp.fc2.bias', 'module.backbone.layers.2.blocks.8.norm1.weight', 'module.backbone.layers.2.blocks.8.norm1.bias', 'module.backbone.layers.2.blocks.8.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.8.attn.relative_position_index', 'module.backbone.layers.2.blocks.8.attn.qkv.weight', 'module.backbone.layers.2.blocks.8.attn.qkv.bias', 'module.backbone.layers.2.blocks.8.attn.proj.weight', 'module.backbone.layers.2.blocks.8.attn.proj.bias', 'module.backbone.layers.2.blocks.8.norm2.weight', 'module.backbone.layers.2.blocks.8.norm2.bias', 'module.backbone.layers.2.blocks.8.mlp.fc1.weight', 'module.backbone.layers.2.blocks.8.mlp.fc1.bias', 'module.backbone.layers.2.blocks.8.mlp.fc2.weight', 'module.backbone.layers.2.blocks.8.mlp.fc2.bias', 'module.backbone.layers.2.blocks.9.norm1.weight', 'module.backbone.layers.2.blocks.9.norm1.bias', 'module.backbone.layers.2.blocks.9.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.9.attn.relative_position_index', 'module.backbone.layers.2.blocks.9.attn.qkv.weight', 'module.backbone.layers.2.blocks.9.attn.qkv.bias', 'module.backbone.layers.2.blocks.9.attn.proj.weight', 'module.backbone.layers.2.blocks.9.attn.proj.bias', 'module.backbone.layers.2.blocks.9.norm2.weight', 'module.backbone.layers.2.blocks.9.norm2.bias', 'module.backbone.layers.2.blocks.9.mlp.fc1.weight', 'module.backbone.layers.2.blocks.9.mlp.fc1.bias', 'module.backbone.layers.2.blocks.9.mlp.fc2.weight', 'module.backbone.layers.2.blocks.9.mlp.fc2.bias', 'module.backbone.layers.2.blocks.10.norm1.weight', 'module.backbone.layers.2.blocks.10.norm1.bias', 'module.backbone.layers.2.blocks.10.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.10.attn.relative_position_index', 'module.backbone.layers.2.blocks.10.attn.qkv.weight', 'module.backbone.layers.2.blocks.10.attn.qkv.bias', 'module.backbone.layers.2.blocks.10.attn.proj.weight', 'module.backbone.layers.2.blocks.10.attn.proj.bias', 'module.backbone.layers.2.blocks.10.norm2.weight', 'module.backbone.layers.2.blocks.10.norm2.bias', 'module.backbone.layers.2.blocks.10.mlp.fc1.weight', 'module.backbone.layers.2.blocks.10.mlp.fc1.bias', 'module.backbone.layers.2.blocks.10.mlp.fc2.weight', 'module.backbone.layers.2.blocks.10.mlp.fc2.bias', 'module.backbone.layers.2.blocks.11.norm1.weight', 'module.backbone.layers.2.blocks.11.norm1.bias', 'module.backbone.layers.2.blocks.11.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.11.attn.relative_position_index', 'module.backbone.layers.2.blocks.11.attn.qkv.weight', 'module.backbone.layers.2.blocks.11.attn.qkv.bias', 'module.backbone.layers.2.blocks.11.attn.proj.weight', 'module.backbone.layers.2.blocks.11.attn.proj.bias', 'module.backbone.layers.2.blocks.11.norm2.weight', 'module.backbone.layers.2.blocks.11.norm2.bias', 'module.backbone.layers.2.blocks.11.mlp.fc1.weight', 'module.backbone.layers.2.blocks.11.mlp.fc1.bias', 'module.backbone.layers.2.blocks.11.mlp.fc2.weight', 'module.backbone.layers.2.blocks.11.mlp.fc2.bias', 'module.backbone.layers.2.blocks.12.norm1.weight', 'module.backbone.layers.2.blocks.12.norm1.bias', 'module.backbone.layers.2.blocks.12.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.12.attn.relative_position_index', 'module.backbone.layers.2.blocks.12.attn.qkv.weight', 'module.backbone.layers.2.blocks.12.attn.qkv.bias', 'module.backbone.layers.2.blocks.12.attn.proj.weight', 'module.backbone.layers.2.blocks.12.attn.proj.bias', 'module.backbone.layers.2.blocks.12.norm2.weight', 'module.backbone.layers.2.blocks.12.norm2.bias', 'module.backbone.layers.2.blocks.12.mlp.fc1.weight', 'module.backbone.layers.2.blocks.12.mlp.fc1.bias', 'module.backbone.layers.2.blocks.12.mlp.fc2.weight', 'module.backbone.layers.2.blocks.12.mlp.fc2.bias', 'module.backbone.layers.2.blocks.13.norm1.weight', 'module.backbone.layers.2.blocks.13.norm1.bias', 'module.backbone.layers.2.blocks.13.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.13.attn.relative_position_index', 'module.backbone.layers.2.blocks.13.attn.qkv.weight', 'module.backbone.layers.2.blocks.13.attn.qkv.bias', 'module.backbone.layers.2.blocks.13.attn.proj.weight', 'module.backbone.layers.2.blocks.13.attn.proj.bias', 'module.backbone.layers.2.blocks.13.norm2.weight', 'module.backbone.layers.2.blocks.13.norm2.bias', 'module.backbone.layers.2.blocks.13.mlp.fc1.weight', 'module.backbone.layers.2.blocks.13.mlp.fc1.bias', 'module.backbone.layers.2.blocks.13.mlp.fc2.weight', 'module.backbone.layers.2.blocks.13.mlp.fc2.bias', 'module.backbone.layers.2.blocks.14.norm1.weight', 'module.backbone.layers.2.blocks.14.norm1.bias', 'module.backbone.layers.2.blocks.14.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.14.attn.relative_position_index', 'module.backbone.layers.2.blocks.14.attn.qkv.weight', 'module.backbone.layers.2.blocks.14.attn.qkv.bias', 'module.backbone.layers.2.blocks.14.attn.proj.weight', 'module.backbone.layers.2.blocks.14.attn.proj.bias', 'module.backbone.layers.2.blocks.14.norm2.weight', 'module.backbone.layers.2.blocks.14.norm2.bias', 'module.backbone.layers.2.blocks.14.mlp.fc1.weight', 'module.backbone.layers.2.blocks.14.mlp.fc1.bias', 'module.backbone.layers.2.blocks.14.mlp.fc2.weight', 'module.backbone.layers.2.blocks.14.mlp.fc2.bias', 'module.backbone.layers.2.blocks.15.norm1.weight', 'module.backbone.layers.2.blocks.15.norm1.bias', 'module.backbone.layers.2.blocks.15.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.15.attn.relative_position_index', 'module.backbone.layers.2.blocks.15.attn.qkv.weight', 'module.backbone.layers.2.blocks.15.attn.qkv.bias', 'module.backbone.layers.2.blocks.15.attn.proj.weight', 'module.backbone.layers.2.blocks.15.attn.proj.bias', 'module.backbone.layers.2.blocks.15.norm2.weight', 'module.backbone.layers.2.blocks.15.norm2.bias', 'module.backbone.layers.2.blocks.15.mlp.fc1.weight', 'module.backbone.layers.2.blocks.15.mlp.fc1.bias', 'module.backbone.layers.2.blocks.15.mlp.fc2.weight', 'module.backbone.layers.2.blocks.15.mlp.fc2.bias', 'module.backbone.layers.2.blocks.16.norm1.weight', 'module.backbone.layers.2.blocks.16.norm1.bias', 'module.backbone.layers.2.blocks.16.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.16.attn.relative_position_index', 'module.backbone.layers.2.blocks.16.attn.qkv.weight', 'module.backbone.layers.2.blocks.16.attn.qkv.bias', 'module.backbone.layers.2.blocks.16.attn.proj.weight', 'module.backbone.layers.2.blocks.16.attn.proj.bias', 'module.backbone.layers.2.blocks.16.norm2.weight', 'module.backbone.layers.2.blocks.16.norm2.bias', 'module.backbone.layers.2.blocks.16.mlp.fc1.weight', 'module.backbone.layers.2.blocks.16.mlp.fc1.bias', 'module.backbone.layers.2.blocks.16.mlp.fc2.weight', 'module.backbone.layers.2.blocks.16.mlp.fc2.bias', 'module.backbone.layers.2.blocks.17.norm1.weight', 'module.backbone.layers.2.blocks.17.norm1.bias', 'module.backbone.layers.2.blocks.17.attn.relative_position_bias_table', 'module.backbone.layers.2.blocks.17.attn.relative_position_index', 'module.backbone.layers.2.blocks.17.attn.qkv.weight', 'module.backbone.layers.2.blocks.17.attn.qkv.bias', 'module.backbone.layers.2.blocks.17.attn.proj.weight', 'module.backbone.layers.2.blocks.17.attn.proj.bias', 'module.backbone.layers.2.blocks.17.norm2.weight', 'module.backbone.layers.2.blocks.17.norm2.bias', 'module.backbone.layers.2.blocks.17.mlp.fc1.weight', 'module.backbone.layers.2.blocks.17.mlp.fc1.bias', 'module.backbone.layers.2.blocks.17.mlp.fc2.weight', 'module.backbone.layers.2.blocks.17.mlp.fc2.bias', 'module.backbone.layers.2.downsample.reduction.weight', 'module.backbone.layers.2.downsample.norm.weight', 'module.backbone.layers.2.downsample.norm.bias', 'module.backbone.layers.3.blocks.0.norm1.weight', 'module.backbone.layers.3.blocks.0.norm1.bias', 'module.backbone.layers.3.blocks.0.attn.relative_position_bias_table', 'module.backbone.layers.3.blocks.0.attn.relative_position_index', 'module.backbone.layers.3.blocks.0.attn.qkv.weight', 'module.backbone.layers.3.blocks.0.attn.qkv.bias', 'module.backbone.layers.3.blocks.0.attn.proj.weight', 'module.backbone.layers.3.blocks.0.attn.proj.bias', 'module.backbone.layers.3.blocks.0.norm2.weight', 'module.backbone.layers.3.blocks.0.norm2.bias', 'module.backbone.layers.3.blocks.0.mlp.fc1.weight', 'module.backbone.layers.3.blocks.0.mlp.fc1.bias', 'module.backbone.layers.3.blocks.0.mlp.fc2.weight', 'module.backbone.layers.3.blocks.0.mlp.fc2.bias', 'module.backbone.layers.3.blocks.1.norm1.weight', 'module.backbone.layers.3.blocks.1.norm1.bias', 'module.backbone.layers.3.blocks.1.attn.relative_position_bias_table', 'module.backbone.layers.3.blocks.1.attn.relative_position_index', 'module.backbone.layers.3.blocks.1.attn.qkv.weight', 'module.backbone.layers.3.blocks.1.attn.qkv.bias', 'module.backbone.layers.3.blocks.1.attn.proj.weight', 'module.backbone.layers.3.blocks.1.attn.proj.bias', 'module.backbone.layers.3.blocks.1.norm2.weight', 'module.backbone.layers.3.blocks.1.norm2.bias', 'module.backbone.layers.3.blocks.1.mlp.fc1.weight', 'module.backbone.layers.3.blocks.1.mlp.fc1.bias', 'module.backbone.layers.3.blocks.1.mlp.fc2.weight', 'module.backbone.layers.3.blocks.1.mlp.fc2.bias', 'module.appearance_encoder.blocks.0.0.norm1.weight', 'module.appearance_encoder.blocks.0.0.norm1.bias', 'module.appearance_encoder.blocks.0.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.0.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.0.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.0.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.0.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.0.0.norm2.weight', 'module.appearance_encoder.blocks.0.0.norm2.bias', 'module.appearance_encoder.blocks.0.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.0.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.0.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.0.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.0.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.0.0.norm3.weight', 'module.appearance_encoder.blocks.0.0.norm3.bias', 'module.appearance_encoder.blocks.0.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.0.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.0.0.ff.net.2.weight', 'module.appearance_encoder.blocks.0.0.ff.net.2.bias', 'module.appearance_encoder.blocks.0.1.norm1.weight', 'module.appearance_encoder.blocks.0.1.norm1.bias', 'module.appearance_encoder.blocks.0.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.0.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.0.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.0.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.0.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.0.1.norm2.weight', 'module.appearance_encoder.blocks.0.1.norm2.bias', 'module.appearance_encoder.blocks.0.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.0.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.0.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.0.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.0.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.0.1.norm3.weight', 'module.appearance_encoder.blocks.0.1.norm3.bias', 'module.appearance_encoder.blocks.0.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.0.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.0.1.ff.net.2.weight', 'module.appearance_encoder.blocks.0.1.ff.net.2.bias', 'module.appearance_encoder.blocks.0.2.norm1.weight', 'module.appearance_encoder.blocks.0.2.norm1.bias', 'module.appearance_encoder.blocks.0.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.0.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.0.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.0.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.0.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.0.2.norm2.weight', 'module.appearance_encoder.blocks.0.2.norm2.bias', 'module.appearance_encoder.blocks.0.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.0.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.0.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.0.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.0.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.0.2.norm3.weight', 'module.appearance_encoder.blocks.0.2.norm3.bias', 'module.appearance_encoder.blocks.0.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.0.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.0.2.ff.net.2.weight', 'module.appearance_encoder.blocks.0.2.ff.net.2.bias', 'module.appearance_encoder.blocks.0.3.norm1.weight', 'module.appearance_encoder.blocks.0.3.norm1.bias', 'module.appearance_encoder.blocks.0.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.0.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.0.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.0.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.0.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.0.3.norm2.weight', 'module.appearance_encoder.blocks.0.3.norm2.bias', 'module.appearance_encoder.blocks.0.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.0.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.0.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.0.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.0.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.0.3.norm3.weight', 'module.appearance_encoder.blocks.0.3.norm3.bias', 'module.appearance_encoder.blocks.0.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.0.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.0.3.ff.net.2.weight', 'module.appearance_encoder.blocks.0.3.ff.net.2.bias', 'module.appearance_encoder.blocks.1.0.norm1.weight', 'module.appearance_encoder.blocks.1.0.norm1.bias', 'module.appearance_encoder.blocks.1.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.1.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.1.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.1.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.1.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.1.0.norm2.weight', 'module.appearance_encoder.blocks.1.0.norm2.bias', 'module.appearance_encoder.blocks.1.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.1.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.1.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.1.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.1.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.1.0.norm3.weight', 'module.appearance_encoder.blocks.1.0.norm3.bias', 'module.appearance_encoder.blocks.1.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.1.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.1.0.ff.net.2.weight', 'module.appearance_encoder.blocks.1.0.ff.net.2.bias', 'module.appearance_encoder.blocks.1.1.norm1.weight', 'module.appearance_encoder.blocks.1.1.norm1.bias', 'module.appearance_encoder.blocks.1.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.1.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.1.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.1.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.1.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.1.1.norm2.weight', 'module.appearance_encoder.blocks.1.1.norm2.bias', 'module.appearance_encoder.blocks.1.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.1.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.1.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.1.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.1.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.1.1.norm3.weight', 'module.appearance_encoder.blocks.1.1.norm3.bias', 'module.appearance_encoder.blocks.1.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.1.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.1.1.ff.net.2.weight', 'module.appearance_encoder.blocks.1.1.ff.net.2.bias', 'module.appearance_encoder.blocks.1.2.norm1.weight', 'module.appearance_encoder.blocks.1.2.norm1.bias', 'module.appearance_encoder.blocks.1.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.1.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.1.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.1.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.1.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.1.2.norm2.weight', 'module.appearance_encoder.blocks.1.2.norm2.bias', 'module.appearance_encoder.blocks.1.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.1.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.1.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.1.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.1.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.1.2.norm3.weight', 'module.appearance_encoder.blocks.1.2.norm3.bias', 'module.appearance_encoder.blocks.1.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.1.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.1.2.ff.net.2.weight', 'module.appearance_encoder.blocks.1.2.ff.net.2.bias', 'module.appearance_encoder.blocks.1.3.norm1.weight', 'module.appearance_encoder.blocks.1.3.norm1.bias', 'module.appearance_encoder.blocks.1.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.1.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.1.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.1.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.1.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.1.3.norm2.weight', 'module.appearance_encoder.blocks.1.3.norm2.bias', 'module.appearance_encoder.blocks.1.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.1.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.1.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.1.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.1.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.1.3.norm3.weight', 'module.appearance_encoder.blocks.1.3.norm3.bias', 'module.appearance_encoder.blocks.1.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.1.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.1.3.ff.net.2.weight', 'module.appearance_encoder.blocks.1.3.ff.net.2.bias', 'module.appearance_encoder.blocks.2.0.norm1.weight', 'module.appearance_encoder.blocks.2.0.norm1.bias', 'module.appearance_encoder.blocks.2.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.2.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.2.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.2.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.2.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.2.0.norm2.weight', 'module.appearance_encoder.blocks.2.0.norm2.bias', 'module.appearance_encoder.blocks.2.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.2.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.2.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.2.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.2.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.2.0.norm3.weight', 'module.appearance_encoder.blocks.2.0.norm3.bias', 'module.appearance_encoder.blocks.2.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.2.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.2.0.ff.net.2.weight', 'module.appearance_encoder.blocks.2.0.ff.net.2.bias', 'module.appearance_encoder.blocks.2.1.norm1.weight', 'module.appearance_encoder.blocks.2.1.norm1.bias', 'module.appearance_encoder.blocks.2.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.2.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.2.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.2.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.2.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.2.1.norm2.weight', 'module.appearance_encoder.blocks.2.1.norm2.bias', 'module.appearance_encoder.blocks.2.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.2.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.2.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.2.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.2.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.2.1.norm3.weight', 'module.appearance_encoder.blocks.2.1.norm3.bias', 'module.appearance_encoder.blocks.2.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.2.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.2.1.ff.net.2.weight', 'module.appearance_encoder.blocks.2.1.ff.net.2.bias', 'module.appearance_encoder.blocks.2.2.norm1.weight', 'module.appearance_encoder.blocks.2.2.norm1.bias', 'module.appearance_encoder.blocks.2.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.2.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.2.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.2.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.2.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.2.2.norm2.weight', 'module.appearance_encoder.blocks.2.2.norm2.bias', 'module.appearance_encoder.blocks.2.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.2.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.2.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.2.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.2.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.2.2.norm3.weight', 'module.appearance_encoder.blocks.2.2.norm3.bias', 'module.appearance_encoder.blocks.2.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.2.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.2.2.ff.net.2.weight', 'module.appearance_encoder.blocks.2.2.ff.net.2.bias', 'module.appearance_encoder.blocks.2.3.norm1.weight', 'module.appearance_encoder.blocks.2.3.norm1.bias', 'module.appearance_encoder.blocks.2.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.2.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.2.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.2.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.2.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.2.3.norm2.weight', 'module.appearance_encoder.blocks.2.3.norm2.bias', 'module.appearance_encoder.blocks.2.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.2.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.2.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.2.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.2.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.2.3.norm3.weight', 'module.appearance_encoder.blocks.2.3.norm3.bias', 'module.appearance_encoder.blocks.2.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.2.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.2.3.ff.net.2.weight', 'module.appearance_encoder.blocks.2.3.ff.net.2.bias', 'module.appearance_encoder.blocks.3.0.norm1.weight', 'module.appearance_encoder.blocks.3.0.norm1.bias', 'module.appearance_encoder.blocks.3.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.3.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.3.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.3.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.3.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.3.0.norm2.weight', 'module.appearance_encoder.blocks.3.0.norm2.bias', 'module.appearance_encoder.blocks.3.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.3.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.3.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.3.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.3.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.3.0.norm3.weight', 'module.appearance_encoder.blocks.3.0.norm3.bias', 'module.appearance_encoder.blocks.3.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.3.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.3.0.ff.net.2.weight', 'module.appearance_encoder.blocks.3.0.ff.net.2.bias', 'module.appearance_encoder.blocks.3.1.norm1.weight', 'module.appearance_encoder.blocks.3.1.norm1.bias', 'module.appearance_encoder.blocks.3.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.3.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.3.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.3.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.3.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.3.1.norm2.weight', 'module.appearance_encoder.blocks.3.1.norm2.bias', 'module.appearance_encoder.blocks.3.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.3.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.3.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.3.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.3.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.3.1.norm3.weight', 'module.appearance_encoder.blocks.3.1.norm3.bias', 'module.appearance_encoder.blocks.3.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.3.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.3.1.ff.net.2.weight', 'module.appearance_encoder.blocks.3.1.ff.net.2.bias', 'module.appearance_encoder.blocks.3.2.norm1.weight', 'module.appearance_encoder.blocks.3.2.norm1.bias', 'module.appearance_encoder.blocks.3.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.3.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.3.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.3.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.3.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.3.2.norm2.weight', 'module.appearance_encoder.blocks.3.2.norm2.bias', 'module.appearance_encoder.blocks.3.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.3.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.3.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.3.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.3.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.3.2.norm3.weight', 'module.appearance_encoder.blocks.3.2.norm3.bias', 'module.appearance_encoder.blocks.3.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.3.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.3.2.ff.net.2.weight', 'module.appearance_encoder.blocks.3.2.ff.net.2.bias', 'module.appearance_encoder.blocks.3.3.norm1.weight', 'module.appearance_encoder.blocks.3.3.norm1.bias', 'module.appearance_encoder.blocks.3.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.3.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.3.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.3.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.3.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.3.3.norm2.weight', 'module.appearance_encoder.blocks.3.3.norm2.bias', 'module.appearance_encoder.blocks.3.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.3.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.3.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.3.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.3.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.3.3.norm3.weight', 'module.appearance_encoder.blocks.3.3.norm3.bias', 'module.appearance_encoder.blocks.3.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.3.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.3.3.ff.net.2.weight', 'module.appearance_encoder.blocks.3.3.ff.net.2.bias', 'module.appearance_encoder.blocks.4.0.norm1.weight', 'module.appearance_encoder.blocks.4.0.norm1.bias', 'module.appearance_encoder.blocks.4.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.4.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.4.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.4.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.4.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.4.0.norm2.weight', 'module.appearance_encoder.blocks.4.0.norm2.bias', 'module.appearance_encoder.blocks.4.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.4.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.4.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.4.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.4.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.4.0.norm3.weight', 'module.appearance_encoder.blocks.4.0.norm3.bias', 'module.appearance_encoder.blocks.4.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.4.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.4.0.ff.net.2.weight', 'module.appearance_encoder.blocks.4.0.ff.net.2.bias', 'module.appearance_encoder.blocks.4.1.norm1.weight', 'module.appearance_encoder.blocks.4.1.norm1.bias', 'module.appearance_encoder.blocks.4.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.4.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.4.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.4.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.4.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.4.1.norm2.weight', 'module.appearance_encoder.blocks.4.1.norm2.bias', 'module.appearance_encoder.blocks.4.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.4.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.4.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.4.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.4.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.4.1.norm3.weight', 'module.appearance_encoder.blocks.4.1.norm3.bias', 'module.appearance_encoder.blocks.4.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.4.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.4.1.ff.net.2.weight', 'module.appearance_encoder.blocks.4.1.ff.net.2.bias', 'module.appearance_encoder.blocks.4.2.norm1.weight', 'module.appearance_encoder.blocks.4.2.norm1.bias', 'module.appearance_encoder.blocks.4.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.4.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.4.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.4.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.4.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.4.2.norm2.weight', 'module.appearance_encoder.blocks.4.2.norm2.bias', 'module.appearance_encoder.blocks.4.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.4.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.4.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.4.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.4.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.4.2.norm3.weight', 'module.appearance_encoder.blocks.4.2.norm3.bias', 'module.appearance_encoder.blocks.4.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.4.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.4.2.ff.net.2.weight', 'module.appearance_encoder.blocks.4.2.ff.net.2.bias', 'module.appearance_encoder.blocks.4.3.norm1.weight', 'module.appearance_encoder.blocks.4.3.norm1.bias', 'module.appearance_encoder.blocks.4.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.4.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.4.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.4.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.4.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.4.3.norm2.weight', 'module.appearance_encoder.blocks.4.3.norm2.bias', 'module.appearance_encoder.blocks.4.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.4.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.4.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.4.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.4.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.4.3.norm3.weight', 'module.appearance_encoder.blocks.4.3.norm3.bias', 'module.appearance_encoder.blocks.4.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.4.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.4.3.ff.net.2.weight', 'module.appearance_encoder.blocks.4.3.ff.net.2.bias', 'module.appearance_encoder.blocks.5.0.norm1.weight', 'module.appearance_encoder.blocks.5.0.norm1.bias', 'module.appearance_encoder.blocks.5.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.5.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.5.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.5.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.5.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.5.0.norm2.weight', 'module.appearance_encoder.blocks.5.0.norm2.bias', 'module.appearance_encoder.blocks.5.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.5.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.5.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.5.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.5.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.5.0.norm3.weight', 'module.appearance_encoder.blocks.5.0.norm3.bias', 'module.appearance_encoder.blocks.5.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.5.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.5.0.ff.net.2.weight', 'module.appearance_encoder.blocks.5.0.ff.net.2.bias', 'module.appearance_encoder.blocks.5.1.norm1.weight', 'module.appearance_encoder.blocks.5.1.norm1.bias', 'module.appearance_encoder.blocks.5.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.5.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.5.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.5.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.5.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.5.1.norm2.weight', 'module.appearance_encoder.blocks.5.1.norm2.bias', 'module.appearance_encoder.blocks.5.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.5.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.5.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.5.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.5.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.5.1.norm3.weight', 'module.appearance_encoder.blocks.5.1.norm3.bias', 'module.appearance_encoder.blocks.5.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.5.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.5.1.ff.net.2.weight', 'module.appearance_encoder.blocks.5.1.ff.net.2.bias', 'module.appearance_encoder.blocks.5.2.norm1.weight', 'module.appearance_encoder.blocks.5.2.norm1.bias', 'module.appearance_encoder.blocks.5.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.5.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.5.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.5.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.5.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.5.2.norm2.weight', 'module.appearance_encoder.blocks.5.2.norm2.bias', 'module.appearance_encoder.blocks.5.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.5.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.5.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.5.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.5.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.5.2.norm3.weight', 'module.appearance_encoder.blocks.5.2.norm3.bias', 'module.appearance_encoder.blocks.5.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.5.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.5.2.ff.net.2.weight', 'module.appearance_encoder.blocks.5.2.ff.net.2.bias', 'module.appearance_encoder.blocks.5.3.norm1.weight', 'module.appearance_encoder.blocks.5.3.norm1.bias', 'module.appearance_encoder.blocks.5.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.5.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.5.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.5.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.5.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.5.3.norm2.weight', 'module.appearance_encoder.blocks.5.3.norm2.bias', 'module.appearance_encoder.blocks.5.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.5.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.5.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.5.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.5.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.5.3.norm3.weight', 'module.appearance_encoder.blocks.5.3.norm3.bias', 'module.appearance_encoder.blocks.5.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.5.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.5.3.ff.net.2.weight', 'module.appearance_encoder.blocks.5.3.ff.net.2.bias', 'module.appearance_encoder.blocks.6.0.norm1.weight', 'module.appearance_encoder.blocks.6.0.norm1.bias', 'module.appearance_encoder.blocks.6.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.6.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.6.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.6.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.6.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.6.0.norm2.weight', 'module.appearance_encoder.blocks.6.0.norm2.bias', 'module.appearance_encoder.blocks.6.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.6.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.6.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.6.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.6.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.6.0.norm3.weight', 'module.appearance_encoder.blocks.6.0.norm3.bias', 'module.appearance_encoder.blocks.6.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.6.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.6.0.ff.net.2.weight', 'module.appearance_encoder.blocks.6.0.ff.net.2.bias', 'module.appearance_encoder.blocks.6.1.norm1.weight', 'module.appearance_encoder.blocks.6.1.norm1.bias', 'module.appearance_encoder.blocks.6.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.6.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.6.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.6.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.6.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.6.1.norm2.weight', 'module.appearance_encoder.blocks.6.1.norm2.bias', 'module.appearance_encoder.blocks.6.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.6.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.6.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.6.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.6.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.6.1.norm3.weight', 'module.appearance_encoder.blocks.6.1.norm3.bias', 'module.appearance_encoder.blocks.6.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.6.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.6.1.ff.net.2.weight', 'module.appearance_encoder.blocks.6.1.ff.net.2.bias', 'module.appearance_encoder.blocks.6.2.norm1.weight', 'module.appearance_encoder.blocks.6.2.norm1.bias', 'module.appearance_encoder.blocks.6.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.6.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.6.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.6.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.6.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.6.2.norm2.weight', 'module.appearance_encoder.blocks.6.2.norm2.bias', 'module.appearance_encoder.blocks.6.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.6.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.6.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.6.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.6.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.6.2.norm3.weight', 'module.appearance_encoder.blocks.6.2.norm3.bias', 'module.appearance_encoder.blocks.6.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.6.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.6.2.ff.net.2.weight', 'module.appearance_encoder.blocks.6.2.ff.net.2.bias', 'module.appearance_encoder.blocks.6.3.norm1.weight', 'module.appearance_encoder.blocks.6.3.norm1.bias', 'module.appearance_encoder.blocks.6.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.6.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.6.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.6.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.6.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.6.3.norm2.weight', 'module.appearance_encoder.blocks.6.3.norm2.bias', 'module.appearance_encoder.blocks.6.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.6.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.6.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.6.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.6.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.6.3.norm3.weight', 'module.appearance_encoder.blocks.6.3.norm3.bias', 'module.appearance_encoder.blocks.6.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.6.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.6.3.ff.net.2.weight', 'module.appearance_encoder.blocks.6.3.ff.net.2.bias', 'module.appearance_encoder.blocks.7.0.norm1.weight', 'module.appearance_encoder.blocks.7.0.norm1.bias', 'module.appearance_encoder.blocks.7.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.7.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.7.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.7.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.7.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.7.0.norm2.weight', 'module.appearance_encoder.blocks.7.0.norm2.bias', 'module.appearance_encoder.blocks.7.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.7.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.7.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.7.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.7.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.7.0.norm3.weight', 'module.appearance_encoder.blocks.7.0.norm3.bias', 'module.appearance_encoder.blocks.7.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.7.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.7.0.ff.net.2.weight', 'module.appearance_encoder.blocks.7.0.ff.net.2.bias', 'module.appearance_encoder.blocks.7.1.norm1.weight', 'module.appearance_encoder.blocks.7.1.norm1.bias', 'module.appearance_encoder.blocks.7.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.7.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.7.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.7.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.7.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.7.1.norm2.weight', 'module.appearance_encoder.blocks.7.1.norm2.bias', 'module.appearance_encoder.blocks.7.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.7.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.7.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.7.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.7.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.7.1.norm3.weight', 'module.appearance_encoder.blocks.7.1.norm3.bias', 'module.appearance_encoder.blocks.7.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.7.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.7.1.ff.net.2.weight', 'module.appearance_encoder.blocks.7.1.ff.net.2.bias', 'module.appearance_encoder.blocks.7.2.norm1.weight', 'module.appearance_encoder.blocks.7.2.norm1.bias', 'module.appearance_encoder.blocks.7.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.7.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.7.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.7.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.7.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.7.2.norm2.weight', 'module.appearance_encoder.blocks.7.2.norm2.bias', 'module.appearance_encoder.blocks.7.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.7.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.7.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.7.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.7.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.7.2.norm3.weight', 'module.appearance_encoder.blocks.7.2.norm3.bias', 'module.appearance_encoder.blocks.7.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.7.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.7.2.ff.net.2.weight', 'module.appearance_encoder.blocks.7.2.ff.net.2.bias', 'module.appearance_encoder.blocks.7.3.norm1.weight', 'module.appearance_encoder.blocks.7.3.norm1.bias', 'module.appearance_encoder.blocks.7.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.7.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.7.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.7.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.7.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.7.3.norm2.weight', 'module.appearance_encoder.blocks.7.3.norm2.bias', 'module.appearance_encoder.blocks.7.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.7.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.7.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.7.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.7.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.7.3.norm3.weight', 'module.appearance_encoder.blocks.7.3.norm3.bias', 'module.appearance_encoder.blocks.7.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.7.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.7.3.ff.net.2.weight', 'module.appearance_encoder.blocks.7.3.ff.net.2.bias', 'module.appearance_encoder.blocks.8.0.norm1.weight', 'module.appearance_encoder.blocks.8.0.norm1.bias', 'module.appearance_encoder.blocks.8.0.attn1.to_q.weight', 'module.appearance_encoder.blocks.8.0.attn1.to_k.weight', 'module.appearance_encoder.blocks.8.0.attn1.to_v.weight', 'module.appearance_encoder.blocks.8.0.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.8.0.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.8.0.norm2.weight', 'module.appearance_encoder.blocks.8.0.norm2.bias', 'module.appearance_encoder.blocks.8.0.attn2.to_q.weight', 'module.appearance_encoder.blocks.8.0.attn2.to_k.weight', 'module.appearance_encoder.blocks.8.0.attn2.to_v.weight', 'module.appearance_encoder.blocks.8.0.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.8.0.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.8.0.norm3.weight', 'module.appearance_encoder.blocks.8.0.norm3.bias', 'module.appearance_encoder.blocks.8.0.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.8.0.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.8.0.ff.net.2.weight', 'module.appearance_encoder.blocks.8.0.ff.net.2.bias', 'module.appearance_encoder.blocks.8.1.norm1.weight', 'module.appearance_encoder.blocks.8.1.norm1.bias', 'module.appearance_encoder.blocks.8.1.attn1.to_q.weight', 'module.appearance_encoder.blocks.8.1.attn1.to_k.weight', 'module.appearance_encoder.blocks.8.1.attn1.to_v.weight', 'module.appearance_encoder.blocks.8.1.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.8.1.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.8.1.norm2.weight', 'module.appearance_encoder.blocks.8.1.norm2.bias', 'module.appearance_encoder.blocks.8.1.attn2.to_q.weight', 'module.appearance_encoder.blocks.8.1.attn2.to_k.weight', 'module.appearance_encoder.blocks.8.1.attn2.to_v.weight', 'module.appearance_encoder.blocks.8.1.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.8.1.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.8.1.norm3.weight', 'module.appearance_encoder.blocks.8.1.norm3.bias', 'module.appearance_encoder.blocks.8.1.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.8.1.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.8.1.ff.net.2.weight', 'module.appearance_encoder.blocks.8.1.ff.net.2.bias', 'module.appearance_encoder.blocks.8.2.norm1.weight', 'module.appearance_encoder.blocks.8.2.norm1.bias', 'module.appearance_encoder.blocks.8.2.attn1.to_q.weight', 'module.appearance_encoder.blocks.8.2.attn1.to_k.weight', 'module.appearance_encoder.blocks.8.2.attn1.to_v.weight', 'module.appearance_encoder.blocks.8.2.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.8.2.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.8.2.norm2.weight', 'module.appearance_encoder.blocks.8.2.norm2.bias', 'module.appearance_encoder.blocks.8.2.attn2.to_q.weight', 'module.appearance_encoder.blocks.8.2.attn2.to_k.weight', 'module.appearance_encoder.blocks.8.2.attn2.to_v.weight', 'module.appearance_encoder.blocks.8.2.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.8.2.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.8.2.norm3.weight', 'module.appearance_encoder.blocks.8.2.norm3.bias', 'module.appearance_encoder.blocks.8.2.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.8.2.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.8.2.ff.net.2.weight', 'module.appearance_encoder.blocks.8.2.ff.net.2.bias', 'module.appearance_encoder.blocks.8.3.norm1.weight', 'module.appearance_encoder.blocks.8.3.norm1.bias', 'module.appearance_encoder.blocks.8.3.attn1.to_q.weight', 'module.appearance_encoder.blocks.8.3.attn1.to_k.weight', 'module.appearance_encoder.blocks.8.3.attn1.to_v.weight', 'module.appearance_encoder.blocks.8.3.attn1.to_out.0.weight', 'module.appearance_encoder.blocks.8.3.attn1.to_out.0.bias', 'module.appearance_encoder.blocks.8.3.norm2.weight', 'module.appearance_encoder.blocks.8.3.norm2.bias', 'module.appearance_encoder.blocks.8.3.attn2.to_q.weight', 'module.appearance_encoder.blocks.8.3.attn2.to_k.weight', 'module.appearance_encoder.blocks.8.3.attn2.to_v.weight', 'module.appearance_encoder.blocks.8.3.attn2.to_out.0.weight', 'module.appearance_encoder.blocks.8.3.attn2.to_out.0.bias', 'module.appearance_encoder.blocks.8.3.norm3.weight', 'module.appearance_encoder.blocks.8.3.norm3.bias', 'module.appearance_encoder.blocks.8.3.ff.net.0.proj.weight', 'module.appearance_encoder.blocks.8.3.ff.net.0.proj.bias', 'module.appearance_encoder.blocks.8.3.ff.net.2.weight', 'module.appearance_encoder.blocks.8.3.ff.net.2.bias', 'module.appearance_encoder.zero_conv_ins.0.weight', 'module.appearance_encoder.zero_conv_ins.0.bias', 'module.appearance_encoder.zero_conv_ins.1.weight', 'module.appearance_encoder.zero_conv_ins.1.bias', 'module.appearance_encoder.zero_conv_ins.2.weight', 'module.appearance_encoder.zero_conv_ins.2.bias', 'module.appearance_encoder.zero_conv_ins.3.weight', 'module.appearance_encoder.zero_conv_ins.3.bias', 'module.appearance_encoder.zero_conv_ins.4.weight', 'module.appearance_encoder.zero_conv_ins.4.bias', 'module.appearance_encoder.zero_conv_ins.5.weight', 'module.appearance_encoder.zero_conv_ins.5.bias', 'module.appearance_encoder.zero_conv_ins.6.weight', 'module.appearance_encoder.zero_conv_ins.6.bias', 'module.appearance_encoder.zero_conv_ins.7.weight', 'module.appearance_encoder.zero_conv_ins.7.bias', 'module.appearance_encoder.zero_conv_ins.8.weight', 'module.appearance_encoder.zero_conv_ins.8.bias', 'module.appearance_encoder.zero_conv_outs.0.weight', 'module.appearance_encoder.zero_conv_outs.0.bias', 'module.appearance_encoder.zero_conv_outs.1.weight', 'module.appearance_encoder.zero_conv_outs.1.bias', 'module.appearance_encoder.zero_conv_outs.2.weight', 'module.appearance_encoder.zero_conv_outs.2.bias', 'module.appearance_encoder.zero_conv_outs.3.weight', 'module.appearance_encoder.zero_conv_outs.3.bias', 'module.appearance_encoder.zero_conv_outs.4.weight', 'module.appearance_encoder.zero_conv_outs.4.bias', 'module.appearance_encoder.zero_conv_outs.5.weight', 'module.appearance_encoder.zero_conv_outs.5.bias', 'module.appearance_encoder.zero_conv_outs.6.weight', 'module.appearance_encoder.zero_conv_outs.6.bias', 'module.appearance_encoder.zero_conv_outs.7.weight', 'module.appearance_encoder.zero_conv_outs.7.bias', 'module.appearance_encoder.zero_conv_outs.8.weight', 'module.appearance_encoder.zero_conv_outs.8.bias', 'module.pose_encoder.conv_in.weight', 'module.pose_encoder.conv_in.bias', 'module.pose_encoder.resnets.0.norm1.weight', 'module.pose_encoder.resnets.0.norm1.bias', 'module.pose_encoder.resnets.0.conv1.weight', 'module.pose_encoder.resnets.0.conv1.bias', 'module.pose_encoder.resnets.0.norm2.weight', 'module.pose_encoder.resnets.0.norm2.bias', 'module.pose_encoder.resnets.0.conv2.weight', 'module.pose_encoder.resnets.0.conv2.bias', 'module.pose_encoder.resnets.1.norm1.weight', 'module.pose_encoder.resnets.1.norm1.bias', 'module.pose_encoder.resnets.1.conv1.weight', 'module.pose_encoder.resnets.1.conv1.bias', 'module.pose_encoder.resnets.1.norm2.weight', 'module.pose_encoder.resnets.1.norm2.bias', 'module.pose_encoder.resnets.1.conv2.weight', 'module.pose_encoder.resnets.1.conv2.bias', 'module.pose_encoder.resnets.1.conv_shortcut.weight', 'module.pose_encoder.resnets.1.conv_shortcut.bias', 'module.pose_encoder.resnets.2.norm1.weight', 'module.pose_encoder.resnets.2.norm1.bias', 'module.pose_encoder.resnets.2.conv1.weight', 'module.pose_encoder.resnets.2.conv1.bias', 'module.pose_encoder.resnets.2.norm2.weight', 'module.pose_encoder.resnets.2.norm2.bias', 'module.pose_encoder.resnets.2.conv2.weight', 'module.pose_encoder.resnets.2.conv2.bias', 'module.pose_encoder.resnets.2.conv_shortcut.weight', 'module.pose_encoder.resnets.2.conv_shortcut.bias'], unexpected_keys=['backbone.patch_embed.proj.weight', 'backbone.patch_embed.proj.bias', 'backbone.patch_embed.norm.weight', 'backbone.patch_embed.norm.bias', 'backbone.layers.0.blocks.0.norm1.weight', 'backbone.layers.0.blocks.0.norm1.bias', 'backbone.layers.0.blocks.0.attn.relative_position_bias_table', 'backbone.layers.0.blocks.0.attn.relative_position_index', 'backbone.layers.0.blocks.0.attn.qkv.weight', 'backbone.layers.0.blocks.0.attn.qkv.bias', 'backbone.layers.0.blocks.0.attn.proj.weight', 'backbone.layers.0.blocks.0.attn.proj.bias', 'backbone.layers.0.blocks.0.norm2.weight', 'backbone.layers.0.blocks.0.norm2.bias', 'backbone.layers.0.blocks.0.mlp.fc1.weight', 'backbone.layers.0.blocks.0.mlp.fc1.bias', 'backbone.layers.0.blocks.0.mlp.fc2.weight', 'backbone.layers.0.blocks.0.mlp.fc2.bias', 'backbone.layers.0.blocks.1.attn_mask', 'backbone.layers.0.blocks.1.norm1.weight', 'backbone.layers.0.blocks.1.norm1.bias', 'backbone.layers.0.blocks.1.attn.relative_position_bias_table', 'backbone.layers.0.blocks.1.attn.relative_position_index', 'backbone.layers.0.blocks.1.attn.qkv.weight', 'backbone.layers.0.blocks.1.attn.qkv.bias', 'backbone.layers.0.blocks.1.attn.proj.weight', 'backbone.layers.0.blocks.1.attn.proj.bias', 'backbone.layers.0.blocks.1.norm2.weight', 'backbone.layers.0.blocks.1.norm2.bias', 'backbone.layers.0.blocks.1.mlp.fc1.weight', 'backbone.layers.0.blocks.1.mlp.fc1.bias', 'backbone.layers.0.blocks.1.mlp.fc2.weight', 'backbone.layers.0.blocks.1.mlp.fc2.bias', 'backbone.layers.0.downsample.reduction.weight', 'backbone.layers.0.downsample.norm.weight', 'backbone.layers.0.downsample.norm.bias', 'backbone.layers.1.blocks.0.norm1.weight', 'backbone.layers.1.blocks.0.norm1.bias', 'backbone.layers.1.blocks.0.attn.relative_position_bias_table', 'backbone.layers.1.blocks.0.attn.relative_position_index', 'backbone.layers.1.blocks.0.attn.qkv.weight', 'backbone.layers.1.blocks.0.attn.qkv.bias', 'backbone.layers.1.blocks.0.attn.proj.weight', 'backbone.layers.1.blocks.0.attn.proj.bias', 'backbone.layers.1.blocks.0.norm2.weight', 'backbone.layers.1.blocks.0.norm2.bias', 'backbone.layers.1.blocks.0.mlp.fc1.weight', 'backbone.layers.1.blocks.0.mlp.fc1.bias', 'backbone.layers.1.blocks.0.mlp.fc2.weight', 'backbone.layers.1.blocks.0.mlp.fc2.bias', 'backbone.layers.1.blocks.1.attn_mask', 'backbone.layers.1.blocks.1.norm1.weight', 'backbone.layers.1.blocks.1.norm1.bias', 'backbone.layers.1.blocks.1.attn.relative_position_bias_table', 'backbone.layers.1.blocks.1.attn.relative_position_index', 'backbone.layers.1.blocks.1.attn.qkv.weight', 'backbone.layers.1.blocks.1.attn.qkv.bias', 'backbone.layers.1.blocks.1.attn.proj.weight', 'backbone.layers.1.blocks.1.attn.proj.bias', 'backbone.layers.1.blocks.1.norm2.weight', 'backbone.layers.1.blocks.1.norm2.bias', 'backbone.layers.1.blocks.1.mlp.fc1.weight', 'backbone.layers.1.blocks.1.mlp.fc1.bias', 'backbone.layers.1.blocks.1.mlp.fc2.weight', 'backbone.layers.1.blocks.1.mlp.fc2.bias', 'backbone.layers.1.downsample.reduction.weight', 'backbone.layers.1.downsample.norm.weight', 'backbone.layers.1.downsample.norm.bias', 'backbone.layers.2.blocks.0.norm1.weight', 'backbone.layers.2.blocks.0.norm1.bias', 'backbone.layers.2.blocks.0.attn.relative_position_bias_table', 'backbone.layers.2.blocks.0.attn.relative_position_index', 'backbone.layers.2.blocks.0.attn.qkv.weight', 'backbone.layers.2.blocks.0.attn.qkv.bias', 'backbone.layers.2.blocks.0.attn.proj.weight', 'backbone.layers.2.blocks.0.attn.proj.bias', 'backbone.layers.2.blocks.0.norm2.weight', 'backbone.layers.2.blocks.0.norm2.bias', 'backbone.layers.2.blocks.0.mlp.fc1.weight', 'backbone.layers.2.blocks.0.mlp.fc1.bias', 'backbone.layers.2.blocks.0.mlp.fc2.weight', 'backbone.layers.2.blocks.0.mlp.fc2.bias', 'backbone.layers.2.blocks.1.norm1.weight', 'backbone.layers.2.blocks.1.norm1.bias', 'backbone.layers.2.blocks.1.attn.relative_position_bias_table', 'backbone.layers.2.blocks.1.attn.relative_position_index', 'backbone.layers.2.blocks.1.attn.qkv.weight', 'backbone.layers.2.blocks.1.attn.qkv.bias', 'backbone.layers.2.blocks.1.attn.proj.weight', 'backbone.layers.2.blocks.1.attn.proj.bias', 'backbone.layers.2.blocks.1.norm2.weight', 'backbone.layers.2.blocks.1.norm2.bias', 'backbone.layers.2.blocks.1.mlp.fc1.weight', 'backbone.layers.2.blocks.1.mlp.fc1.bias', 'backbone.layers.2.blocks.1.mlp.fc2.weight', 'backbone.layers.2.blocks.1.mlp.fc2.bias', 'backbone.layers.2.blocks.2.norm1.weight', 'backbone.layers.2.blocks.2.norm1.bias', 'backbone.layers.2.blocks.2.attn.relative_position_bias_table', 'backbone.layers.2.blocks.2.attn.relative_position_index', 'backbone.layers.2.blocks.2.attn.qkv.weight', 'backbone.layers.2.blocks.2.attn.qkv.bias', 'backbone.layers.2.blocks.2.attn.proj.weight', 'backbone.layers.2.blocks.2.attn.proj.bias', 'backbone.layers.2.blocks.2.norm2.weight', 'backbone.layers.2.blocks.2.norm2.bias', 'backbone.layers.2.blocks.2.mlp.fc1.weight', 'backbone.layers.2.blocks.2.mlp.fc1.bias', 'backbone.layers.2.blocks.2.mlp.fc2.weight', 'backbone.layers.2.blocks.2.mlp.fc2.bias', 'backbone.layers.2.blocks.3.norm1.weight', 'backbone.layers.2.blocks.3.norm1.bias', 'backbone.layers.2.blocks.3.attn.relative_position_bias_table', 'backbone.layers.2.blocks.3.attn.relative_position_index', 'backbone.layers.2.blocks.3.attn.qkv.weight', 'backbone.layers.2.blocks.3.attn.qkv.bias', 'backbone.layers.2.blocks.3.attn.proj.weight', 'backbone.layers.2.blocks.3.attn.proj.bias', 'backbone.layers.2.blocks.3.norm2.weight', 'backbone.layers.2.blocks.3.norm2.bias', 'backbone.layers.2.blocks.3.mlp.fc1.weight', 'backbone.layers.2.blocks.3.mlp.fc1.bias', 'backbone.layers.2.blocks.3.mlp.fc2.weight', 'backbone.layers.2.blocks.3.mlp.fc2.bias', 'backbone.layers.2.blocks.4.norm1.weight', 'backbone.layers.2.blocks.4.norm1.bias', 'backbone.layers.2.blocks.4.attn.relative_position_bias_table', 'backbone.layers.2.blocks.4.attn.relative_position_index', 'backbone.layers.2.blocks.4.attn.qkv.weight', 'backbone.layers.2.blocks.4.attn.qkv.bias', 'backbone.layers.2.blocks.4.attn.proj.weight', 'backbone.layers.2.blocks.4.attn.proj.bias', 'backbone.layers.2.blocks.4.norm2.weight', 'backbone.layers.2.blocks.4.norm2.bias', 'backbone.layers.2.blocks.4.mlp.fc1.weight', 'backbone.layers.2.blocks.4.mlp.fc1.bias', 'backbone.layers.2.blocks.4.mlp.fc2.weight', 'backbone.layers.2.blocks.4.mlp.fc2.bias', 'backbone.layers.2.blocks.5.norm1.weight', 'backbone.layers.2.blocks.5.norm1.bias', 'backbone.layers.2.blocks.5.attn.relative_position_bias_table', 'backbone.layers.2.blocks.5.attn.relative_position_index', 'backbone.layers.2.blocks.5.attn.qkv.weight', 'backbone.layers.2.blocks.5.attn.qkv.bias', 'backbone.layers.2.blocks.5.attn.proj.weight', 'backbone.layers.2.blocks.5.attn.proj.bias', 'backbone.layers.2.blocks.5.norm2.weight', 'backbone.layers.2.blocks.5.norm2.bias', 'backbone.layers.2.blocks.5.mlp.fc1.weight', 'backbone.layers.2.blocks.5.mlp.fc1.bias', 'backbone.layers.2.blocks.5.mlp.fc2.weight', 'backbone.layers.2.blocks.5.mlp.fc2.bias', 'backbone.layers.2.blocks.6.norm1.weight', 'backbone.layers.2.blocks.6.norm1.bias', 'backbone.layers.2.blocks.6.attn.relative_position_bias_table', 'backbone.layers.2.blocks.6.attn.relative_position_index', 'backbone.layers.2.blocks.6.attn.qkv.weight', 'backbone.layers.2.blocks.6.attn.qkv.bias', 'backbone.layers.2.blocks.6.attn.proj.weight', 'backbone.layers.2.blocks.6.attn.proj.bias', 'backbone.layers.2.blocks.6.norm2.weight', 'backbone.layers.2.blocks.6.norm2.bias', 'backbone.layers.2.blocks.6.mlp.fc1.weight', 'backbone.layers.2.blocks.6.mlp.fc1.bias', 'backbone.layers.2.blocks.6.mlp.fc2.weight', 'backbone.layers.2.blocks.6.mlp.fc2.bias', 'backbone.layers.2.blocks.7.norm1.weight', 'backbone.layers.2.blocks.7.norm1.bias', 'backbone.layers.2.blocks.7.attn.relative_position_bias_table', 'backbone.layers.2.blocks.7.attn.relative_position_index', 'backbone.layers.2.blocks.7.attn.qkv.weight', 'backbone.layers.2.blocks.7.attn.qkv.bias', 'backbone.layers.2.blocks.7.attn.proj.weight', 'backbone.layers.2.blocks.7.attn.proj.bias', 'backbone.layers.2.blocks.7.norm2.weight', 'backbone.layers.2.blocks.7.norm2.bias', 'backbone.layers.2.blocks.7.mlp.fc1.weight', 'backbone.layers.2.blocks.7.mlp.fc1.bias', 'backbone.layers.2.blocks.7.mlp.fc2.weight', 'backbone.layers.2.blocks.7.mlp.fc2.bias', 'backbone.layers.2.blocks.8.norm1.weight', 'backbone.layers.2.blocks.8.norm1.bias', 'backbone.layers.2.blocks.8.attn.relative_position_bias_table', 'backbone.layers.2.blocks.8.attn.relative_position_index', 'backbone.layers.2.blocks.8.attn.qkv.weight', 'backbone.layers.2.blocks.8.attn.qkv.bias', 'backbone.layers.2.blocks.8.attn.proj.weight', 'backbone.layers.2.blocks.8.attn.proj.bias', 'backbone.layers.2.blocks.8.norm2.weight', 'backbone.layers.2.blocks.8.norm2.bias', 'backbone.layers.2.blocks.8.mlp.fc1.weight', 'backbone.layers.2.blocks.8.mlp.fc1.bias', 'backbone.layers.2.blocks.8.mlp.fc2.weight', 'backbone.layers.2.blocks.8.mlp.fc2.bias', 'backbone.layers.2.blocks.9.norm1.weight', 'backbone.layers.2.blocks.9.norm1.bias', 'backbone.layers.2.blocks.9.attn.relative_position_bias_table', 'backbone.layers.2.blocks.9.attn.relative_position_index', 'backbone.layers.2.blocks.9.attn.qkv.weight', 'backbone.layers.2.blocks.9.attn.qkv.bias', 'backbone.layers.2.blocks.9.attn.proj.weight', 'backbone.layers.2.blocks.9.attn.proj.bias', 'backbone.layers.2.blocks.9.norm2.weight', 'backbone.layers.2.blocks.9.norm2.bias', 'backbone.layers.2.blocks.9.mlp.fc1.weight', 'backbone.layers.2.blocks.9.mlp.fc1.bias', 'backbone.layers.2.blocks.9.mlp.fc2.weight', 'backbone.layers.2.blocks.9.mlp.fc2.bias', 'backbone.layers.2.blocks.10.norm1.weight', 'backbone.layers.2.blocks.10.norm1.bias', 'backbone.layers.2.blocks.10.attn.relative_position_bias_table', 'backbone.layers.2.blocks.10.attn.relative_position_index', 'backbone.layers.2.blocks.10.attn.qkv.weight', 'backbone.layers.2.blocks.10.attn.qkv.bias', 'backbone.layers.2.blocks.10.attn.proj.weight', 'backbone.layers.2.blocks.10.attn.proj.bias', 'backbone.layers.2.blocks.10.norm2.weight', 'backbone.layers.2.blocks.10.norm2.bias', 'backbone.layers.2.blocks.10.mlp.fc1.weight', 'backbone.layers.2.blocks.10.mlp.fc1.bias', 'backbone.layers.2.blocks.10.mlp.fc2.weight', 'backbone.layers.2.blocks.10.mlp.fc2.bias', 'backbone.layers.2.blocks.11.norm1.weight', 'backbone.layers.2.blocks.11.norm1.bias', 'backbone.layers.2.blocks.11.attn.relative_position_bias_table', 'backbone.layers.2.blocks.11.attn.relative_position_index', 'backbone.layers.2.blocks.11.attn.qkv.weight', 'backbone.layers.2.blocks.11.attn.qkv.bias', 'backbone.layers.2.blocks.11.attn.proj.weight', 'backbone.layers.2.blocks.11.attn.proj.bias', 'backbone.layers.2.blocks.11.norm2.weight', 'backbone.layers.2.blocks.11.norm2.bias', 'backbone.layers.2.blocks.11.mlp.fc1.weight', 'backbone.layers.2.blocks.11.mlp.fc1.bias', 'backbone.layers.2.blocks.11.mlp.fc2.weight', 'backbone.layers.2.blocks.11.mlp.fc2.bias', 'backbone.layers.2.blocks.12.norm1.weight', 'backbone.layers.2.blocks.12.norm1.bias', 'backbone.layers.2.blocks.12.attn.relative_position_bias_table', 'backbone.layers.2.blocks.12.attn.relative_position_index', 'backbone.layers.2.blocks.12.attn.qkv.weight', 'backbone.layers.2.blocks.12.attn.qkv.bias', 'backbone.layers.2.blocks.12.attn.proj.weight', 'backbone.layers.2.blocks.12.attn.proj.bias', 'backbone.layers.2.blocks.12.norm2.weight', 'backbone.layers.2.blocks.12.norm2.bias', 'backbone.layers.2.blocks.12.mlp.fc1.weight', 'backbone.layers.2.blocks.12.mlp.fc1.bias', 'backbone.layers.2.blocks.12.mlp.fc2.weight', 'backbone.layers.2.blocks.12.mlp.fc2.bias', 'backbone.layers.2.blocks.13.norm1.weight', 'backbone.layers.2.blocks.13.norm1.bias', 'backbone.layers.2.blocks.13.attn.relative_position_bias_table', 'backbone.layers.2.blocks.13.attn.relative_position_index', 'backbone.layers.2.blocks.13.attn.qkv.weight', 'backbone.layers.2.blocks.13.attn.qkv.bias', 'backbone.layers.2.blocks.13.attn.proj.weight', 'backbone.layers.2.blocks.13.attn.proj.bias', 'backbone.layers.2.blocks.13.norm2.weight', 'backbone.layers.2.blocks.13.norm2.bias', 'backbone.layers.2.blocks.13.mlp.fc1.weight', 'backbone.layers.2.blocks.13.mlp.fc1.bias', 'backbone.layers.2.blocks.13.mlp.fc2.weight', 'backbone.layers.2.blocks.13.mlp.fc2.bias', 'backbone.layers.2.blocks.14.norm1.weight', 'backbone.layers.2.blocks.14.norm1.bias', 'backbone.layers.2.blocks.14.attn.relative_position_bias_table', 'backbone.layers.2.blocks.14.attn.relative_position_index', 'backbone.layers.2.blocks.14.attn.qkv.weight', 'backbone.layers.2.blocks.14.attn.qkv.bias', 'backbone.layers.2.blocks.14.attn.proj.weight', 'backbone.layers.2.blocks.14.attn.proj.bias', 'backbone.layers.2.blocks.14.norm2.weight', 'backbone.layers.2.blocks.14.norm2.bias', 'backbone.layers.2.blocks.14.mlp.fc1.weight', 'backbone.layers.2.blocks.14.mlp.fc1.bias', 'backbone.layers.2.blocks.14.mlp.fc2.weight', 'backbone.layers.2.blocks.14.mlp.fc2.bias', 'backbone.layers.2.blocks.15.norm1.weight', 'backbone.layers.2.blocks.15.norm1.bias', 'backbone.layers.2.blocks.15.attn.relative_position_bias_table', 'backbone.layers.2.blocks.15.attn.relative_position_index', 'backbone.layers.2.blocks.15.attn.qkv.weight', 'backbone.layers.2.blocks.15.attn.qkv.bias', 'backbone.layers.2.blocks.15.attn.proj.weight', 'backbone.layers.2.blocks.15.attn.proj.bias', 'backbone.layers.2.blocks.15.norm2.weight', 'backbone.layers.2.blocks.15.norm2.bias', 'backbone.layers.2.blocks.15.mlp.fc1.weight', 'backbone.layers.2.blocks.15.mlp.fc1.bias', 'backbone.layers.2.blocks.15.mlp.fc2.weight', 'backbone.layers.2.blocks.15.mlp.fc2.bias', 'backbone.layers.2.blocks.16.norm1.weight', 'backbone.layers.2.blocks.16.norm1.bias', 'backbone.layers.2.blocks.16.attn.relative_position_bias_table', 'backbone.layers.2.blocks.16.attn.relative_position_index', 'backbone.layers.2.blocks.16.attn.qkv.weight', 'backbone.layers.2.blocks.16.attn.qkv.bias', 'backbone.layers.2.blocks.16.attn.proj.weight', 'backbone.layers.2.blocks.16.attn.proj.bias', 'backbone.layers.2.blocks.16.norm2.weight', 'backbone.layers.2.blocks.16.norm2.bias', 'backbone.layers.2.blocks.16.mlp.fc1.weight', 'backbone.layers.2.blocks.16.mlp.fc1.bias', 'backbone.layers.2.blocks.16.mlp.fc2.weight', 'backbone.layers.2.blocks.16.mlp.fc2.bias', 'backbone.layers.2.blocks.17.norm1.weight', 'backbone.layers.2.blocks.17.norm1.bias', 'backbone.layers.2.blocks.17.attn.relative_position_bias_table', 'backbone.layers.2.blocks.17.attn.relative_position_index', 'backbone.layers.2.blocks.17.attn.qkv.weight', 'backbone.layers.2.blocks.17.attn.qkv.bias', 'backbone.layers.2.blocks.17.attn.proj.weight', 'backbone.layers.2.blocks.17.attn.proj.bias', 'backbone.layers.2.blocks.17.norm2.weight', 'backbone.layers.2.blocks.17.norm2.bias', 'backbone.layers.2.blocks.17.mlp.fc1.weight', 'backbone.layers.2.blocks.17.mlp.fc1.bias', 'backbone.layers.2.blocks.17.mlp.fc2.weight', 'backbone.layers.2.blocks.17.mlp.fc2.bias', 'backbone.layers.2.downsample.reduction.weight', 'backbone.layers.2.downsample.norm.weight', 'backbone.layers.2.downsample.norm.bias', 'backbone.layers.3.blocks.0.norm1.weight', 'backbone.layers.3.blocks.0.norm1.bias', 'backbone.layers.3.blocks.0.attn.relative_position_bias_table', 'backbone.layers.3.blocks.0.attn.relative_position_index', 'backbone.layers.3.blocks.0.attn.qkv.weight', 'backbone.layers.3.blocks.0.attn.qkv.bias', 'backbone.layers.3.blocks.0.attn.proj.weight', 'backbone.layers.3.blocks.0.attn.proj.bias', 'backbone.layers.3.blocks.0.norm2.weight', 'backbone.layers.3.blocks.0.norm2.bias', 'backbone.layers.3.blocks.0.mlp.fc1.weight', 'backbone.layers.3.blocks.0.mlp.fc1.bias', 'backbone.layers.3.blocks.0.mlp.fc2.weight', 'backbone.layers.3.blocks.0.mlp.fc2.bias', 'backbone.layers.3.blocks.1.norm1.weight', 'backbone.layers.3.blocks.1.norm1.bias', 'backbone.layers.3.blocks.1.attn.relative_position_bias_table', 'backbone.layers.3.blocks.1.attn.relative_position_index', 'backbone.layers.3.blocks.1.attn.qkv.weight', 'backbone.layers.3.blocks.1.attn.qkv.bias', 'backbone.layers.3.blocks.1.attn.proj.weight', 'backbone.layers.3.blocks.1.attn.proj.bias', 'backbone.layers.3.blocks.1.norm2.weight', 'backbone.layers.3.blocks.1.norm2.bias', 'backbone.layers.3.blocks.1.mlp.fc1.weight', 'backbone.layers.3.blocks.1.mlp.fc1.bias', 'backbone.layers.3.blocks.1.mlp.fc2.weight', 'backbone.layers.3.blocks.1.mlp.fc2.bias', 'appearance_encoder.blocks.0.0.norm1.weight', 'appearance_encoder.blocks.0.0.norm1.bias', 'appearance_encoder.blocks.0.0.attn1.to_q.weight', 'appearance_encoder.blocks.0.0.attn1.to_k.weight', 'appearance_encoder.blocks.0.0.attn1.to_v.weight', 'appearance_encoder.blocks.0.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.0.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.0.0.norm2.weight', 'appearance_encoder.blocks.0.0.norm2.bias', 'appearance_encoder.blocks.0.0.attn2.to_q.weight', 'appearance_encoder.blocks.0.0.attn2.to_k.weight', 'appearance_encoder.blocks.0.0.attn2.to_v.weight', 'appearance_encoder.blocks.0.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.0.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.0.0.norm3.weight', 'appearance_encoder.blocks.0.0.norm3.bias', 'appearance_encoder.blocks.0.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.0.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.0.0.ff.net.2.weight', 'appearance_encoder.blocks.0.0.ff.net.2.bias', 'appearance_encoder.blocks.0.1.norm1.weight', 'appearance_encoder.blocks.0.1.norm1.bias', 'appearance_encoder.blocks.0.1.attn1.to_q.weight', 'appearance_encoder.blocks.0.1.attn1.to_k.weight', 'appearance_encoder.blocks.0.1.attn1.to_v.weight', 'appearance_encoder.blocks.0.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.0.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.0.1.norm2.weight', 'appearance_encoder.blocks.0.1.norm2.bias', 'appearance_encoder.blocks.0.1.attn2.to_q.weight', 'appearance_encoder.blocks.0.1.attn2.to_k.weight', 'appearance_encoder.blocks.0.1.attn2.to_v.weight', 'appearance_encoder.blocks.0.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.0.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.0.1.norm3.weight', 'appearance_encoder.blocks.0.1.norm3.bias', 'appearance_encoder.blocks.0.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.0.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.0.1.ff.net.2.weight', 'appearance_encoder.blocks.0.1.ff.net.2.bias', 'appearance_encoder.blocks.0.2.norm1.weight', 'appearance_encoder.blocks.0.2.norm1.bias', 'appearance_encoder.blocks.0.2.attn1.to_q.weight', 'appearance_encoder.blocks.0.2.attn1.to_k.weight', 'appearance_encoder.blocks.0.2.attn1.to_v.weight', 'appearance_encoder.blocks.0.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.0.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.0.2.norm2.weight', 'appearance_encoder.blocks.0.2.norm2.bias', 'appearance_encoder.blocks.0.2.attn2.to_q.weight', 'appearance_encoder.blocks.0.2.attn2.to_k.weight', 'appearance_encoder.blocks.0.2.attn2.to_v.weight', 'appearance_encoder.blocks.0.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.0.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.0.2.norm3.weight', 'appearance_encoder.blocks.0.2.norm3.bias', 'appearance_encoder.blocks.0.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.0.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.0.2.ff.net.2.weight', 'appearance_encoder.blocks.0.2.ff.net.2.bias', 'appearance_encoder.blocks.0.3.norm1.weight', 'appearance_encoder.blocks.0.3.norm1.bias', 'appearance_encoder.blocks.0.3.attn1.to_q.weight', 'appearance_encoder.blocks.0.3.attn1.to_k.weight', 'appearance_encoder.blocks.0.3.attn1.to_v.weight', 'appearance_encoder.blocks.0.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.0.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.0.3.norm2.weight', 'appearance_encoder.blocks.0.3.norm2.bias', 'appearance_encoder.blocks.0.3.attn2.to_q.weight', 'appearance_encoder.blocks.0.3.attn2.to_k.weight', 'appearance_encoder.blocks.0.3.attn2.to_v.weight', 'appearance_encoder.blocks.0.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.0.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.0.3.norm3.weight', 'appearance_encoder.blocks.0.3.norm3.bias', 'appearance_encoder.blocks.0.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.0.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.0.3.ff.net.2.weight', 'appearance_encoder.blocks.0.3.ff.net.2.bias', 'appearance_encoder.blocks.1.0.norm1.weight', 'appearance_encoder.blocks.1.0.norm1.bias', 'appearance_encoder.blocks.1.0.attn1.to_q.weight', 'appearance_encoder.blocks.1.0.attn1.to_k.weight', 'appearance_encoder.blocks.1.0.attn1.to_v.weight', 'appearance_encoder.blocks.1.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.1.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.1.0.norm2.weight', 'appearance_encoder.blocks.1.0.norm2.bias', 'appearance_encoder.blocks.1.0.attn2.to_q.weight', 'appearance_encoder.blocks.1.0.attn2.to_k.weight', 'appearance_encoder.blocks.1.0.attn2.to_v.weight', 'appearance_encoder.blocks.1.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.1.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.1.0.norm3.weight', 'appearance_encoder.blocks.1.0.norm3.bias', 'appearance_encoder.blocks.1.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.1.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.1.0.ff.net.2.weight', 'appearance_encoder.blocks.1.0.ff.net.2.bias', 'appearance_encoder.blocks.1.1.norm1.weight', 'appearance_encoder.blocks.1.1.norm1.bias', 'appearance_encoder.blocks.1.1.attn1.to_q.weight', 'appearance_encoder.blocks.1.1.attn1.to_k.weight', 'appearance_encoder.blocks.1.1.attn1.to_v.weight', 'appearance_encoder.blocks.1.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.1.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.1.1.norm2.weight', 'appearance_encoder.blocks.1.1.norm2.bias', 'appearance_encoder.blocks.1.1.attn2.to_q.weight', 'appearance_encoder.blocks.1.1.attn2.to_k.weight', 'appearance_encoder.blocks.1.1.attn2.to_v.weight', 'appearance_encoder.blocks.1.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.1.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.1.1.norm3.weight', 'appearance_encoder.blocks.1.1.norm3.bias', 'appearance_encoder.blocks.1.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.1.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.1.1.ff.net.2.weight', 'appearance_encoder.blocks.1.1.ff.net.2.bias', 'appearance_encoder.blocks.1.2.norm1.weight', 'appearance_encoder.blocks.1.2.norm1.bias', 'appearance_encoder.blocks.1.2.attn1.to_q.weight', 'appearance_encoder.blocks.1.2.attn1.to_k.weight', 'appearance_encoder.blocks.1.2.attn1.to_v.weight', 'appearance_encoder.blocks.1.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.1.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.1.2.norm2.weight', 'appearance_encoder.blocks.1.2.norm2.bias', 'appearance_encoder.blocks.1.2.attn2.to_q.weight', 'appearance_encoder.blocks.1.2.attn2.to_k.weight', 'appearance_encoder.blocks.1.2.attn2.to_v.weight', 'appearance_encoder.blocks.1.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.1.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.1.2.norm3.weight', 'appearance_encoder.blocks.1.2.norm3.bias', 'appearance_encoder.blocks.1.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.1.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.1.2.ff.net.2.weight', 'appearance_encoder.blocks.1.2.ff.net.2.bias', 'appearance_encoder.blocks.1.3.norm1.weight', 'appearance_encoder.blocks.1.3.norm1.bias', 'appearance_encoder.blocks.1.3.attn1.to_q.weight', 'appearance_encoder.blocks.1.3.attn1.to_k.weight', 'appearance_encoder.blocks.1.3.attn1.to_v.weight', 'appearance_encoder.blocks.1.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.1.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.1.3.norm2.weight', 'appearance_encoder.blocks.1.3.norm2.bias', 'appearance_encoder.blocks.1.3.attn2.to_q.weight', 'appearance_encoder.blocks.1.3.attn2.to_k.weight', 'appearance_encoder.blocks.1.3.attn2.to_v.weight', 'appearance_encoder.blocks.1.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.1.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.1.3.norm3.weight', 'appearance_encoder.blocks.1.3.norm3.bias', 'appearance_encoder.blocks.1.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.1.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.1.3.ff.net.2.weight', 'appearance_encoder.blocks.1.3.ff.net.2.bias', 'appearance_encoder.blocks.2.0.norm1.weight', 'appearance_encoder.blocks.2.0.norm1.bias', 'appearance_encoder.blocks.2.0.attn1.to_q.weight', 'appearance_encoder.blocks.2.0.attn1.to_k.weight', 'appearance_encoder.blocks.2.0.attn1.to_v.weight', 'appearance_encoder.blocks.2.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.2.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.2.0.norm2.weight', 'appearance_encoder.blocks.2.0.norm2.bias', 'appearance_encoder.blocks.2.0.attn2.to_q.weight', 'appearance_encoder.blocks.2.0.attn2.to_k.weight', 'appearance_encoder.blocks.2.0.attn2.to_v.weight', 'appearance_encoder.blocks.2.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.2.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.2.0.norm3.weight', 'appearance_encoder.blocks.2.0.norm3.bias', 'appearance_encoder.blocks.2.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.2.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.2.0.ff.net.2.weight', 'appearance_encoder.blocks.2.0.ff.net.2.bias', 'appearance_encoder.blocks.2.1.norm1.weight', 'appearance_encoder.blocks.2.1.norm1.bias', 'appearance_encoder.blocks.2.1.attn1.to_q.weight', 'appearance_encoder.blocks.2.1.attn1.to_k.weight', 'appearance_encoder.blocks.2.1.attn1.to_v.weight', 'appearance_encoder.blocks.2.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.2.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.2.1.norm2.weight', 'appearance_encoder.blocks.2.1.norm2.bias', 'appearance_encoder.blocks.2.1.attn2.to_q.weight', 'appearance_encoder.blocks.2.1.attn2.to_k.weight', 'appearance_encoder.blocks.2.1.attn2.to_v.weight', 'appearance_encoder.blocks.2.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.2.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.2.1.norm3.weight', 'appearance_encoder.blocks.2.1.norm3.bias', 'appearance_encoder.blocks.2.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.2.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.2.1.ff.net.2.weight', 'appearance_encoder.blocks.2.1.ff.net.2.bias', 'appearance_encoder.blocks.2.2.norm1.weight', 'appearance_encoder.blocks.2.2.norm1.bias', 'appearance_encoder.blocks.2.2.attn1.to_q.weight', 'appearance_encoder.blocks.2.2.attn1.to_k.weight', 'appearance_encoder.blocks.2.2.attn1.to_v.weight', 'appearance_encoder.blocks.2.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.2.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.2.2.norm2.weight', 'appearance_encoder.blocks.2.2.norm2.bias', 'appearance_encoder.blocks.2.2.attn2.to_q.weight', 'appearance_encoder.blocks.2.2.attn2.to_k.weight', 'appearance_encoder.blocks.2.2.attn2.to_v.weight', 'appearance_encoder.blocks.2.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.2.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.2.2.norm3.weight', 'appearance_encoder.blocks.2.2.norm3.bias', 'appearance_encoder.blocks.2.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.2.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.2.2.ff.net.2.weight', 'appearance_encoder.blocks.2.2.ff.net.2.bias', 'appearance_encoder.blocks.2.3.norm1.weight', 'appearance_encoder.blocks.2.3.norm1.bias', 'appearance_encoder.blocks.2.3.attn1.to_q.weight', 'appearance_encoder.blocks.2.3.attn1.to_k.weight', 'appearance_encoder.blocks.2.3.attn1.to_v.weight', 'appearance_encoder.blocks.2.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.2.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.2.3.norm2.weight', 'appearance_encoder.blocks.2.3.norm2.bias', 'appearance_encoder.blocks.2.3.attn2.to_q.weight', 'appearance_encoder.blocks.2.3.attn2.to_k.weight', 'appearance_encoder.blocks.2.3.attn2.to_v.weight', 'appearance_encoder.blocks.2.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.2.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.2.3.norm3.weight', 'appearance_encoder.blocks.2.3.norm3.bias', 'appearance_encoder.blocks.2.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.2.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.2.3.ff.net.2.weight', 'appearance_encoder.blocks.2.3.ff.net.2.bias', 'appearance_encoder.blocks.3.0.norm1.weight', 'appearance_encoder.blocks.3.0.norm1.bias', 'appearance_encoder.blocks.3.0.attn1.to_q.weight', 'appearance_encoder.blocks.3.0.attn1.to_k.weight', 'appearance_encoder.blocks.3.0.attn1.to_v.weight', 'appearance_encoder.blocks.3.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.3.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.3.0.norm2.weight', 'appearance_encoder.blocks.3.0.norm2.bias', 'appearance_encoder.blocks.3.0.attn2.to_q.weight', 'appearance_encoder.blocks.3.0.attn2.to_k.weight', 'appearance_encoder.blocks.3.0.attn2.to_v.weight', 'appearance_encoder.blocks.3.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.3.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.3.0.norm3.weight', 'appearance_encoder.blocks.3.0.norm3.bias', 'appearance_encoder.blocks.3.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.3.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.3.0.ff.net.2.weight', 'appearance_encoder.blocks.3.0.ff.net.2.bias', 'appearance_encoder.blocks.3.1.norm1.weight', 'appearance_encoder.blocks.3.1.norm1.bias', 'appearance_encoder.blocks.3.1.attn1.to_q.weight', 'appearance_encoder.blocks.3.1.attn1.to_k.weight', 'appearance_encoder.blocks.3.1.attn1.to_v.weight', 'appearance_encoder.blocks.3.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.3.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.3.1.norm2.weight', 'appearance_encoder.blocks.3.1.norm2.bias', 'appearance_encoder.blocks.3.1.attn2.to_q.weight', 'appearance_encoder.blocks.3.1.attn2.to_k.weight', 'appearance_encoder.blocks.3.1.attn2.to_v.weight', 'appearance_encoder.blocks.3.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.3.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.3.1.norm3.weight', 'appearance_encoder.blocks.3.1.norm3.bias', 'appearance_encoder.blocks.3.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.3.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.3.1.ff.net.2.weight', 'appearance_encoder.blocks.3.1.ff.net.2.bias', 'appearance_encoder.blocks.3.2.norm1.weight', 'appearance_encoder.blocks.3.2.norm1.bias', 'appearance_encoder.blocks.3.2.attn1.to_q.weight', 'appearance_encoder.blocks.3.2.attn1.to_k.weight', 'appearance_encoder.blocks.3.2.attn1.to_v.weight', 'appearance_encoder.blocks.3.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.3.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.3.2.norm2.weight', 'appearance_encoder.blocks.3.2.norm2.bias', 'appearance_encoder.blocks.3.2.attn2.to_q.weight', 'appearance_encoder.blocks.3.2.attn2.to_k.weight', 'appearance_encoder.blocks.3.2.attn2.to_v.weight', 'appearance_encoder.blocks.3.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.3.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.3.2.norm3.weight', 'appearance_encoder.blocks.3.2.norm3.bias', 'appearance_encoder.blocks.3.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.3.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.3.2.ff.net.2.weight', 'appearance_encoder.blocks.3.2.ff.net.2.bias', 'appearance_encoder.blocks.3.3.norm1.weight', 'appearance_encoder.blocks.3.3.norm1.bias', 'appearance_encoder.blocks.3.3.attn1.to_q.weight', 'appearance_encoder.blocks.3.3.attn1.to_k.weight', 'appearance_encoder.blocks.3.3.attn1.to_v.weight', 'appearance_encoder.blocks.3.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.3.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.3.3.norm2.weight', 'appearance_encoder.blocks.3.3.norm2.bias', 'appearance_encoder.blocks.3.3.attn2.to_q.weight', 'appearance_encoder.blocks.3.3.attn2.to_k.weight', 'appearance_encoder.blocks.3.3.attn2.to_v.weight', 'appearance_encoder.blocks.3.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.3.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.3.3.norm3.weight', 'appearance_encoder.blocks.3.3.norm3.bias', 'appearance_encoder.blocks.3.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.3.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.3.3.ff.net.2.weight', 'appearance_encoder.blocks.3.3.ff.net.2.bias', 'appearance_encoder.blocks.4.0.norm1.weight', 'appearance_encoder.blocks.4.0.norm1.bias', 'appearance_encoder.blocks.4.0.attn1.to_q.weight', 'appearance_encoder.blocks.4.0.attn1.to_k.weight', 'appearance_encoder.blocks.4.0.attn1.to_v.weight', 'appearance_encoder.blocks.4.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.4.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.4.0.norm2.weight', 'appearance_encoder.blocks.4.0.norm2.bias', 'appearance_encoder.blocks.4.0.attn2.to_q.weight', 'appearance_encoder.blocks.4.0.attn2.to_k.weight', 'appearance_encoder.blocks.4.0.attn2.to_v.weight', 'appearance_encoder.blocks.4.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.4.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.4.0.norm3.weight', 'appearance_encoder.blocks.4.0.norm3.bias', 'appearance_encoder.blocks.4.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.4.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.4.0.ff.net.2.weight', 'appearance_encoder.blocks.4.0.ff.net.2.bias', 'appearance_encoder.blocks.4.1.norm1.weight', 'appearance_encoder.blocks.4.1.norm1.bias', 'appearance_encoder.blocks.4.1.attn1.to_q.weight', 'appearance_encoder.blocks.4.1.attn1.to_k.weight', 'appearance_encoder.blocks.4.1.attn1.to_v.weight', 'appearance_encoder.blocks.4.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.4.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.4.1.norm2.weight', 'appearance_encoder.blocks.4.1.norm2.bias', 'appearance_encoder.blocks.4.1.attn2.to_q.weight', 'appearance_encoder.blocks.4.1.attn2.to_k.weight', 'appearance_encoder.blocks.4.1.attn2.to_v.weight', 'appearance_encoder.blocks.4.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.4.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.4.1.norm3.weight', 'appearance_encoder.blocks.4.1.norm3.bias', 'appearance_encoder.blocks.4.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.4.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.4.1.ff.net.2.weight', 'appearance_encoder.blocks.4.1.ff.net.2.bias', 'appearance_encoder.blocks.4.2.norm1.weight', 'appearance_encoder.blocks.4.2.norm1.bias', 'appearance_encoder.blocks.4.2.attn1.to_q.weight', 'appearance_encoder.blocks.4.2.attn1.to_k.weight', 'appearance_encoder.blocks.4.2.attn1.to_v.weight', 'appearance_encoder.blocks.4.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.4.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.4.2.norm2.weight', 'appearance_encoder.blocks.4.2.norm2.bias', 'appearance_encoder.blocks.4.2.attn2.to_q.weight', 'appearance_encoder.blocks.4.2.attn2.to_k.weight', 'appearance_encoder.blocks.4.2.attn2.to_v.weight', 'appearance_encoder.blocks.4.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.4.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.4.2.norm3.weight', 'appearance_encoder.blocks.4.2.norm3.bias', 'appearance_encoder.blocks.4.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.4.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.4.2.ff.net.2.weight', 'appearance_encoder.blocks.4.2.ff.net.2.bias', 'appearance_encoder.blocks.4.3.norm1.weight', 'appearance_encoder.blocks.4.3.norm1.bias', 'appearance_encoder.blocks.4.3.attn1.to_q.weight', 'appearance_encoder.blocks.4.3.attn1.to_k.weight', 'appearance_encoder.blocks.4.3.attn1.to_v.weight', 'appearance_encoder.blocks.4.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.4.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.4.3.norm2.weight', 'appearance_encoder.blocks.4.3.norm2.bias', 'appearance_encoder.blocks.4.3.attn2.to_q.weight', 'appearance_encoder.blocks.4.3.attn2.to_k.weight', 'appearance_encoder.blocks.4.3.attn2.to_v.weight', 'appearance_encoder.blocks.4.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.4.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.4.3.norm3.weight', 'appearance_encoder.blocks.4.3.norm3.bias', 'appearance_encoder.blocks.4.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.4.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.4.3.ff.net.2.weight', 'appearance_encoder.blocks.4.3.ff.net.2.bias', 'appearance_encoder.blocks.5.0.norm1.weight', 'appearance_encoder.blocks.5.0.norm1.bias', 'appearance_encoder.blocks.5.0.attn1.to_q.weight', 'appearance_encoder.blocks.5.0.attn1.to_k.weight', 'appearance_encoder.blocks.5.0.attn1.to_v.weight', 'appearance_encoder.blocks.5.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.5.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.5.0.norm2.weight', 'appearance_encoder.blocks.5.0.norm2.bias', 'appearance_encoder.blocks.5.0.attn2.to_q.weight', 'appearance_encoder.blocks.5.0.attn2.to_k.weight', 'appearance_encoder.blocks.5.0.attn2.to_v.weight', 'appearance_encoder.blocks.5.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.5.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.5.0.norm3.weight', 'appearance_encoder.blocks.5.0.norm3.bias', 'appearance_encoder.blocks.5.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.5.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.5.0.ff.net.2.weight', 'appearance_encoder.blocks.5.0.ff.net.2.bias', 'appearance_encoder.blocks.5.1.norm1.weight', 'appearance_encoder.blocks.5.1.norm1.bias', 'appearance_encoder.blocks.5.1.attn1.to_q.weight', 'appearance_encoder.blocks.5.1.attn1.to_k.weight', 'appearance_encoder.blocks.5.1.attn1.to_v.weight', 'appearance_encoder.blocks.5.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.5.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.5.1.norm2.weight', 'appearance_encoder.blocks.5.1.norm2.bias', 'appearance_encoder.blocks.5.1.attn2.to_q.weight', 'appearance_encoder.blocks.5.1.attn2.to_k.weight', 'appearance_encoder.blocks.5.1.attn2.to_v.weight', 'appearance_encoder.blocks.5.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.5.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.5.1.norm3.weight', 'appearance_encoder.blocks.5.1.norm3.bias', 'appearance_encoder.blocks.5.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.5.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.5.1.ff.net.2.weight', 'appearance_encoder.blocks.5.1.ff.net.2.bias', 'appearance_encoder.blocks.5.2.norm1.weight', 'appearance_encoder.blocks.5.2.norm1.bias', 'appearance_encoder.blocks.5.2.attn1.to_q.weight', 'appearance_encoder.blocks.5.2.attn1.to_k.weight', 'appearance_encoder.blocks.5.2.attn1.to_v.weight', 'appearance_encoder.blocks.5.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.5.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.5.2.norm2.weight', 'appearance_encoder.blocks.5.2.norm2.bias', 'appearance_encoder.blocks.5.2.attn2.to_q.weight', 'appearance_encoder.blocks.5.2.attn2.to_k.weight', 'appearance_encoder.blocks.5.2.attn2.to_v.weight', 'appearance_encoder.blocks.5.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.5.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.5.2.norm3.weight', 'appearance_encoder.blocks.5.2.norm3.bias', 'appearance_encoder.blocks.5.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.5.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.5.2.ff.net.2.weight', 'appearance_encoder.blocks.5.2.ff.net.2.bias', 'appearance_encoder.blocks.5.3.norm1.weight', 'appearance_encoder.blocks.5.3.norm1.bias', 'appearance_encoder.blocks.5.3.attn1.to_q.weight', 'appearance_encoder.blocks.5.3.attn1.to_k.weight', 'appearance_encoder.blocks.5.3.attn1.to_v.weight', 'appearance_encoder.blocks.5.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.5.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.5.3.norm2.weight', 'appearance_encoder.blocks.5.3.norm2.bias', 'appearance_encoder.blocks.5.3.attn2.to_q.weight', 'appearance_encoder.blocks.5.3.attn2.to_k.weight', 'appearance_encoder.blocks.5.3.attn2.to_v.weight', 'appearance_encoder.blocks.5.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.5.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.5.3.norm3.weight', 'appearance_encoder.blocks.5.3.norm3.bias', 'appearance_encoder.blocks.5.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.5.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.5.3.ff.net.2.weight', 'appearance_encoder.blocks.5.3.ff.net.2.bias', 'appearance_encoder.blocks.6.0.norm1.weight', 'appearance_encoder.blocks.6.0.norm1.bias', 'appearance_encoder.blocks.6.0.attn1.to_q.weight', 'appearance_encoder.blocks.6.0.attn1.to_k.weight', 'appearance_encoder.blocks.6.0.attn1.to_v.weight', 'appearance_encoder.blocks.6.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.6.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.6.0.norm2.weight', 'appearance_encoder.blocks.6.0.norm2.bias', 'appearance_encoder.blocks.6.0.attn2.to_q.weight', 'appearance_encoder.blocks.6.0.attn2.to_k.weight', 'appearance_encoder.blocks.6.0.attn2.to_v.weight', 'appearance_encoder.blocks.6.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.6.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.6.0.norm3.weight', 'appearance_encoder.blocks.6.0.norm3.bias', 'appearance_encoder.blocks.6.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.6.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.6.0.ff.net.2.weight', 'appearance_encoder.blocks.6.0.ff.net.2.bias', 'appearance_encoder.blocks.6.1.norm1.weight', 'appearance_encoder.blocks.6.1.norm1.bias', 'appearance_encoder.blocks.6.1.attn1.to_q.weight', 'appearance_encoder.blocks.6.1.attn1.to_k.weight', 'appearance_encoder.blocks.6.1.attn1.to_v.weight', 'appearance_encoder.blocks.6.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.6.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.6.1.norm2.weight', 'appearance_encoder.blocks.6.1.norm2.bias', 'appearance_encoder.blocks.6.1.attn2.to_q.weight', 'appearance_encoder.blocks.6.1.attn2.to_k.weight', 'appearance_encoder.blocks.6.1.attn2.to_v.weight', 'appearance_encoder.blocks.6.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.6.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.6.1.norm3.weight', 'appearance_encoder.blocks.6.1.norm3.bias', 'appearance_encoder.blocks.6.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.6.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.6.1.ff.net.2.weight', 'appearance_encoder.blocks.6.1.ff.net.2.bias', 'appearance_encoder.blocks.6.2.norm1.weight', 'appearance_encoder.blocks.6.2.norm1.bias', 'appearance_encoder.blocks.6.2.attn1.to_q.weight', 'appearance_encoder.blocks.6.2.attn1.to_k.weight', 'appearance_encoder.blocks.6.2.attn1.to_v.weight', 'appearance_encoder.blocks.6.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.6.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.6.2.norm2.weight', 'appearance_encoder.blocks.6.2.norm2.bias', 'appearance_encoder.blocks.6.2.attn2.to_q.weight', 'appearance_encoder.blocks.6.2.attn2.to_k.weight', 'appearance_encoder.blocks.6.2.attn2.to_v.weight', 'appearance_encoder.blocks.6.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.6.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.6.2.norm3.weight', 'appearance_encoder.blocks.6.2.norm3.bias', 'appearance_encoder.blocks.6.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.6.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.6.2.ff.net.2.weight', 'appearance_encoder.blocks.6.2.ff.net.2.bias', 'appearance_encoder.blocks.6.3.norm1.weight', 'appearance_encoder.blocks.6.3.norm1.bias', 'appearance_encoder.blocks.6.3.attn1.to_q.weight', 'appearance_encoder.blocks.6.3.attn1.to_k.weight', 'appearance_encoder.blocks.6.3.attn1.to_v.weight', 'appearance_encoder.blocks.6.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.6.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.6.3.norm2.weight', 'appearance_encoder.blocks.6.3.norm2.bias', 'appearance_encoder.blocks.6.3.attn2.to_q.weight', 'appearance_encoder.blocks.6.3.attn2.to_k.weight', 'appearance_encoder.blocks.6.3.attn2.to_v.weight', 'appearance_encoder.blocks.6.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.6.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.6.3.norm3.weight', 'appearance_encoder.blocks.6.3.norm3.bias', 'appearance_encoder.blocks.6.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.6.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.6.3.ff.net.2.weight', 'appearance_encoder.blocks.6.3.ff.net.2.bias', 'appearance_encoder.blocks.7.0.norm1.weight', 'appearance_encoder.blocks.7.0.norm1.bias', 'appearance_encoder.blocks.7.0.attn1.to_q.weight', 'appearance_encoder.blocks.7.0.attn1.to_k.weight', 'appearance_encoder.blocks.7.0.attn1.to_v.weight', 'appearance_encoder.blocks.7.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.7.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.7.0.norm2.weight', 'appearance_encoder.blocks.7.0.norm2.bias', 'appearance_encoder.blocks.7.0.attn2.to_q.weight', 'appearance_encoder.blocks.7.0.attn2.to_k.weight', 'appearance_encoder.blocks.7.0.attn2.to_v.weight', 'appearance_encoder.blocks.7.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.7.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.7.0.norm3.weight', 'appearance_encoder.blocks.7.0.norm3.bias', 'appearance_encoder.blocks.7.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.7.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.7.0.ff.net.2.weight', 'appearance_encoder.blocks.7.0.ff.net.2.bias', 'appearance_encoder.blocks.7.1.norm1.weight', 'appearance_encoder.blocks.7.1.norm1.bias', 'appearance_encoder.blocks.7.1.attn1.to_q.weight', 'appearance_encoder.blocks.7.1.attn1.to_k.weight', 'appearance_encoder.blocks.7.1.attn1.to_v.weight', 'appearance_encoder.blocks.7.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.7.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.7.1.norm2.weight', 'appearance_encoder.blocks.7.1.norm2.bias', 'appearance_encoder.blocks.7.1.attn2.to_q.weight', 'appearance_encoder.blocks.7.1.attn2.to_k.weight', 'appearance_encoder.blocks.7.1.attn2.to_v.weight', 'appearance_encoder.blocks.7.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.7.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.7.1.norm3.weight', 'appearance_encoder.blocks.7.1.norm3.bias', 'appearance_encoder.blocks.7.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.7.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.7.1.ff.net.2.weight', 'appearance_encoder.blocks.7.1.ff.net.2.bias', 'appearance_encoder.blocks.7.2.norm1.weight', 'appearance_encoder.blocks.7.2.norm1.bias', 'appearance_encoder.blocks.7.2.attn1.to_q.weight', 'appearance_encoder.blocks.7.2.attn1.to_k.weight', 'appearance_encoder.blocks.7.2.attn1.to_v.weight', 'appearance_encoder.blocks.7.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.7.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.7.2.norm2.weight', 'appearance_encoder.blocks.7.2.norm2.bias', 'appearance_encoder.blocks.7.2.attn2.to_q.weight', 'appearance_encoder.blocks.7.2.attn2.to_k.weight', 'appearance_encoder.blocks.7.2.attn2.to_v.weight', 'appearance_encoder.blocks.7.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.7.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.7.2.norm3.weight', 'appearance_encoder.blocks.7.2.norm3.bias', 'appearance_encoder.blocks.7.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.7.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.7.2.ff.net.2.weight', 'appearance_encoder.blocks.7.2.ff.net.2.bias', 'appearance_encoder.blocks.7.3.norm1.weight', 'appearance_encoder.blocks.7.3.norm1.bias', 'appearance_encoder.blocks.7.3.attn1.to_q.weight', 'appearance_encoder.blocks.7.3.attn1.to_k.weight', 'appearance_encoder.blocks.7.3.attn1.to_v.weight', 'appearance_encoder.blocks.7.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.7.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.7.3.norm2.weight', 'appearance_encoder.blocks.7.3.norm2.bias', 'appearance_encoder.blocks.7.3.attn2.to_q.weight', 'appearance_encoder.blocks.7.3.attn2.to_k.weight', 'appearance_encoder.blocks.7.3.attn2.to_v.weight', 'appearance_encoder.blocks.7.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.7.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.7.3.norm3.weight', 'appearance_encoder.blocks.7.3.norm3.bias', 'appearance_encoder.blocks.7.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.7.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.7.3.ff.net.2.weight', 'appearance_encoder.blocks.7.3.ff.net.2.bias', 'appearance_encoder.blocks.8.0.norm1.weight', 'appearance_encoder.blocks.8.0.norm1.bias', 'appearance_encoder.blocks.8.0.attn1.to_q.weight', 'appearance_encoder.blocks.8.0.attn1.to_k.weight', 'appearance_encoder.blocks.8.0.attn1.to_v.weight', 'appearance_encoder.blocks.8.0.attn1.to_out.0.weight', 'appearance_encoder.blocks.8.0.attn1.to_out.0.bias', 'appearance_encoder.blocks.8.0.norm2.weight', 'appearance_encoder.blocks.8.0.norm2.bias', 'appearance_encoder.blocks.8.0.attn2.to_q.weight', 'appearance_encoder.blocks.8.0.attn2.to_k.weight', 'appearance_encoder.blocks.8.0.attn2.to_v.weight', 'appearance_encoder.blocks.8.0.attn2.to_out.0.weight', 'appearance_encoder.blocks.8.0.attn2.to_out.0.bias', 'appearance_encoder.blocks.8.0.norm3.weight', 'appearance_encoder.blocks.8.0.norm3.bias', 'appearance_encoder.blocks.8.0.ff.net.0.proj.weight', 'appearance_encoder.blocks.8.0.ff.net.0.proj.bias', 'appearance_encoder.blocks.8.0.ff.net.2.weight', 'appearance_encoder.blocks.8.0.ff.net.2.bias', 'appearance_encoder.blocks.8.1.norm1.weight', 'appearance_encoder.blocks.8.1.norm1.bias', 'appearance_encoder.blocks.8.1.attn1.to_q.weight', 'appearance_encoder.blocks.8.1.attn1.to_k.weight', 'appearance_encoder.blocks.8.1.attn1.to_v.weight', 'appearance_encoder.blocks.8.1.attn1.to_out.0.weight', 'appearance_encoder.blocks.8.1.attn1.to_out.0.bias', 'appearance_encoder.blocks.8.1.norm2.weight', 'appearance_encoder.blocks.8.1.norm2.bias', 'appearance_encoder.blocks.8.1.attn2.to_q.weight', 'appearance_encoder.blocks.8.1.attn2.to_k.weight', 'appearance_encoder.blocks.8.1.attn2.to_v.weight', 'appearance_encoder.blocks.8.1.attn2.to_out.0.weight', 'appearance_encoder.blocks.8.1.attn2.to_out.0.bias', 'appearance_encoder.blocks.8.1.norm3.weight', 'appearance_encoder.blocks.8.1.norm3.bias', 'appearance_encoder.blocks.8.1.ff.net.0.proj.weight', 'appearance_encoder.blocks.8.1.ff.net.0.proj.bias', 'appearance_encoder.blocks.8.1.ff.net.2.weight', 'appearance_encoder.blocks.8.1.ff.net.2.bias', 'appearance_encoder.blocks.8.2.norm1.weight', 'appearance_encoder.blocks.8.2.norm1.bias', 'appearance_encoder.blocks.8.2.attn1.to_q.weight', 'appearance_encoder.blocks.8.2.attn1.to_k.weight', 'appearance_encoder.blocks.8.2.attn1.to_v.weight', 'appearance_encoder.blocks.8.2.attn1.to_out.0.weight', 'appearance_encoder.blocks.8.2.attn1.to_out.0.bias', 'appearance_encoder.blocks.8.2.norm2.weight', 'appearance_encoder.blocks.8.2.norm2.bias', 'appearance_encoder.blocks.8.2.attn2.to_q.weight', 'appearance_encoder.blocks.8.2.attn2.to_k.weight', 'appearance_encoder.blocks.8.2.attn2.to_v.weight', 'appearance_encoder.blocks.8.2.attn2.to_out.0.weight', 'appearance_encoder.blocks.8.2.attn2.to_out.0.bias', 'appearance_encoder.blocks.8.2.norm3.weight', 'appearance_encoder.blocks.8.2.norm3.bias', 'appearance_encoder.blocks.8.2.ff.net.0.proj.weight', 'appearance_encoder.blocks.8.2.ff.net.0.proj.bias', 'appearance_encoder.blocks.8.2.ff.net.2.weight', 'appearance_encoder.blocks.8.2.ff.net.2.bias', 'appearance_encoder.blocks.8.3.norm1.weight', 'appearance_encoder.blocks.8.3.norm1.bias', 'appearance_encoder.blocks.8.3.attn1.to_q.weight', 'appearance_encoder.blocks.8.3.attn1.to_k.weight', 'appearance_encoder.blocks.8.3.attn1.to_v.weight', 'appearance_encoder.blocks.8.3.attn1.to_out.0.weight', 'appearance_encoder.blocks.8.3.attn1.to_out.0.bias', 'appearance_encoder.blocks.8.3.norm2.weight', 'appearance_encoder.blocks.8.3.norm2.bias', 'appearance_encoder.blocks.8.3.attn2.to_q.weight', 'appearance_encoder.blocks.8.3.attn2.to_k.weight', 'appearance_encoder.blocks.8.3.attn2.to_v.weight', 'appearance_encoder.blocks.8.3.attn2.to_out.0.weight', 'appearance_encoder.blocks.8.3.attn2.to_out.0.bias', 'appearance_encoder.blocks.8.3.norm3.weight', 'appearance_encoder.blocks.8.3.norm3.bias', 'appearance_encoder.blocks.8.3.ff.net.0.proj.weight', 'appearance_encoder.blocks.8.3.ff.net.0.proj.bias', 'appearance_encoder.blocks.8.3.ff.net.2.weight', 'appearance_encoder.blocks.8.3.ff.net.2.bias', 'appearance_encoder.zero_conv_ins.0.weight', 'appearance_encoder.zero_conv_ins.0.bias', 'appearance_encoder.zero_conv_ins.1.weight', 'appearance_encoder.zero_conv_ins.1.bias', 'appearance_encoder.zero_conv_ins.2.weight', 'appearance_encoder.zero_conv_ins.2.bias', 'appearance_encoder.zero_conv_ins.3.weight', 'appearance_encoder.zero_conv_ins.3.bias', 'appearance_encoder.zero_conv_ins.4.weight', 'appearance_encoder.zero_conv_ins.4.bias', 'appearance_encoder.zero_conv_ins.5.weight', 'appearance_encoder.zero_conv_ins.5.bias', 'appearance_encoder.zero_conv_ins.6.weight', 'appearance_encoder.zero_conv_ins.6.bias', 'appearance_encoder.zero_conv_ins.7.weight', 'appearance_encoder.zero_conv_ins.7.bias', 'appearance_encoder.zero_conv_ins.8.weight', 'appearance_encoder.zero_conv_ins.8.bias', 'appearance_encoder.zero_conv_outs.0.weight', 'appearance_encoder.zero_conv_outs.0.bias', 'appearance_encoder.zero_conv_outs.1.weight', 'appearance_encoder.zero_conv_outs.1.bias', 'appearance_encoder.zero_conv_outs.2.weight', 'appearance_encoder.zero_conv_outs.2.bias', 'appearance_encoder.zero_conv_outs.3.weight', 'appearance_encoder.zero_conv_outs.3.bias', 'appearance_encoder.zero_conv_outs.4.weight', 'appearance_encoder.zero_conv_outs.4.bias', 'appearance_encoder.zero_conv_outs.5.weight', 'appearance_encoder.zero_conv_outs.5.bias', 'appearance_encoder.zero_conv_outs.6.weight', 'appearance_encoder.zero_conv_outs.6.bias', 'appearance_encoder.zero_conv_outs.7.weight', 'appearance_encoder.zero_conv_outs.7.bias', 'appearance_encoder.zero_conv_outs.8.weight', 'appearance_encoder.zero_conv_outs.8.bias', 'pose_encoder.conv_in.weight', 'pose_encoder.conv_in.bias', 'pose_encoder.resnets.0.norm1.weight', 'pose_encoder.resnets.0.norm1.bias', 'pose_encoder.resnets.0.conv1.weight', 'pose_encoder.resnets.0.conv1.bias', 'pose_encoder.resnets.0.norm2.weight', 'pose_encoder.resnets.0.norm2.bias', 'pose_encoder.resnets.0.conv2.weight', 'pose_encoder.resnets.0.conv2.bias', 'pose_encoder.resnets.1.norm1.weight', 'pose_encoder.resnets.1.norm1.bias', 'pose_encoder.resnets.1.conv1.weight', 'pose_encoder.resnets.1.conv1.bias', 'pose_encoder.resnets.1.norm2.weight', 'pose_encoder.resnets.1.norm2.bias', 'pose_encoder.resnets.1.conv2.weight', 'pose_encoder.resnets.1.conv2.bias', 'pose_encoder.resnets.1.conv_shortcut.weight', 'pose_encoder.resnets.1.conv_shortcut.bias', 'pose_encoder.resnets.2.norm1.weight', 'pose_encoder.resnets.2.norm1.bias', 'pose_encoder.resnets.2.conv1.weight', 'pose_encoder.resnets.2.conv1.bias', 'pose_encoder.resnets.2.norm2.weight', 'pose_encoder.resnets.2.norm2.bias', 'pose_encoder.resnets.2.conv2.weight', 'pose_encoder.resnets.2.conv2.bias', 'pose_encoder.resnets.2.conv_shortcut.weight', 'pose_encoder.resnets.2.conv_shortcut.bias'])
[2024-03-20 07:34:55 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['module.model.conv_in.weight', 'module.model.conv_in.bias', 'module.model.time_embedding.linear_1.weight', 'module.model.time_embedding.linear_1.bias', 'module.model.time_embedding.linear_2.weight', 'module.model.time_embedding.linear_2.bias', 'module.model.down_blocks.0.attentions.0.norm.weight', 'module.model.down_blocks.0.attentions.0.norm.bias', 'module.model.down_blocks.0.attentions.0.proj_in.weight', 'module.model.down_blocks.0.attentions.0.proj_in.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.0.attentions.0.proj_out.weight', 'module.model.down_blocks.0.attentions.0.proj_out.bias', 'module.model.down_blocks.0.attentions.1.norm.weight', 'module.model.down_blocks.0.attentions.1.norm.bias', 'module.model.down_blocks.0.attentions.1.proj_in.weight', 'module.model.down_blocks.0.attentions.1.proj_in.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.0.attentions.1.proj_out.weight', 'module.model.down_blocks.0.attentions.1.proj_out.bias', 'module.model.down_blocks.0.resnets.0.norm1.weight', 'module.model.down_blocks.0.resnets.0.norm1.bias', 'module.model.down_blocks.0.resnets.0.conv1.weight', 'module.model.down_blocks.0.resnets.0.conv1.bias', 'module.model.down_blocks.0.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.0.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.0.resnets.0.norm2.weight', 'module.model.down_blocks.0.resnets.0.norm2.bias', 'module.model.down_blocks.0.resnets.0.conv2.weight', 'module.model.down_blocks.0.resnets.0.conv2.bias', 'module.model.down_blocks.0.resnets.1.norm1.weight', 'module.model.down_blocks.0.resnets.1.norm1.bias', 'module.model.down_blocks.0.resnets.1.conv1.weight', 'module.model.down_blocks.0.resnets.1.conv1.bias', 'module.model.down_blocks.0.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.0.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.0.resnets.1.norm2.weight', 'module.model.down_blocks.0.resnets.1.norm2.bias', 'module.model.down_blocks.0.resnets.1.conv2.weight', 'module.model.down_blocks.0.resnets.1.conv2.bias', 'module.model.down_blocks.0.downsamplers.0.conv.weight', 'module.model.down_blocks.0.downsamplers.0.conv.bias', 'module.model.down_blocks.1.attentions.0.norm.weight', 'module.model.down_blocks.1.attentions.0.norm.bias', 'module.model.down_blocks.1.attentions.0.proj_in.weight', 'module.model.down_blocks.1.attentions.0.proj_in.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.1.attentions.0.proj_out.weight', 'module.model.down_blocks.1.attentions.0.proj_out.bias', 'module.model.down_blocks.1.attentions.1.norm.weight', 'module.model.down_blocks.1.attentions.1.norm.bias', 'module.model.down_blocks.1.attentions.1.proj_in.weight', 'module.model.down_blocks.1.attentions.1.proj_in.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.1.attentions.1.proj_out.weight', 'module.model.down_blocks.1.attentions.1.proj_out.bias', 'module.model.down_blocks.1.resnets.0.norm1.weight', 'module.model.down_blocks.1.resnets.0.norm1.bias', 'module.model.down_blocks.1.resnets.0.conv1.weight', 'module.model.down_blocks.1.resnets.0.conv1.bias', 'module.model.down_blocks.1.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.1.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.1.resnets.0.norm2.weight', 'module.model.down_blocks.1.resnets.0.norm2.bias', 'module.model.down_blocks.1.resnets.0.conv2.weight', 'module.model.down_blocks.1.resnets.0.conv2.bias', 'module.model.down_blocks.1.resnets.0.conv_shortcut.weight', 'module.model.down_blocks.1.resnets.0.conv_shortcut.bias', 'module.model.down_blocks.1.resnets.1.norm1.weight', 'module.model.down_blocks.1.resnets.1.norm1.bias', 'module.model.down_blocks.1.resnets.1.conv1.weight', 'module.model.down_blocks.1.resnets.1.conv1.bias', 'module.model.down_blocks.1.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.1.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.1.resnets.1.norm2.weight', 'module.model.down_blocks.1.resnets.1.norm2.bias', 'module.model.down_blocks.1.resnets.1.conv2.weight', 'module.model.down_blocks.1.resnets.1.conv2.bias', 'module.model.down_blocks.1.downsamplers.0.conv.weight', 'module.model.down_blocks.1.downsamplers.0.conv.bias', 'module.model.down_blocks.2.attentions.0.norm.weight', 'module.model.down_blocks.2.attentions.0.norm.bias', 'module.model.down_blocks.2.attentions.0.proj_in.weight', 'module.model.down_blocks.2.attentions.0.proj_in.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.2.attentions.0.proj_out.weight', 'module.model.down_blocks.2.attentions.0.proj_out.bias', 'module.model.down_blocks.2.attentions.1.norm.weight', 'module.model.down_blocks.2.attentions.1.norm.bias', 'module.model.down_blocks.2.attentions.1.proj_in.weight', 'module.model.down_blocks.2.attentions.1.proj_in.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.down_blocks.2.attentions.1.proj_out.weight', 'module.model.down_blocks.2.attentions.1.proj_out.bias', 'module.model.down_blocks.2.resnets.0.norm1.weight', 'module.model.down_blocks.2.resnets.0.norm1.bias', 'module.model.down_blocks.2.resnets.0.conv1.weight', 'module.model.down_blocks.2.resnets.0.conv1.bias', 'module.model.down_blocks.2.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.2.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.2.resnets.0.norm2.weight', 'module.model.down_blocks.2.resnets.0.norm2.bias', 'module.model.down_blocks.2.resnets.0.conv2.weight', 'module.model.down_blocks.2.resnets.0.conv2.bias', 'module.model.down_blocks.2.resnets.0.conv_shortcut.weight', 'module.model.down_blocks.2.resnets.0.conv_shortcut.bias', 'module.model.down_blocks.2.resnets.1.norm1.weight', 'module.model.down_blocks.2.resnets.1.norm1.bias', 'module.model.down_blocks.2.resnets.1.conv1.weight', 'module.model.down_blocks.2.resnets.1.conv1.bias', 'module.model.down_blocks.2.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.2.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.2.resnets.1.norm2.weight', 'module.model.down_blocks.2.resnets.1.norm2.bias', 'module.model.down_blocks.2.resnets.1.conv2.weight', 'module.model.down_blocks.2.resnets.1.conv2.bias', 'module.model.down_blocks.2.downsamplers.0.conv.weight', 'module.model.down_blocks.2.downsamplers.0.conv.bias', 'module.model.down_blocks.3.resnets.0.norm1.weight', 'module.model.down_blocks.3.resnets.0.norm1.bias', 'module.model.down_blocks.3.resnets.0.conv1.weight', 'module.model.down_blocks.3.resnets.0.conv1.bias', 'module.model.down_blocks.3.resnets.0.time_emb_proj.weight', 'module.model.down_blocks.3.resnets.0.time_emb_proj.bias', 'module.model.down_blocks.3.resnets.0.norm2.weight', 'module.model.down_blocks.3.resnets.0.norm2.bias', 'module.model.down_blocks.3.resnets.0.conv2.weight', 'module.model.down_blocks.3.resnets.0.conv2.bias', 'module.model.down_blocks.3.resnets.1.norm1.weight', 'module.model.down_blocks.3.resnets.1.norm1.bias', 'module.model.down_blocks.3.resnets.1.conv1.weight', 'module.model.down_blocks.3.resnets.1.conv1.bias', 'module.model.down_blocks.3.resnets.1.time_emb_proj.weight', 'module.model.down_blocks.3.resnets.1.time_emb_proj.bias', 'module.model.down_blocks.3.resnets.1.norm2.weight', 'module.model.down_blocks.3.resnets.1.norm2.bias', 'module.model.down_blocks.3.resnets.1.conv2.weight', 'module.model.down_blocks.3.resnets.1.conv2.bias', 'module.model.up_blocks.0.resnets.0.norm1.weight', 'module.model.up_blocks.0.resnets.0.norm1.bias', 'module.model.up_blocks.0.resnets.0.conv1.weight', 'module.model.up_blocks.0.resnets.0.conv1.bias', 'module.model.up_blocks.0.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.0.norm2.weight', 'module.model.up_blocks.0.resnets.0.norm2.bias', 'module.model.up_blocks.0.resnets.0.conv2.weight', 'module.model.up_blocks.0.resnets.0.conv2.bias', 'module.model.up_blocks.0.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.0.resnets.1.norm1.weight', 'module.model.up_blocks.0.resnets.1.norm1.bias', 'module.model.up_blocks.0.resnets.1.conv1.weight', 'module.model.up_blocks.0.resnets.1.conv1.bias', 'module.model.up_blocks.0.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.1.norm2.weight', 'module.model.up_blocks.0.resnets.1.norm2.bias', 'module.model.up_blocks.0.resnets.1.conv2.weight', 'module.model.up_blocks.0.resnets.1.conv2.bias', 'module.model.up_blocks.0.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.0.resnets.2.norm1.weight', 'module.model.up_blocks.0.resnets.2.norm1.bias', 'module.model.up_blocks.0.resnets.2.conv1.weight', 'module.model.up_blocks.0.resnets.2.conv1.bias', 'module.model.up_blocks.0.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.0.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.0.resnets.2.norm2.weight', 'module.model.up_blocks.0.resnets.2.norm2.bias', 'module.model.up_blocks.0.resnets.2.conv2.weight', 'module.model.up_blocks.0.resnets.2.conv2.bias', 'module.model.up_blocks.0.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.0.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.0.upsamplers.0.conv.weight', 'module.model.up_blocks.0.upsamplers.0.conv.bias', 'module.model.up_blocks.1.attentions.0.norm.weight', 'module.model.up_blocks.1.attentions.0.norm.bias', 'module.model.up_blocks.1.attentions.0.proj_in.weight', 'module.model.up_blocks.1.attentions.0.proj_in.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.0.proj_out.weight', 'module.model.up_blocks.1.attentions.0.proj_out.bias', 'module.model.up_blocks.1.attentions.1.norm.weight', 'module.model.up_blocks.1.attentions.1.norm.bias', 'module.model.up_blocks.1.attentions.1.proj_in.weight', 'module.model.up_blocks.1.attentions.1.proj_in.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.1.proj_out.weight', 'module.model.up_blocks.1.attentions.1.proj_out.bias', 'module.model.up_blocks.1.attentions.2.norm.weight', 'module.model.up_blocks.1.attentions.2.norm.bias', 'module.model.up_blocks.1.attentions.2.proj_in.weight', 'module.model.up_blocks.1.attentions.2.proj_in.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.attentions.2.proj_out.weight', 'module.model.up_blocks.1.attentions.2.proj_out.bias', 'module.model.up_blocks.1.resnets.0.norm1.weight', 'module.model.up_blocks.1.resnets.0.norm1.bias', 'module.model.up_blocks.1.resnets.0.conv1.weight', 'module.model.up_blocks.1.resnets.0.conv1.bias', 'module.model.up_blocks.1.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.0.norm2.weight', 'module.model.up_blocks.1.resnets.0.norm2.bias', 'module.model.up_blocks.1.resnets.0.conv2.weight', 'module.model.up_blocks.1.resnets.0.conv2.bias', 'module.model.up_blocks.1.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.1.resnets.1.norm1.weight', 'module.model.up_blocks.1.resnets.1.norm1.bias', 'module.model.up_blocks.1.resnets.1.conv1.weight', 'module.model.up_blocks.1.resnets.1.conv1.bias', 'module.model.up_blocks.1.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.1.norm2.weight', 'module.model.up_blocks.1.resnets.1.norm2.bias', 'module.model.up_blocks.1.resnets.1.conv2.weight', 'module.model.up_blocks.1.resnets.1.conv2.bias', 'module.model.up_blocks.1.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.1.resnets.2.norm1.weight', 'module.model.up_blocks.1.resnets.2.norm1.bias', 'module.model.up_blocks.1.resnets.2.conv1.weight', 'module.model.up_blocks.1.resnets.2.conv1.bias', 'module.model.up_blocks.1.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.1.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.1.resnets.2.norm2.weight', 'module.model.up_blocks.1.resnets.2.norm2.bias', 'module.model.up_blocks.1.resnets.2.conv2.weight', 'module.model.up_blocks.1.resnets.2.conv2.bias', 'module.model.up_blocks.1.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.1.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.1.warpflows.0.norm.weight', 'module.model.up_blocks.1.warpflows.0.norm.bias', 'module.model.up_blocks.1.warpflows.0.proj_in.weight', 'module.model.up_blocks.1.warpflows.0.proj_in.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.0.proj_out.weight', 'module.model.up_blocks.1.warpflows.0.proj_out.bias', 'module.model.up_blocks.1.warpflows.1.norm.weight', 'module.model.up_blocks.1.warpflows.1.norm.bias', 'module.model.up_blocks.1.warpflows.1.proj_in.weight', 'module.model.up_blocks.1.warpflows.1.proj_in.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.1.proj_out.weight', 'module.model.up_blocks.1.warpflows.1.proj_out.bias', 'module.model.up_blocks.1.warpflows.2.norm.weight', 'module.model.up_blocks.1.warpflows.2.norm.bias', 'module.model.up_blocks.1.warpflows.2.proj_in.weight', 'module.model.up_blocks.1.warpflows.2.proj_in.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.1.warpflows.2.proj_out.weight', 'module.model.up_blocks.1.warpflows.2.proj_out.bias', 'module.model.up_blocks.1.warpzc.0.weight', 'module.model.up_blocks.1.warpzc.0.bias', 'module.model.up_blocks.1.warpzc.1.weight', 'module.model.up_blocks.1.warpzc.1.bias', 'module.model.up_blocks.1.warpzc.2.weight', 'module.model.up_blocks.1.warpzc.2.bias', 'module.model.up_blocks.1.upsamplers.0.conv.weight', 'module.model.up_blocks.1.upsamplers.0.conv.bias', 'module.model.up_blocks.2.attentions.0.norm.weight', 'module.model.up_blocks.2.attentions.0.norm.bias', 'module.model.up_blocks.2.attentions.0.proj_in.weight', 'module.model.up_blocks.2.attentions.0.proj_in.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.0.proj_out.weight', 'module.model.up_blocks.2.attentions.0.proj_out.bias', 'module.model.up_blocks.2.attentions.1.norm.weight', 'module.model.up_blocks.2.attentions.1.norm.bias', 'module.model.up_blocks.2.attentions.1.proj_in.weight', 'module.model.up_blocks.2.attentions.1.proj_in.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.1.proj_out.weight', 'module.model.up_blocks.2.attentions.1.proj_out.bias', 'module.model.up_blocks.2.attentions.2.norm.weight', 'module.model.up_blocks.2.attentions.2.norm.bias', 'module.model.up_blocks.2.attentions.2.proj_in.weight', 'module.model.up_blocks.2.attentions.2.proj_in.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.attentions.2.proj_out.weight', 'module.model.up_blocks.2.attentions.2.proj_out.bias', 'module.model.up_blocks.2.resnets.0.norm1.weight', 'module.model.up_blocks.2.resnets.0.norm1.bias', 'module.model.up_blocks.2.resnets.0.conv1.weight', 'module.model.up_blocks.2.resnets.0.conv1.bias', 'module.model.up_blocks.2.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.0.norm2.weight', 'module.model.up_blocks.2.resnets.0.norm2.bias', 'module.model.up_blocks.2.resnets.0.conv2.weight', 'module.model.up_blocks.2.resnets.0.conv2.bias', 'module.model.up_blocks.2.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.2.resnets.1.norm1.weight', 'module.model.up_blocks.2.resnets.1.norm1.bias', 'module.model.up_blocks.2.resnets.1.conv1.weight', 'module.model.up_blocks.2.resnets.1.conv1.bias', 'module.model.up_blocks.2.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.1.norm2.weight', 'module.model.up_blocks.2.resnets.1.norm2.bias', 'module.model.up_blocks.2.resnets.1.conv2.weight', 'module.model.up_blocks.2.resnets.1.conv2.bias', 'module.model.up_blocks.2.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.2.resnets.2.norm1.weight', 'module.model.up_blocks.2.resnets.2.norm1.bias', 'module.model.up_blocks.2.resnets.2.conv1.weight', 'module.model.up_blocks.2.resnets.2.conv1.bias', 'module.model.up_blocks.2.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.2.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.2.resnets.2.norm2.weight', 'module.model.up_blocks.2.resnets.2.norm2.bias', 'module.model.up_blocks.2.resnets.2.conv2.weight', 'module.model.up_blocks.2.resnets.2.conv2.bias', 'module.model.up_blocks.2.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.2.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.2.warpflows.0.norm.weight', 'module.model.up_blocks.2.warpflows.0.norm.bias', 'module.model.up_blocks.2.warpflows.0.proj_in.weight', 'module.model.up_blocks.2.warpflows.0.proj_in.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.0.proj_out.weight', 'module.model.up_blocks.2.warpflows.0.proj_out.bias', 'module.model.up_blocks.2.warpflows.1.norm.weight', 'module.model.up_blocks.2.warpflows.1.norm.bias', 'module.model.up_blocks.2.warpflows.1.proj_in.weight', 'module.model.up_blocks.2.warpflows.1.proj_in.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.1.proj_out.weight', 'module.model.up_blocks.2.warpflows.1.proj_out.bias', 'module.model.up_blocks.2.warpflows.2.norm.weight', 'module.model.up_blocks.2.warpflows.2.norm.bias', 'module.model.up_blocks.2.warpflows.2.proj_in.weight', 'module.model.up_blocks.2.warpflows.2.proj_in.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.2.warpflows.2.proj_out.weight', 'module.model.up_blocks.2.warpflows.2.proj_out.bias', 'module.model.up_blocks.2.warpzc.0.weight', 'module.model.up_blocks.2.warpzc.0.bias', 'module.model.up_blocks.2.warpzc.1.weight', 'module.model.up_blocks.2.warpzc.1.bias', 'module.model.up_blocks.2.warpzc.2.weight', 'module.model.up_blocks.2.warpzc.2.bias', 'module.model.up_blocks.2.upsamplers.0.conv.weight', 'module.model.up_blocks.2.upsamplers.0.conv.bias', 'module.model.up_blocks.3.attentions.0.norm.weight', 'module.model.up_blocks.3.attentions.0.norm.bias', 'module.model.up_blocks.3.attentions.0.proj_in.weight', 'module.model.up_blocks.3.attentions.0.proj_in.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.0.proj_out.weight', 'module.model.up_blocks.3.attentions.0.proj_out.bias', 'module.model.up_blocks.3.attentions.1.norm.weight', 'module.model.up_blocks.3.attentions.1.norm.bias', 'module.model.up_blocks.3.attentions.1.proj_in.weight', 'module.model.up_blocks.3.attentions.1.proj_in.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.1.proj_out.weight', 'module.model.up_blocks.3.attentions.1.proj_out.bias', 'module.model.up_blocks.3.attentions.2.norm.weight', 'module.model.up_blocks.3.attentions.2.norm.bias', 'module.model.up_blocks.3.attentions.2.proj_in.weight', 'module.model.up_blocks.3.attentions.2.proj_in.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.attentions.2.proj_out.weight', 'module.model.up_blocks.3.attentions.2.proj_out.bias', 'module.model.up_blocks.3.resnets.0.norm1.weight', 'module.model.up_blocks.3.resnets.0.norm1.bias', 'module.model.up_blocks.3.resnets.0.conv1.weight', 'module.model.up_blocks.3.resnets.0.conv1.bias', 'module.model.up_blocks.3.resnets.0.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.0.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.0.norm2.weight', 'module.model.up_blocks.3.resnets.0.norm2.bias', 'module.model.up_blocks.3.resnets.0.conv2.weight', 'module.model.up_blocks.3.resnets.0.conv2.bias', 'module.model.up_blocks.3.resnets.0.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.0.conv_shortcut.bias', 'module.model.up_blocks.3.resnets.1.norm1.weight', 'module.model.up_blocks.3.resnets.1.norm1.bias', 'module.model.up_blocks.3.resnets.1.conv1.weight', 'module.model.up_blocks.3.resnets.1.conv1.bias', 'module.model.up_blocks.3.resnets.1.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.1.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.1.norm2.weight', 'module.model.up_blocks.3.resnets.1.norm2.bias', 'module.model.up_blocks.3.resnets.1.conv2.weight', 'module.model.up_blocks.3.resnets.1.conv2.bias', 'module.model.up_blocks.3.resnets.1.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.1.conv_shortcut.bias', 'module.model.up_blocks.3.resnets.2.norm1.weight', 'module.model.up_blocks.3.resnets.2.norm1.bias', 'module.model.up_blocks.3.resnets.2.conv1.weight', 'module.model.up_blocks.3.resnets.2.conv1.bias', 'module.model.up_blocks.3.resnets.2.time_emb_proj.weight', 'module.model.up_blocks.3.resnets.2.time_emb_proj.bias', 'module.model.up_blocks.3.resnets.2.norm2.weight', 'module.model.up_blocks.3.resnets.2.norm2.bias', 'module.model.up_blocks.3.resnets.2.conv2.weight', 'module.model.up_blocks.3.resnets.2.conv2.bias', 'module.model.up_blocks.3.resnets.2.conv_shortcut.weight', 'module.model.up_blocks.3.resnets.2.conv_shortcut.bias', 'module.model.up_blocks.3.warpflows.0.norm.weight', 'module.model.up_blocks.3.warpflows.0.norm.bias', 'module.model.up_blocks.3.warpflows.0.proj_in.weight', 'module.model.up_blocks.3.warpflows.0.proj_in.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.0.proj_out.weight', 'module.model.up_blocks.3.warpflows.0.proj_out.bias', 'module.model.up_blocks.3.warpflows.1.norm.weight', 'module.model.up_blocks.3.warpflows.1.norm.bias', 'module.model.up_blocks.3.warpflows.1.proj_in.weight', 'module.model.up_blocks.3.warpflows.1.proj_in.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.1.proj_out.weight', 'module.model.up_blocks.3.warpflows.1.proj_out.bias', 'module.model.up_blocks.3.warpflows.2.norm.weight', 'module.model.up_blocks.3.warpflows.2.norm.bias', 'module.model.up_blocks.3.warpflows.2.proj_in.weight', 'module.model.up_blocks.3.warpflows.2.proj_in.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'module.model.up_blocks.3.warpflows.2.proj_out.weight', 'module.model.up_blocks.3.warpflows.2.proj_out.bias', 'module.model.up_blocks.3.warpzc.0.weight', 'module.model.up_blocks.3.warpzc.0.bias', 'module.model.up_blocks.3.warpzc.1.weight', 'module.model.up_blocks.3.warpzc.1.bias', 'module.model.up_blocks.3.warpzc.2.weight', 'module.model.up_blocks.3.warpzc.2.bias', 'module.model.mid_block.attentions.0.norm.weight', 'module.model.mid_block.attentions.0.norm.bias', 'module.model.mid_block.attentions.0.proj_in.weight', 'module.model.mid_block.attentions.0.proj_in.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'module.model.mid_block.attentions.0.proj_out.weight', 'module.model.mid_block.attentions.0.proj_out.bias', 'module.model.mid_block.resnets.0.norm1.weight', 'module.model.mid_block.resnets.0.norm1.bias', 'module.model.mid_block.resnets.0.conv1.weight', 'module.model.mid_block.resnets.0.conv1.bias', 'module.model.mid_block.resnets.0.time_emb_proj.weight', 'module.model.mid_block.resnets.0.time_emb_proj.bias', 'module.model.mid_block.resnets.0.norm2.weight', 'module.model.mid_block.resnets.0.norm2.bias', 'module.model.mid_block.resnets.0.conv2.weight', 'module.model.mid_block.resnets.0.conv2.bias', 'module.model.mid_block.resnets.1.norm1.weight', 'module.model.mid_block.resnets.1.norm1.bias', 'module.model.mid_block.resnets.1.conv1.weight', 'module.model.mid_block.resnets.1.conv1.bias', 'module.model.mid_block.resnets.1.time_emb_proj.weight', 'module.model.mid_block.resnets.1.time_emb_proj.bias', 'module.model.mid_block.resnets.1.norm2.weight', 'module.model.mid_block.resnets.1.norm2.bias', 'module.model.mid_block.resnets.1.conv2.weight', 'module.model.mid_block.resnets.1.conv2.bias', 'module.model.conv_norm_out.weight', 'module.model.conv_norm_out.bias', 'module.model.conv_out.weight', 'module.model.conv_out.bias'], unexpected_keys=['model.conv_in.weight', 'model.conv_in.bias', 'model.time_embedding.linear_1.weight', 'model.time_embedding.linear_1.bias', 'model.time_embedding.linear_2.weight', 'model.time_embedding.linear_2.bias', 'model.down_blocks.0.attentions.0.norm.weight', 'model.down_blocks.0.attentions.0.norm.bias', 'model.down_blocks.0.attentions.0.proj_in.weight', 'model.down_blocks.0.attentions.0.proj_in.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.0.attentions.0.proj_out.weight', 'model.down_blocks.0.attentions.0.proj_out.bias', 'model.down_blocks.0.attentions.1.norm.weight', 'model.down_blocks.0.attentions.1.norm.bias', 'model.down_blocks.0.attentions.1.proj_in.weight', 'model.down_blocks.0.attentions.1.proj_in.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.0.attentions.1.proj_out.weight', 'model.down_blocks.0.attentions.1.proj_out.bias', 'model.down_blocks.0.resnets.0.norm1.weight', 'model.down_blocks.0.resnets.0.norm1.bias', 'model.down_blocks.0.resnets.0.conv1.weight', 'model.down_blocks.0.resnets.0.conv1.bias', 'model.down_blocks.0.resnets.0.time_emb_proj.weight', 'model.down_blocks.0.resnets.0.time_emb_proj.bias', 'model.down_blocks.0.resnets.0.norm2.weight', 'model.down_blocks.0.resnets.0.norm2.bias', 'model.down_blocks.0.resnets.0.conv2.weight', 'model.down_blocks.0.resnets.0.conv2.bias', 'model.down_blocks.0.resnets.1.norm1.weight', 'model.down_blocks.0.resnets.1.norm1.bias', 'model.down_blocks.0.resnets.1.conv1.weight', 'model.down_blocks.0.resnets.1.conv1.bias', 'model.down_blocks.0.resnets.1.time_emb_proj.weight', 'model.down_blocks.0.resnets.1.time_emb_proj.bias', 'model.down_blocks.0.resnets.1.norm2.weight', 'model.down_blocks.0.resnets.1.norm2.bias', 'model.down_blocks.0.resnets.1.conv2.weight', 'model.down_blocks.0.resnets.1.conv2.bias', 'model.down_blocks.0.downsamplers.0.conv.weight', 'model.down_blocks.0.downsamplers.0.conv.bias', 'model.down_blocks.1.attentions.0.norm.weight', 'model.down_blocks.1.attentions.0.norm.bias', 'model.down_blocks.1.attentions.0.proj_in.weight', 'model.down_blocks.1.attentions.0.proj_in.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.1.attentions.0.proj_out.weight', 'model.down_blocks.1.attentions.0.proj_out.bias', 'model.down_blocks.1.attentions.1.norm.weight', 'model.down_blocks.1.attentions.1.norm.bias', 'model.down_blocks.1.attentions.1.proj_in.weight', 'model.down_blocks.1.attentions.1.proj_in.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.1.attentions.1.proj_out.weight', 'model.down_blocks.1.attentions.1.proj_out.bias', 'model.down_blocks.1.resnets.0.norm1.weight', 'model.down_blocks.1.resnets.0.norm1.bias', 'model.down_blocks.1.resnets.0.conv1.weight', 'model.down_blocks.1.resnets.0.conv1.bias', 'model.down_blocks.1.resnets.0.time_emb_proj.weight', 'model.down_blocks.1.resnets.0.time_emb_proj.bias', 'model.down_blocks.1.resnets.0.norm2.weight', 'model.down_blocks.1.resnets.0.norm2.bias', 'model.down_blocks.1.resnets.0.conv2.weight', 'model.down_blocks.1.resnets.0.conv2.bias', 'model.down_blocks.1.resnets.0.conv_shortcut.weight', 'model.down_blocks.1.resnets.0.conv_shortcut.bias', 'model.down_blocks.1.resnets.1.norm1.weight', 'model.down_blocks.1.resnets.1.norm1.bias', 'model.down_blocks.1.resnets.1.conv1.weight', 'model.down_blocks.1.resnets.1.conv1.bias', 'model.down_blocks.1.resnets.1.time_emb_proj.weight', 'model.down_blocks.1.resnets.1.time_emb_proj.bias', 'model.down_blocks.1.resnets.1.norm2.weight', 'model.down_blocks.1.resnets.1.norm2.bias', 'model.down_blocks.1.resnets.1.conv2.weight', 'model.down_blocks.1.resnets.1.conv2.bias', 'model.down_blocks.1.downsamplers.0.conv.weight', 'model.down_blocks.1.downsamplers.0.conv.bias', 'model.down_blocks.2.attentions.0.norm.weight', 'model.down_blocks.2.attentions.0.norm.bias', 'model.down_blocks.2.attentions.0.proj_in.weight', 'model.down_blocks.2.attentions.0.proj_in.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.2.attentions.0.proj_out.weight', 'model.down_blocks.2.attentions.0.proj_out.bias', 'model.down_blocks.2.attentions.1.norm.weight', 'model.down_blocks.2.attentions.1.norm.bias', 'model.down_blocks.2.attentions.1.proj_in.weight', 'model.down_blocks.2.attentions.1.proj_in.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.down_blocks.2.attentions.1.proj_out.weight', 'model.down_blocks.2.attentions.1.proj_out.bias', 'model.down_blocks.2.resnets.0.norm1.weight', 'model.down_blocks.2.resnets.0.norm1.bias', 'model.down_blocks.2.resnets.0.conv1.weight', 'model.down_blocks.2.resnets.0.conv1.bias', 'model.down_blocks.2.resnets.0.time_emb_proj.weight', 'model.down_blocks.2.resnets.0.time_emb_proj.bias', 'model.down_blocks.2.resnets.0.norm2.weight', 'model.down_blocks.2.resnets.0.norm2.bias', 'model.down_blocks.2.resnets.0.conv2.weight', 'model.down_blocks.2.resnets.0.conv2.bias', 'model.down_blocks.2.resnets.0.conv_shortcut.weight', 'model.down_blocks.2.resnets.0.conv_shortcut.bias', 'model.down_blocks.2.resnets.1.norm1.weight', 'model.down_blocks.2.resnets.1.norm1.bias', 'model.down_blocks.2.resnets.1.conv1.weight', 'model.down_blocks.2.resnets.1.conv1.bias', 'model.down_blocks.2.resnets.1.time_emb_proj.weight', 'model.down_blocks.2.resnets.1.time_emb_proj.bias', 'model.down_blocks.2.resnets.1.norm2.weight', 'model.down_blocks.2.resnets.1.norm2.bias', 'model.down_blocks.2.resnets.1.conv2.weight', 'model.down_blocks.2.resnets.1.conv2.bias', 'model.down_blocks.2.downsamplers.0.conv.weight', 'model.down_blocks.2.downsamplers.0.conv.bias', 'model.down_blocks.3.resnets.0.norm1.weight', 'model.down_blocks.3.resnets.0.norm1.bias', 'model.down_blocks.3.resnets.0.conv1.weight', 'model.down_blocks.3.resnets.0.conv1.bias', 'model.down_blocks.3.resnets.0.time_emb_proj.weight', 'model.down_blocks.3.resnets.0.time_emb_proj.bias', 'model.down_blocks.3.resnets.0.norm2.weight', 'model.down_blocks.3.resnets.0.norm2.bias', 'model.down_blocks.3.resnets.0.conv2.weight', 'model.down_blocks.3.resnets.0.conv2.bias', 'model.down_blocks.3.resnets.1.norm1.weight', 'model.down_blocks.3.resnets.1.norm1.bias', 'model.down_blocks.3.resnets.1.conv1.weight', 'model.down_blocks.3.resnets.1.conv1.bias', 'model.down_blocks.3.resnets.1.time_emb_proj.weight', 'model.down_blocks.3.resnets.1.time_emb_proj.bias', 'model.down_blocks.3.resnets.1.norm2.weight', 'model.down_blocks.3.resnets.1.norm2.bias', 'model.down_blocks.3.resnets.1.conv2.weight', 'model.down_blocks.3.resnets.1.conv2.bias', 'model.up_blocks.0.resnets.0.norm1.weight', 'model.up_blocks.0.resnets.0.norm1.bias', 'model.up_blocks.0.resnets.0.conv1.weight', 'model.up_blocks.0.resnets.0.conv1.bias', 'model.up_blocks.0.resnets.0.time_emb_proj.weight', 'model.up_blocks.0.resnets.0.time_emb_proj.bias', 'model.up_blocks.0.resnets.0.norm2.weight', 'model.up_blocks.0.resnets.0.norm2.bias', 'model.up_blocks.0.resnets.0.conv2.weight', 'model.up_blocks.0.resnets.0.conv2.bias', 'model.up_blocks.0.resnets.0.conv_shortcut.weight', 'model.up_blocks.0.resnets.0.conv_shortcut.bias', 'model.up_blocks.0.resnets.1.norm1.weight', 'model.up_blocks.0.resnets.1.norm1.bias', 'model.up_blocks.0.resnets.1.conv1.weight', 'model.up_blocks.0.resnets.1.conv1.bias', 'model.up_blocks.0.resnets.1.time_emb_proj.weight', 'model.up_blocks.0.resnets.1.time_emb_proj.bias', 'model.up_blocks.0.resnets.1.norm2.weight', 'model.up_blocks.0.resnets.1.norm2.bias', 'model.up_blocks.0.resnets.1.conv2.weight', 'model.up_blocks.0.resnets.1.conv2.bias', 'model.up_blocks.0.resnets.1.conv_shortcut.weight', 'model.up_blocks.0.resnets.1.conv_shortcut.bias', 'model.up_blocks.0.resnets.2.norm1.weight', 'model.up_blocks.0.resnets.2.norm1.bias', 'model.up_blocks.0.resnets.2.conv1.weight', 'model.up_blocks.0.resnets.2.conv1.bias', 'model.up_blocks.0.resnets.2.time_emb_proj.weight', 'model.up_blocks.0.resnets.2.time_emb_proj.bias', 'model.up_blocks.0.resnets.2.norm2.weight', 'model.up_blocks.0.resnets.2.norm2.bias', 'model.up_blocks.0.resnets.2.conv2.weight', 'model.up_blocks.0.resnets.2.conv2.bias', 'model.up_blocks.0.resnets.2.conv_shortcut.weight', 'model.up_blocks.0.resnets.2.conv_shortcut.bias', 'model.up_blocks.0.upsamplers.0.conv.weight', 'model.up_blocks.0.upsamplers.0.conv.bias', 'model.up_blocks.1.attentions.0.norm.weight', 'model.up_blocks.1.attentions.0.norm.bias', 'model.up_blocks.1.attentions.0.proj_in.weight', 'model.up_blocks.1.attentions.0.proj_in.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.0.proj_out.weight', 'model.up_blocks.1.attentions.0.proj_out.bias', 'model.up_blocks.1.attentions.1.norm.weight', 'model.up_blocks.1.attentions.1.norm.bias', 'model.up_blocks.1.attentions.1.proj_in.weight', 'model.up_blocks.1.attentions.1.proj_in.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.1.proj_out.weight', 'model.up_blocks.1.attentions.1.proj_out.bias', 'model.up_blocks.1.attentions.2.norm.weight', 'model.up_blocks.1.attentions.2.norm.bias', 'model.up_blocks.1.attentions.2.proj_in.weight', 'model.up_blocks.1.attentions.2.proj_in.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.attentions.2.proj_out.weight', 'model.up_blocks.1.attentions.2.proj_out.bias', 'model.up_blocks.1.resnets.0.norm1.weight', 'model.up_blocks.1.resnets.0.norm1.bias', 'model.up_blocks.1.resnets.0.conv1.weight', 'model.up_blocks.1.resnets.0.conv1.bias', 'model.up_blocks.1.resnets.0.time_emb_proj.weight', 'model.up_blocks.1.resnets.0.time_emb_proj.bias', 'model.up_blocks.1.resnets.0.norm2.weight', 'model.up_blocks.1.resnets.0.norm2.bias', 'model.up_blocks.1.resnets.0.conv2.weight', 'model.up_blocks.1.resnets.0.conv2.bias', 'model.up_blocks.1.resnets.0.conv_shortcut.weight', 'model.up_blocks.1.resnets.0.conv_shortcut.bias', 'model.up_blocks.1.resnets.1.norm1.weight', 'model.up_blocks.1.resnets.1.norm1.bias', 'model.up_blocks.1.resnets.1.conv1.weight', 'model.up_blocks.1.resnets.1.conv1.bias', 'model.up_blocks.1.resnets.1.time_emb_proj.weight', 'model.up_blocks.1.resnets.1.time_emb_proj.bias', 'model.up_blocks.1.resnets.1.norm2.weight', 'model.up_blocks.1.resnets.1.norm2.bias', 'model.up_blocks.1.resnets.1.conv2.weight', 'model.up_blocks.1.resnets.1.conv2.bias', 'model.up_blocks.1.resnets.1.conv_shortcut.weight', 'model.up_blocks.1.resnets.1.conv_shortcut.bias', 'model.up_blocks.1.resnets.2.norm1.weight', 'model.up_blocks.1.resnets.2.norm1.bias', 'model.up_blocks.1.resnets.2.conv1.weight', 'model.up_blocks.1.resnets.2.conv1.bias', 'model.up_blocks.1.resnets.2.time_emb_proj.weight', 'model.up_blocks.1.resnets.2.time_emb_proj.bias', 'model.up_blocks.1.resnets.2.norm2.weight', 'model.up_blocks.1.resnets.2.norm2.bias', 'model.up_blocks.1.resnets.2.conv2.weight', 'model.up_blocks.1.resnets.2.conv2.bias', 'model.up_blocks.1.resnets.2.conv_shortcut.weight', 'model.up_blocks.1.resnets.2.conv_shortcut.bias', 'model.up_blocks.1.upsamplers.0.conv.weight', 'model.up_blocks.1.upsamplers.0.conv.bias', 'model.up_blocks.2.attentions.0.norm.weight', 'model.up_blocks.2.attentions.0.norm.bias', 'model.up_blocks.2.attentions.0.proj_in.weight', 'model.up_blocks.2.attentions.0.proj_in.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.0.proj_out.weight', 'model.up_blocks.2.attentions.0.proj_out.bias', 'model.up_blocks.2.attentions.1.norm.weight', 'model.up_blocks.2.attentions.1.norm.bias', 'model.up_blocks.2.attentions.1.proj_in.weight', 'model.up_blocks.2.attentions.1.proj_in.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.1.proj_out.weight', 'model.up_blocks.2.attentions.1.proj_out.bias', 'model.up_blocks.2.attentions.2.norm.weight', 'model.up_blocks.2.attentions.2.norm.bias', 'model.up_blocks.2.attentions.2.proj_in.weight', 'model.up_blocks.2.attentions.2.proj_in.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.attentions.2.proj_out.weight', 'model.up_blocks.2.attentions.2.proj_out.bias', 'model.up_blocks.2.resnets.0.norm1.weight', 'model.up_blocks.2.resnets.0.norm1.bias', 'model.up_blocks.2.resnets.0.conv1.weight', 'model.up_blocks.2.resnets.0.conv1.bias', 'model.up_blocks.2.resnets.0.time_emb_proj.weight', 'model.up_blocks.2.resnets.0.time_emb_proj.bias', 'model.up_blocks.2.resnets.0.norm2.weight', 'model.up_blocks.2.resnets.0.norm2.bias', 'model.up_blocks.2.resnets.0.conv2.weight', 'model.up_blocks.2.resnets.0.conv2.bias', 'model.up_blocks.2.resnets.0.conv_shortcut.weight', 'model.up_blocks.2.resnets.0.conv_shortcut.bias', 'model.up_blocks.2.resnets.1.norm1.weight', 'model.up_blocks.2.resnets.1.norm1.bias', 'model.up_blocks.2.resnets.1.conv1.weight', 'model.up_blocks.2.resnets.1.conv1.bias', 'model.up_blocks.2.resnets.1.time_emb_proj.weight', 'model.up_blocks.2.resnets.1.time_emb_proj.bias', 'model.up_blocks.2.resnets.1.norm2.weight', 'model.up_blocks.2.resnets.1.norm2.bias', 'model.up_blocks.2.resnets.1.conv2.weight', 'model.up_blocks.2.resnets.1.conv2.bias', 'model.up_blocks.2.resnets.1.conv_shortcut.weight', 'model.up_blocks.2.resnets.1.conv_shortcut.bias', 'model.up_blocks.2.resnets.2.norm1.weight', 'model.up_blocks.2.resnets.2.norm1.bias', 'model.up_blocks.2.resnets.2.conv1.weight', 'model.up_blocks.2.resnets.2.conv1.bias', 'model.up_blocks.2.resnets.2.time_emb_proj.weight', 'model.up_blocks.2.resnets.2.time_emb_proj.bias', 'model.up_blocks.2.resnets.2.norm2.weight', 'model.up_blocks.2.resnets.2.norm2.bias', 'model.up_blocks.2.resnets.2.conv2.weight', 'model.up_blocks.2.resnets.2.conv2.bias', 'model.up_blocks.2.resnets.2.conv_shortcut.weight', 'model.up_blocks.2.resnets.2.conv_shortcut.bias', 'model.up_blocks.2.upsamplers.0.conv.weight', 'model.up_blocks.2.upsamplers.0.conv.bias', 'model.up_blocks.3.attentions.0.norm.weight', 'model.up_blocks.3.attentions.0.norm.bias', 'model.up_blocks.3.attentions.0.proj_in.weight', 'model.up_blocks.3.attentions.0.proj_in.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.0.proj_out.weight', 'model.up_blocks.3.attentions.0.proj_out.bias', 'model.up_blocks.3.attentions.1.norm.weight', 'model.up_blocks.3.attentions.1.norm.bias', 'model.up_blocks.3.attentions.1.proj_in.weight', 'model.up_blocks.3.attentions.1.proj_in.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.1.proj_out.weight', 'model.up_blocks.3.attentions.1.proj_out.bias', 'model.up_blocks.3.attentions.2.norm.weight', 'model.up_blocks.3.attentions.2.norm.bias', 'model.up_blocks.3.attentions.2.proj_in.weight', 'model.up_blocks.3.attentions.2.proj_in.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.attentions.2.proj_out.weight', 'model.up_blocks.3.attentions.2.proj_out.bias', 'model.up_blocks.3.resnets.0.norm1.weight', 'model.up_blocks.3.resnets.0.norm1.bias', 'model.up_blocks.3.resnets.0.conv1.weight', 'model.up_blocks.3.resnets.0.conv1.bias', 'model.up_blocks.3.resnets.0.time_emb_proj.weight', 'model.up_blocks.3.resnets.0.time_emb_proj.bias', 'model.up_blocks.3.resnets.0.norm2.weight', 'model.up_blocks.3.resnets.0.norm2.bias', 'model.up_blocks.3.resnets.0.conv2.weight', 'model.up_blocks.3.resnets.0.conv2.bias', 'model.up_blocks.3.resnets.0.conv_shortcut.weight', 'model.up_blocks.3.resnets.0.conv_shortcut.bias', 'model.up_blocks.3.resnets.1.norm1.weight', 'model.up_blocks.3.resnets.1.norm1.bias', 'model.up_blocks.3.resnets.1.conv1.weight', 'model.up_blocks.3.resnets.1.conv1.bias', 'model.up_blocks.3.resnets.1.time_emb_proj.weight', 'model.up_blocks.3.resnets.1.time_emb_proj.bias', 'model.up_blocks.3.resnets.1.norm2.weight', 'model.up_blocks.3.resnets.1.norm2.bias', 'model.up_blocks.3.resnets.1.conv2.weight', 'model.up_blocks.3.resnets.1.conv2.bias', 'model.up_blocks.3.resnets.1.conv_shortcut.weight', 'model.up_blocks.3.resnets.1.conv_shortcut.bias', 'model.up_blocks.3.resnets.2.norm1.weight', 'model.up_blocks.3.resnets.2.norm1.bias', 'model.up_blocks.3.resnets.2.conv1.weight', 'model.up_blocks.3.resnets.2.conv1.bias', 'model.up_blocks.3.resnets.2.time_emb_proj.weight', 'model.up_blocks.3.resnets.2.time_emb_proj.bias', 'model.up_blocks.3.resnets.2.norm2.weight', 'model.up_blocks.3.resnets.2.norm2.bias', 'model.up_blocks.3.resnets.2.conv2.weight', 'model.up_blocks.3.resnets.2.conv2.bias', 'model.up_blocks.3.resnets.2.conv_shortcut.weight', 'model.up_blocks.3.resnets.2.conv_shortcut.bias', 'model.mid_block.attentions.0.norm.weight', 'model.mid_block.attentions.0.norm.bias', 'model.mid_block.attentions.0.proj_in.weight', 'model.mid_block.attentions.0.proj_in.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm1.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm1.bias', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm2.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm2.bias', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.mid_block.attentions.0.transformer_blocks.0.norm3.weight', 'model.mid_block.attentions.0.transformer_blocks.0.norm3.bias', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight', 'model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias', 'model.mid_block.attentions.0.proj_out.weight', 'model.mid_block.attentions.0.proj_out.bias', 'model.mid_block.resnets.0.norm1.weight', 'model.mid_block.resnets.0.norm1.bias', 'model.mid_block.resnets.0.conv1.weight', 'model.mid_block.resnets.0.conv1.bias', 'model.mid_block.resnets.0.time_emb_proj.weight', 'model.mid_block.resnets.0.time_emb_proj.bias', 'model.mid_block.resnets.0.norm2.weight', 'model.mid_block.resnets.0.norm2.bias', 'model.mid_block.resnets.0.conv2.weight', 'model.mid_block.resnets.0.conv2.bias', 'model.mid_block.resnets.1.norm1.weight', 'model.mid_block.resnets.1.norm1.bias', 'model.mid_block.resnets.1.conv1.weight', 'model.mid_block.resnets.1.conv1.bias', 'model.mid_block.resnets.1.time_emb_proj.weight', 'model.mid_block.resnets.1.time_emb_proj.bias', 'model.mid_block.resnets.1.norm2.weight', 'model.mid_block.resnets.1.norm2.bias', 'model.mid_block.resnets.1.conv2.weight', 'model.mid_block.resnets.1.conv2.bias', 'model.conv_norm_out.weight', 'model.conv_norm_out.bias', 'model.conv_out.weight', 'model.conv_out.bias'])
[2024-03-20 07:34:55 pose_transfer_train.py:396] preparing lr scheduler...
[2024-03-20 07:34:55 pose_transfer_train.py:402] start training...
[2024-03-20 07:34:55 pose_transfer_train.py:411] epoch 1 start
[2024-03-20 07:42:32 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:42:32 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:42:41 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:42:45 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:42:59 pose_transfer_train.py:343] number of trainable parameters: 213329208
[2024-03-20 07:42:59 pose_transfer_train.py:362] preparing optimizer...
[2024-03-20 07:42:59 pose_transfer_train.py:368] preparing accelerator...
[2024-03-20 07:43:01 pose_transfer_train.py:375] loading states from ./checkpoints/
[2024-03-20 07:43:02 pose_transfer_train.py:385] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 07:43:05 pose_transfer_train.py:388] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 07:43:05 pose_transfer_train.py:394] preparing lr scheduler...
[2024-03-20 07:43:05 pose_transfer_train.py:400] start training...
[2024-03-20 07:43:05 pose_transfer_train.py:409] epoch 1 start
[2024-03-20 07:48:20 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:48:20 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:48:30 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:48:33 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:48:48 pose_transfer_train.py:343] number of trainable parameters: 213329208
[2024-03-20 07:48:48 pose_transfer_train.py:362] preparing optimizer...
[2024-03-20 07:48:48 pose_transfer_train.py:368] preparing accelerator...
[2024-03-20 07:48:49 pose_transfer_train.py:375] loading states from ./checkpoints/
[2024-03-20 07:48:50 pose_transfer_train.py:385] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 07:54:36 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 07:54:36 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 07:54:46 pose_transfer_train.py:298] preparing model...
[2024-03-20 07:54:49 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 07:55:05 pose_transfer_train.py:343] number of trainable parameters: 213329208
[2024-03-20 07:55:05 pose_transfer_train.py:358] preparing optimizer...
[2024-03-20 07:55:05 pose_transfer_train.py:364] preparing accelerator...
[2024-03-20 07:55:05 pose_transfer_train.py:371] loading states from ./checkpoints/
[2024-03-20 07:55:06 pose_transfer_train.py:381] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 07:55:09 pose_transfer_train.py:384] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 07:55:09 pose_transfer_train.py:390] preparing lr scheduler...
[2024-03-20 07:55:09 pose_transfer_train.py:396] start training...
[2024-03-20 07:55:09 pose_transfer_train.py:405] epoch 1 start
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.conv_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.conv_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.time_embedding.linear_1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.time_embedding.linear_1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.time_embedding.linear_2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.time_embedding.linear_2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.downsamplers.0.conv.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.downsamplers.0.conv.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.downsamplers.0.conv.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.downsamplers.0.conv.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.downsamplers.0.conv.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.downsamplers.0.conv.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.upsamplers.0.conv.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.upsamplers.0.conv.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.upsamplers.0.conv.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.upsamplers.0.conv.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.upsamplers.0.conv.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.upsamplers.0.conv.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv_shortcut.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv_shortcut.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.norm.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.norm.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.proj_in.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.proj_in.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.proj_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.proj_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.norm1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.norm1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.conv1.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.conv1.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.time_emb_proj.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.time_emb_proj.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.norm2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.norm2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.conv2.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.conv2.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.conv_norm_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.conv_norm_out.bias
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.conv_out.weight
[2024-03-20 07:55:18 pose_transfer_train.py:490] param.grad is None: module.model.conv_out.bias
[2024-03-20 08:04:18 pose_transfer_train.py:273] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:04:18 pose_transfer_train.py:275] preparing datasets...
[2024-03-20 08:04:28 pose_transfer_train.py:298] preparing model...
[2024-03-20 08:04:31 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:04:44 pose_transfer_train.py:343] number of trainable parameters: 213329208
[2024-03-20 08:04:44 pose_transfer_train.py:358] preparing optimizer...
[2024-03-20 08:04:44 pose_transfer_train.py:364] preparing accelerator...
[2024-03-20 08:04:45 pose_transfer_train.py:371] loading states from ./checkpoints/
[2024-03-20 08:04:45 pose_transfer_train.py:381] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 08:04:48 pose_transfer_train.py:384] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 08:04:48 pose_transfer_train.py:390] preparing lr scheduler...
[2024-03-20 08:04:48 pose_transfer_train.py:396] start training...
[2024-03-20 08:04:48 pose_transfer_train.py:405] epoch 1 start
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.conv_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.conv_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.time_embedding.linear_1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.time_embedding.linear_1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.time_embedding.linear_2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.time_embedding.linear_2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.attentions.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.downsamplers.0.conv.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.0.downsamplers.0.conv.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.attentions.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.0.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.downsamplers.0.conv.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.1.downsamplers.0.conv.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.attentions.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.0.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.downsamplers.0.conv.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.2.downsamplers.0.conv.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.down_blocks.3.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.0.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.1.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.resnets.2.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.upsamplers.0.conv.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.0.upsamplers.0.conv.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.attentions.2.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.0.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.1.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.resnets.2.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.warpzc.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.upsamplers.0.conv.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.1.upsamplers.0.conv.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.attentions.2.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.0.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.1.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.resnets.2.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.warpzc.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.upsamplers.0.conv.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.2.upsamplers.0.conv.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.attentions.2.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.0.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.1.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv_shortcut.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.resnets.2.conv_shortcut.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.up_blocks.3.warpzc.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.norm.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.norm.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.proj_in.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.proj_in.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.proj_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.attentions.0.proj_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.0.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.norm1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.norm1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.conv1.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.conv1.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.time_emb_proj.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.time_emb_proj.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.norm2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.norm2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.conv2.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.mid_block.resnets.1.conv2.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.conv_norm_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.conv_norm_out.bias
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.conv_out.weight
[2024-03-20 08:04:58 pose_transfer_train.py:490] param.grad is None: module.model.conv_out.bias
[2024-03-20 08:20:56 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:20:56 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 08:21:05 pose_transfer_train.py:300] preparing model...
[2024-03-20 08:21:09 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:21:23 pose_transfer_train.py:345] number of trainable parameters: 213329208
[2024-03-20 08:21:23 pose_transfer_train.py:360] preparing optimizer...
[2024-03-20 08:21:23 pose_transfer_train.py:366] preparing accelerator...
[2024-03-20 08:21:23 pose_transfer_train.py:373] loading states from ./checkpoints/
[2024-03-20 08:21:24 pose_transfer_train.py:383] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 08:21:28 pose_transfer_train.py:386] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 08:21:28 pose_transfer_train.py:392] preparing lr scheduler...
[2024-03-20 08:21:28 pose_transfer_train.py:398] start training...
[2024-03-20 08:21:28 pose_transfer_train.py:407] epoch 1 start
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.conv_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.conv_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.time_embedding.linear_1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.time_embedding.linear_1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.time_embedding.linear_2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.time_embedding.linear_2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.attentions.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.downsamplers.0.conv.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.0.downsamplers.0.conv.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.attentions.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.0.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.downsamplers.0.conv.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.1.downsamplers.0.conv.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.attentions.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.0.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.downsamplers.0.conv.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.2.downsamplers.0.conv.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.down_blocks.3.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.0.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.1.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.resnets.2.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.upsamplers.0.conv.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.0.upsamplers.0.conv.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.attentions.2.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.0.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.1.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.resnets.2.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpflows.2.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpzc.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpzc.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpzc.1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpzc.1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpzc.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.warpzc.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.upsamplers.0.conv.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.1.upsamplers.0.conv.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.attentions.2.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.0.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.1.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.resnets.2.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpflows.2.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpzc.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpzc.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpzc.1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpzc.1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpzc.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.warpzc.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.upsamplers.0.conv.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.2.upsamplers.0.conv.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.attentions.2.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.0.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.1.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.conv_shortcut.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.resnets.2.conv_shortcut.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.1.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpflows.2.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpzc.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpzc.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpzc.1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpzc.1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpzc.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.up_blocks.3.warpzc.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.norm.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.norm.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.proj_in.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.proj_in.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm3.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.norm3.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.proj_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.attentions.0.proj_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.0.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.norm1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.norm1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.conv1.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.conv1.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.time_emb_proj.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.time_emb_proj.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.norm2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.norm2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.conv2.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.mid_block.resnets.1.conv2.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.conv_norm_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.conv_norm_out.bias
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.conv_out.weight
[2024-03-20 08:21:37 pose_transfer_train.py:486] param.grad is None: module.model.conv_out.bias
[2024-03-20 08:26:48 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:26:48 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 08:26:59 pose_transfer_train.py:300] preparing model...
[2024-03-20 08:27:02 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:27:18 pose_transfer_train.py:345] number of trainable parameters: 213329208
[2024-03-20 08:27:18 pose_transfer_train.py:360] preparing optimizer...
[2024-03-20 08:27:18 pose_transfer_train.py:366] preparing accelerator...
[2024-03-20 08:27:18 pose_transfer_train.py:373] loading states from ./checkpoints/
[2024-03-20 08:27:19 pose_transfer_train.py:383] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 08:27:22 pose_transfer_train.py:386] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 08:27:22 pose_transfer_train.py:392] preparing lr scheduler...
[2024-03-20 08:27:22 pose_transfer_train.py:398] start training...
[2024-03-20 08:27:22 pose_transfer_train.py:407] epoch 1 start
[2024-03-20 08:28:20 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:28:20 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 08:28:29 pose_transfer_train.py:300] preparing model...
[2024-03-20 08:28:34 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:28:47 pose_transfer_train.py:345] number of trainable parameters: 213329208
[2024-03-20 08:28:47 pose_transfer_train.py:360] preparing optimizer...
[2024-03-20 08:28:47 pose_transfer_train.py:366] preparing accelerator...
[2024-03-20 08:28:50 pose_transfer_train.py:373] loading states from ./checkpoints/
[2024-03-20 08:28:51 pose_transfer_train.py:383] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 08:28:54 pose_transfer_train.py:386] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 08:28:54 pose_transfer_train.py:392] preparing lr scheduler...
[2024-03-20 08:28:54 pose_transfer_train.py:398] start training...
[2024-03-20 08:28:54 pose_transfer_train.py:407] epoch 1 start
[2024-03-20 08:38:23 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 8
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: 
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:38:23 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 08:38:32 pose_transfer_train.py:300] preparing model...
[2024-03-20 08:39:01 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:39:24 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 08:39:24 pose_transfer_train.py:359] preparing optimizer...
[2024-03-20 08:39:24 pose_transfer_train.py:365] preparing accelerator...
[2024-03-20 08:39:26 pose_transfer_train.py:391] preparing lr scheduler...
[2024-03-20 08:39:26 pose_transfer_train.py:397] start training...
[2024-03-20 08:39:26 pose_transfer_train.py:406] epoch 1 start
[2024-03-20 08:40:09 pose_transfer_train.py:512] Train [1/100](10/12745)  Time 3.5369(4.2273)  Loss 0.0284(0.0437)  Lr 0.00001099  Eta 14:57:14
[2024-03-20 08:40:44 pose_transfer_train.py:512] Train [1/100](20/12745)  Time 3.4952(3.9079)  Loss 0.0493(0.0455)  Lr 0.00001189  Eta 13:48:47
[2024-03-20 08:41:21 pose_transfer_train.py:512] Train [1/100](30/12745)  Time 3.7576(3.8139)  Loss 0.0594(0.0467)  Lr 0.00001279  Eta 13:28:13
[2024-03-20 08:41:57 pose_transfer_train.py:512] Train [1/100](40/12745)  Time 3.5210(3.7646)  Loss 0.0150(0.0447)  Lr 0.00001369  Eta 13:17:09
[2024-03-20 08:42:33 pose_transfer_train.py:512] Train [1/100](50/12745)  Time 3.6253(3.7278)  Loss 0.0246(0.0434)  Lr 0.00001459  Eta 13:08:44
[2024-03-20 08:43:16 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 8
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: 
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:43:16 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 08:43:25 pose_transfer_train.py:300] preparing model...
[2024-03-20 08:43:53 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:44:16 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 08:44:16 pose_transfer_train.py:359] preparing optimizer...
[2024-03-20 08:44:16 pose_transfer_train.py:365] preparing accelerator...
[2024-03-20 08:44:19 pose_transfer_train.py:391] preparing lr scheduler...
[2024-03-20 08:44:19 pose_transfer_train.py:397] start training...
[2024-03-20 08:44:19 pose_transfer_train.py:406] epoch 1 start
[2024-03-20 08:44:29 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:44:29 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 08:44:38 pose_transfer_train.py:300] preparing model...
[2024-03-20 08:44:42 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:44:55 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 08:44:55 pose_transfer_train.py:359] preparing optimizer...
[2024-03-20 08:44:55 pose_transfer_train.py:365] preparing accelerator...
[2024-03-20 08:45:00 pose_transfer_train.py:372] loading states from ./checkpoints/
[2024-03-20 08:45:00 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 08:45:03 pose_transfer_train.py:385] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 08:45:03 pose_transfer_train.py:391] preparing lr scheduler...
[2024-03-20 08:45:03 pose_transfer_train.py:397] start training...
[2024-03-20 08:45:03 pose_transfer_train.py:406] epoch 1 start
[2024-03-20 08:47:44 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: 
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:47:44 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 08:47:53 pose_transfer_train.py:300] preparing model...
[2024-03-20 08:48:22 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:48:44 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 08:48:44 pose_transfer_train.py:359] preparing optimizer...
[2024-03-20 08:48:44 pose_transfer_train.py:365] preparing accelerator...
[2024-03-20 08:48:48 pose_transfer_train.py:391] preparing lr scheduler...
[2024-03-20 08:48:48 pose_transfer_train.py:397] start training...
[2024-03-20 08:48:48 pose_transfer_train.py:406] epoch 1 start
[2024-03-20 08:49:15 pose_transfer_train.py:512] Train [1/100](10/25491)  Time 2.3233(2.7696)  Loss 0.0133(0.0381)  Lr 0.00001099  Eta 19:36:11
[2024-03-20 08:50:01 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 2
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: 
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 08:50:01 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 08:50:10 pose_transfer_train.py:300] preparing model...
[2024-03-20 08:50:37 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 08:51:00 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 08:51:00 pose_transfer_train.py:359] preparing optimizer...
[2024-03-20 08:51:00 pose_transfer_train.py:365] preparing accelerator...
[2024-03-20 08:51:03 pose_transfer_train.py:391] preparing lr scheduler...
[2024-03-20 08:51:03 pose_transfer_train.py:397] start training...
[2024-03-20 08:51:03 pose_transfer_train.py:406] epoch 1 start
[2024-03-20 08:51:22 pose_transfer_train.py:512] Train [1/100](10/50983)  Time 1.5958(1.8816)  Loss 0.0070(0.0424)  Lr 0.00001099  Eta 1 day, 2:38:30
[2024-03-20 08:51:38 pose_transfer_train.py:512] Train [1/100](20/50983)  Time 1.8313(1.7166)  Loss 0.1189(0.0518)  Lr 0.00001189  Eta 1 day, 0:18:01
[2024-03-20 09:05:10 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 09:05:10 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 09:05:20 pose_transfer_train.py:300] preparing model...
[2024-03-20 09:05:24 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 09:05:35 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 09:05:36 pose_transfer_train.py:359] preparing optimizer...
[2024-03-20 09:05:36 pose_transfer_train.py:365] preparing accelerator...
[2024-03-20 09:05:39 pose_transfer_train.py:372] loading states from ./checkpoints/
[2024-03-20 09:05:39 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 09:05:42 pose_transfer_train.py:385] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 09:05:42 pose_transfer_train.py:391] preparing lr scheduler...
[2024-03-20 09:05:42 pose_transfer_train.py:397] start training...
[2024-03-20 09:05:42 pose_transfer_train.py:406] epoch 1 start
[2024-03-20 09:20:26 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 09:20:26 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 09:20:35 pose_transfer_train.py:300] preparing model...
[2024-03-20 09:20:39 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 09:20:52 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 09:20:52 pose_transfer_train.py:359] preparing optimizer...
[2024-03-20 09:20:52 pose_transfer_train.py:365] preparing accelerator...
[2024-03-20 09:20:56 pose_transfer_train.py:372] loading states from ./checkpoints/
[2024-03-20 09:20:58 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 09:21:01 pose_transfer_train.py:385] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 09:21:01 pose_transfer_train.py:391] preparing lr scheduler...
[2024-03-20 09:21:01 pose_transfer_train.py:397] start training...
[2024-03-20 09:21:01 pose_transfer_train.py:406] epoch 1 start
[2024-03-20 09:57:11 pose_transfer_train.py:275] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 09:57:11 pose_transfer_train.py:277] preparing datasets...
[2024-03-20 09:57:23 pose_transfer_train.py:300] preparing model...
[2024-03-20 09:57:27 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 09:57:39 pose_transfer_train.py:344] number of trainable parameters: 213329208
[2024-03-20 09:57:39 pose_transfer_train.py:359] preparing optimizer...
[2024-03-20 09:57:39 pose_transfer_train.py:365] preparing accelerator...
[2024-03-20 09:57:42 pose_transfer_train.py:372] loading states from ./checkpoints/
[2024-03-20 09:57:42 pose_transfer_train.py:382] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 09:57:45 pose_transfer_train.py:385] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 09:57:45 pose_transfer_train.py:391] preparing lr scheduler...
[2024-03-20 09:57:45 pose_transfer_train.py:397] start training...
[2024-03-20 09:57:45 pose_transfer_train.py:406] epoch 1 start
[2024-03-20 10:11:20 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 10:11:20 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 10:11:30 pose_transfer_train.py:306] preparing model...
[2024-03-20 10:11:33 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 10:11:45 pose_transfer_train.py:352] number of trainable parameters: 213329208
[2024-03-20 10:11:45 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 10:11:45 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 10:11:48 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 10:11:49 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 10:11:52 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 10:11:52 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 10:11:52 pose_transfer_train.py:405] start training...
[2024-03-20 10:11:52 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 10:12:32 pose_transfer_train.py:524] Train [1/100](10/6797)  Time 3.0432(3.9691)  Loss 0.0105(0.0292)  Lr 0.00001099  Eta 7:28:58
[2024-03-20 10:13:33 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 8
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 10:13:33 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 10:13:43 pose_transfer_train.py:306] preparing model...
[2024-03-20 10:13:47 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 10:13:59 pose_transfer_train.py:352] number of trainable parameters: 213329208
[2024-03-20 10:13:59 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 10:13:59 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 10:14:04 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 10:14:04 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 10:14:07 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 10:14:07 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 10:14:07 pose_transfer_train.py:405] start training...
[2024-03-20 10:14:07 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 10:14:33 pose_transfer_train.py:524] Train [1/100](10/16994)  Time 1.8389(2.6204)  Loss 0.0056(0.0351)  Lr 0.00001099  Eta 12:21:44
[2024-03-20 10:14:53 pose_transfer_train.py:524] Train [1/100](20/16994)  Time 1.8595(2.2790)  Loss 0.1203(0.0484)  Lr 0.00001189  Eta 10:44:43
[2024-03-20 10:15:44 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 8
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 10:15:44 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 10:16:28 pose_transfer_train.py:280] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 8
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 10:16:28 pose_transfer_train.py:282] preparing datasets...
[2024-03-20 10:16:37 pose_transfer_train.py:305] preparing model...
[2024-03-20 10:16:41 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 10:16:54 pose_transfer_train.py:351] number of trainable parameters: 213329208
[2024-03-20 10:16:54 pose_transfer_train.py:366] preparing optimizer...
[2024-03-20 10:16:54 pose_transfer_train.py:372] preparing accelerator...
[2024-03-20 10:16:59 pose_transfer_train.py:379] loading states from ./checkpoints/
[2024-03-20 10:17:00 pose_transfer_train.py:389] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 10:17:03 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 10:17:03 pose_transfer_train.py:398] preparing lr scheduler...
[2024-03-20 10:17:03 pose_transfer_train.py:404] start training...
[2024-03-20 10:17:03 pose_transfer_train.py:413] epoch 1 start
[2024-03-20 10:17:30 pose_transfer_train.py:523] Train [1/100](10/16994)  Time 2.3021(2.6555)  Loss 0.0056(0.0351)  Lr 0.00001099  Eta 12:31:40
[2024-03-20 10:17:53 pose_transfer_train.py:523] Train [1/100](20/16994)  Time 2.0017(2.4682)  Loss 0.1203(0.0484)  Lr 0.00001189  Eta 11:38:15
[2024-03-20 10:19:50 pose_transfer_train.py:280] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 10:19:50 pose_transfer_train.py:282] preparing datasets...
[2024-03-20 10:19:59 pose_transfer_train.py:305] preparing model...
[2024-03-20 10:20:03 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 10:20:15 pose_transfer_train.py:351] number of trainable parameters: 213329208
[2024-03-20 10:20:15 pose_transfer_train.py:366] preparing optimizer...
[2024-03-20 10:20:15 pose_transfer_train.py:372] preparing accelerator...
[2024-03-20 10:20:20 pose_transfer_train.py:379] loading states from ./checkpoints/
[2024-03-20 10:20:20 pose_transfer_train.py:389] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 10:20:23 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 10:20:23 pose_transfer_train.py:398] preparing lr scheduler...
[2024-03-20 10:20:23 pose_transfer_train.py:404] start training...
[2024-03-20 10:20:23 pose_transfer_train.py:413] epoch 1 start
[2024-03-20 10:20:46 pose_transfer_train.py:523] Train [1/100](10/25491)  Time 1.8132(2.2831)  Loss 0.0058(0.0388)  Lr 0.00001099  Eta 16:09:36
[2024-03-20 10:21:05 pose_transfer_train.py:523] Train [1/100](20/25491)  Time 2.3953(2.1079)  Loss 0.1106(0.0519)  Lr 0.00001189  Eta 14:54:49
[2024-03-20 10:25:27 pose_transfer_train.py:280] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 2
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 10:25:27 pose_transfer_train.py:282] preparing datasets...
[2024-03-20 10:25:36 pose_transfer_train.py:305] preparing model...
[2024-03-20 10:25:39 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 10:25:51 pose_transfer_train.py:351] number of trainable parameters: 213329208
[2024-03-20 10:25:51 pose_transfer_train.py:366] preparing optimizer...
[2024-03-20 10:25:51 pose_transfer_train.py:372] preparing accelerator...
[2024-03-20 10:25:56 pose_transfer_train.py:379] loading states from ./checkpoints/
[2024-03-20 10:25:56 pose_transfer_train.py:389] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 10:26:00 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 10:26:00 pose_transfer_train.py:398] preparing lr scheduler...
[2024-03-20 10:26:00 pose_transfer_train.py:404] start training...
[2024-03-20 10:26:00 pose_transfer_train.py:413] epoch 1 start
[2024-03-20 10:26:20 pose_transfer_train.py:523] Train [1/100](10/50983)  Time 1.6919(2.0380)  Loss 0.0024(0.0366)  Lr 0.00001099  Eta 1 day, 4:51:24
[2024-03-20 10:26:38 pose_transfer_train.py:523] Train [1/100](20/50983)  Time 2.0249(1.9051)  Loss 0.2217(0.0598)  Lr 0.00001189  Eta 1 day, 2:58:10
[2024-03-20 10:26:55 pose_transfer_train.py:523] Train [1/100](30/50983)  Time 2.0005(1.8384)  Loss 0.0034(0.0594)  Lr 0.00001279  Eta 1 day, 2:01:13
[2024-03-20 10:27:11 pose_transfer_train.py:523] Train [1/100](40/50983)  Time 1.5011(1.7878)  Loss 0.0017(0.0590)  Lr 0.00001369  Eta 1 day, 1:17:54
[2024-03-20 12:34:01 pose_transfer_train.py:280] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 12:34:01 pose_transfer_train.py:282] preparing datasets...
[2024-03-20 12:34:19 pose_transfer_train.py:305] preparing model...
[2024-03-20 12:34:28 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 12:34:53 pose_transfer_train.py:351] number of trainable parameters: 213329208
[2024-03-20 12:34:53 pose_transfer_train.py:366] preparing optimizer...
[2024-03-20 12:34:53 pose_transfer_train.py:372] preparing accelerator...
[2024-03-20 12:34:58 pose_transfer_train.py:379] loading states from ./checkpoints/
[2024-03-20 12:35:00 pose_transfer_train.py:389] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 12:35:08 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 12:35:08 pose_transfer_train.py:398] preparing lr scheduler...
[2024-03-20 12:35:08 pose_transfer_train.py:404] start training...
[2024-03-20 12:35:08 pose_transfer_train.py:413] epoch 1 start
[2024-03-20 12:35:45 pose_transfer_train.py:523] Train [1/100](10/25491)  Time 3.5556(3.7414)  Loss 0.0031(0.0340)  Lr 0.00001099  Eta 1 day, 2:28:55
[2024-03-20 12:36:16 pose_transfer_train.py:523] Train [1/100](20/25491)  Time 3.5056(3.4369)  Loss 0.2351(0.0622)  Lr 0.00001189  Eta 1 day, 0:19:01
[2024-03-20 12:36:48 pose_transfer_train.py:523] Train [1/100](30/25491)  Time 3.0587(3.3343)  Loss 0.0049(0.0606)  Lr 0.00001279  Eta 23:34:55
[2024-03-20 12:37:18 pose_transfer_train.py:523] Train [1/100](40/25491)  Time 3.5441(3.2664)  Loss 0.0019(0.0546)  Lr 0.00001369  Eta 23:05:32
[2024-03-20 12:47:00 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 12:47:00 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 12:47:16 pose_transfer_train.py:306] preparing model...
[2024-03-20 12:47:23 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 12:47:47 pose_transfer_train.py:352] number of trainable parameters: 213329208
[2024-03-20 12:47:47 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 12:47:47 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 12:48:01 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 12:48:02 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 12:48:10 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 12:48:10 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 12:48:10 pose_transfer_train.py:405] start training...
[2024-03-20 12:48:10 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 12:48:56 pose_transfer_train.py:520] Train [1/100](10/25491)  Time 3.2308(4.6478)  Loss 0.0031(0.0340)  Lr 0.00001099  Eta 1 day, 8:53:51
[2024-03-20 12:49:29 pose_transfer_train.py:520] Train [1/100](20/25491)  Time 3.2218(3.9302)  Loss 0.2351(0.0622)  Lr 0.00001189  Eta 1 day, 3:48:25
[2024-03-20 12:50:00 pose_transfer_train.py:520] Train [1/100](30/25491)  Time 2.8421(3.6707)  Loss 0.0049(0.0606)  Lr 0.00001279  Eta 1 day, 1:57:40
[2024-03-20 12:50:32 pose_transfer_train.py:520] Train [1/100](40/25491)  Time 2.7583(3.5558)  Loss 0.0019(0.0546)  Lr 0.00001369  Eta 1 day, 1:08:17
[2024-03-20 12:52:55 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 12:52:55 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 12:53:11 pose_transfer_train.py:306] preparing model...
[2024-03-20 12:53:18 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 12:53:39 pose_transfer_train.py:352] number of trainable parameters: 213329208
[2024-03-20 12:53:40 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 12:53:40 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 12:53:55 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 12:53:56 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 12:54:01 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 12:54:01 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 12:54:01 pose_transfer_train.py:405] start training...
[2024-03-20 12:54:01 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 12:54:54 pose_transfer_train.py:520] Train [1/100](10/25491)  Time 3.2562(5.2746)  Loss 0.0031(0.0340)  Lr 0.00001099  Eta 1 day, 13:20:02
[2024-03-20 12:55:26 pose_transfer_train.py:520] Train [1/100](20/25491)  Time 3.0846(4.2514)  Loss 0.2351(0.0622)  Lr 0.00001189  Eta 1 day, 6:04:47
[2024-03-20 12:55:56 pose_transfer_train.py:520] Train [1/100](30/25491)  Time 3.2609(3.8488)  Loss 0.0049(0.0606)  Lr 0.00001279  Eta 1 day, 3:13:15
[2024-03-20 12:56:26 pose_transfer_train.py:520] Train [1/100](40/25491)  Time 2.8931(3.6241)  Loss 0.0019(0.0546)  Lr 0.00001369  Eta 1 day, 1:37:17
[2024-03-20 12:59:59 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 12:59:59 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 13:00:20 pose_transfer_train.py:306] preparing model...
[2024-03-20 13:00:27 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 13:00:48 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 13:00:48 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 13:00:48 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 13:00:57 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 13:02:12 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 13:02:12 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 13:02:29 pose_transfer_train.py:306] preparing model...
[2024-03-20 13:02:34 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 13:02:58 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 13:02:58 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 13:02:58 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 13:03:09 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 13:03:10 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 13:03:16 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 13:03:16 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 13:03:16 pose_transfer_train.py:405] start training...
[2024-03-20 13:03:16 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 13:03:48 pose_transfer_train.py:520] Train [1/100](10/25491)  Time 1.6913(3.1060)  Loss 0.0031(0.0341)  Lr 0.00001099  Eta 21:59:04
[2024-03-20 13:04:08 pose_transfer_train.py:520] Train [1/100](20/25491)  Time 2.0509(2.5769)  Loss 0.2355(0.0623)  Lr 0.00001189  Eta 18:13:55
[2024-03-20 13:04:27 pose_transfer_train.py:520] Train [1/100](30/25491)  Time 1.8563(2.3617)  Loss 0.0050(0.0607)  Lr 0.00001279  Eta 16:42:11
[2024-03-20 13:04:48 pose_transfer_train.py:520] Train [1/100](40/25491)  Time 2.1254(2.2766)  Loss 0.0020(0.0547)  Lr 0.00001369  Eta 16:05:42
[2024-03-20 13:05:07 pose_transfer_train.py:520] Train [1/100](50/25491)  Time 1.9417(2.2105)  Loss 0.0026(0.0474)  Lr 0.00001459  Eta 15:37:18
[2024-03-20 13:05:27 pose_transfer_train.py:520] Train [1/100](60/25491)  Time 1.7193(2.1685)  Loss 0.0030(0.0462)  Lr 0.00001549  Eta 15:19:06
[2024-03-20 13:05:46 pose_transfer_train.py:520] Train [1/100](70/25491)  Time 1.9606(2.1352)  Loss 0.0156(0.0444)  Lr 0.00001639  Eta 15:04:40
[2024-03-20 13:06:07 pose_transfer_train.py:520] Train [1/100](80/25491)  Time 2.6241(2.1365)  Loss 0.0031(0.0437)  Lr 0.00001729  Eta 15:04:49
[2024-03-20 13:06:27 pose_transfer_train.py:520] Train [1/100](90/25491)  Time 2.0352(2.1126)  Loss 0.0031(0.0430)  Lr 0.00001819  Eta 14:54:23
[2024-03-20 13:06:47 pose_transfer_train.py:520] Train [1/100](100/25491)  Time 2.1889(2.1052)  Loss 0.0071(0.0428)  Lr 0.00001909  Eta 14:50:52
[2024-03-20 13:07:06 pose_transfer_train.py:520] Train [1/100](110/25491)  Time 1.4680(2.0907)  Loss 0.0037(0.0424)  Lr 0.00001999  Eta 14:44:22
[2024-03-20 13:18:13 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 13:18:13 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 13:18:35 pose_transfer_train.py:306] preparing model...
[2024-03-20 13:18:41 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 13:19:04 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 13:19:04 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 13:19:04 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 13:19:11 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 13:19:13 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 13:19:19 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 13:19:19 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 13:19:19 pose_transfer_train.py:405] start training...
[2024-03-20 13:19:19 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 13:19:55 pose_transfer_train.py:516] Train [1/100](10/25491)  Time 2.3457(3.6254)  Loss 0.0031(0.0341)  Lr 0.00001099  Eta 1 day, 1:39:39
[2024-03-20 13:20:17 pose_transfer_train.py:516] Train [1/100](20/25491)  Time 2.0066(2.9003)  Loss 0.2355(0.0623)  Lr 0.00001189  Eta 20:31:14
[2024-03-20 13:20:40 pose_transfer_train.py:516] Train [1/100](30/25491)  Time 2.1168(2.6883)  Loss 0.0050(0.0607)  Lr 0.00001279  Eta 19:00:45
[2024-03-20 13:21:03 pose_transfer_train.py:516] Train [1/100](40/25491)  Time 2.2202(2.5966)  Loss 0.0020(0.0547)  Lr 0.00001369  Eta 18:21:26
[2024-03-20 13:21:25 pose_transfer_train.py:516] Train [1/100](50/25491)  Time 1.8273(2.5209)  Loss 0.0026(0.0474)  Lr 0.00001459  Eta 17:48:54
[2024-03-20 13:21:49 pose_transfer_train.py:516] Train [1/100](60/25491)  Time 2.1260(2.5058)  Loss 0.0030(0.0462)  Lr 0.00001549  Eta 17:42:05
[2024-03-20 13:22:12 pose_transfer_train.py:516] Train [1/100](70/25491)  Time 2.3796(2.4635)  Loss 0.0156(0.0444)  Lr 0.00001639  Eta 17:23:43
[2024-03-20 13:22:36 pose_transfer_train.py:516] Train [1/100](80/25491)  Time 3.3912(2.4582)  Loss 0.0031(0.0437)  Lr 0.00001729  Eta 17:21:05
[2024-03-20 13:22:58 pose_transfer_train.py:516] Train [1/100](90/25491)  Time 1.7552(2.4301)  Loss 0.0031(0.0430)  Lr 0.00001819  Eta 17:08:45
[2024-03-20 13:23:21 pose_transfer_train.py:516] Train [1/100](100/25491)  Time 2.3817(2.4196)  Loss 0.0071(0.0428)  Lr 0.00001909  Eta 17:03:57
[2024-03-20 13:34:50 pose_transfer_train.py:283] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 13:34:50 pose_transfer_train.py:285] preparing datasets...
[2024-03-20 13:35:08 pose_transfer_train.py:308] preparing model...
[2024-03-20 13:35:16 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 13:35:38 pose_transfer_train.py:354] number of trainable parameters: 73112832
[2024-03-20 13:35:39 pose_transfer_train.py:369] preparing optimizer...
[2024-03-20 13:35:39 pose_transfer_train.py:375] preparing accelerator...
[2024-03-20 13:35:47 pose_transfer_train.py:382] loading states from ./checkpoints/
[2024-03-20 13:35:49 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['learnabl_evector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 13:35:55 pose_transfer_train.py:395] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 13:35:55 pose_transfer_train.py:401] preparing lr scheduler...
[2024-03-20 13:35:55 pose_transfer_train.py:407] start training...
[2024-03-20 13:35:55 pose_transfer_train.py:416] epoch 1 start
[2024-03-20 13:38:07 pose_transfer_train.py:283] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 13:38:07 pose_transfer_train.py:285] preparing datasets...
[2024-03-20 13:38:28 pose_transfer_train.py:308] preparing model...
[2024-03-20 13:38:34 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 13:38:59 pose_transfer_train.py:354] number of trainable parameters: 73112832
[2024-03-20 13:39:00 pose_transfer_train.py:369] preparing optimizer...
[2024-03-20 13:39:00 pose_transfer_train.py:375] preparing accelerator...
[2024-03-20 13:39:04 pose_transfer_train.py:382] loading states from ./checkpoints/
[2024-03-20 13:39:06 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 13:39:11 pose_transfer_train.py:395] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 13:39:11 pose_transfer_train.py:401] preparing lr scheduler...
[2024-03-20 13:39:11 pose_transfer_train.py:407] start training...
[2024-03-20 13:39:11 pose_transfer_train.py:416] epoch 1 start
[2024-03-20 13:39:43 pose_transfer_train.py:518] Train [1/100](10/25491)  Time 2.0107(3.2220)  Loss 0.0031(0.0341)  Lr 0.00001099  Eta 22:48:20
[2024-03-20 13:40:06 pose_transfer_train.py:518] Train [1/100](20/25491)  Time 2.3995(2.7195)  Loss 0.2355(0.0623)  Lr 0.00001189  Eta 19:14:27
[2024-03-20 13:40:28 pose_transfer_train.py:518] Train [1/100](30/25491)  Time 2.3266(2.5710)  Loss 0.0050(0.0607)  Lr 0.00001279  Eta 18:10:59
[2024-03-20 13:40:52 pose_transfer_train.py:518] Train [1/100](40/25491)  Time 2.2428(2.5202)  Loss 0.0020(0.0547)  Lr 0.00001369  Eta 17:49:02
[2024-03-20 13:41:15 pose_transfer_train.py:518] Train [1/100](50/25491)  Time 2.5174(2.4720)  Loss 0.0026(0.0474)  Lr 0.00001459  Eta 17:28:10
[2024-03-20 13:41:39 pose_transfer_train.py:518] Train [1/100](60/25491)  Time 2.3669(2.4600)  Loss 0.0030(0.0462)  Lr 0.00001549  Eta 17:22:41
[2024-03-20 13:42:02 pose_transfer_train.py:518] Train [1/100](70/25491)  Time 2.0148(2.4441)  Loss 0.0156(0.0444)  Lr 0.00001639  Eta 17:15:31
[2024-03-20 13:42:26 pose_transfer_train.py:518] Train [1/100](80/25491)  Time 3.0908(2.4361)  Loss 0.0031(0.0437)  Lr 0.00001729  Eta 17:11:43
[2024-03-20 13:42:48 pose_transfer_train.py:518] Train [1/100](90/25491)  Time 2.4131(2.4117)  Loss 0.0031(0.0430)  Lr 0.00001819  Eta 17:00:58
[2024-03-20 13:43:11 pose_transfer_train.py:518] Train [1/100](100/25491)  Time 2.1414(2.3989)  Loss 0.0071(0.0428)  Lr 0.00001909  Eta 16:55:11
[2024-03-20 13:43:34 pose_transfer_train.py:518] Train [1/100](110/25491)  Time 2.0679(2.3916)  Loss 0.0037(0.0424)  Lr 0.00001999  Eta 16:51:40
[2024-03-20 13:50:18 pose_transfer_train.py:283] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 13:50:18 pose_transfer_train.py:285] preparing datasets...
[2024-03-20 13:50:38 pose_transfer_train.py:308] preparing model...
[2024-03-20 13:50:45 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 13:51:04 pose_transfer_train.py:354] number of trainable parameters: 73112832
[2024-03-20 13:51:04 pose_transfer_train.py:369] preparing optimizer...
[2024-03-20 13:51:04 pose_transfer_train.py:375] preparing accelerator...
[2024-03-20 13:51:18 pose_transfer_train.py:382] loading states from ./checkpoints/
[2024-03-20 13:51:20 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 13:51:28 pose_transfer_train.py:395] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 13:51:28 pose_transfer_train.py:401] preparing lr scheduler...
[2024-03-20 13:51:28 pose_transfer_train.py:407] start training...
[2024-03-20 13:51:28 pose_transfer_train.py:416] epoch 1 start
[2024-03-20 13:52:01 pose_transfer_train.py:518] Train [1/100](10/25491)  Time 1.7786(3.3016)  Loss 0.0031(0.0341)  Lr 0.00001099  Eta 23:22:08
[2024-03-20 13:52:24 pose_transfer_train.py:518] Train [1/100](20/25491)  Time 2.3756(2.8027)  Loss 0.2355(0.0623)  Lr 0.00001189  Eta 19:49:46
[2024-03-20 13:52:46 pose_transfer_train.py:518] Train [1/100](30/25491)  Time 2.2160(2.6083)  Loss 0.0050(0.0607)  Lr 0.00001279  Eta 18:26:49
[2024-03-20 13:53:09 pose_transfer_train.py:518] Train [1/100](40/25491)  Time 2.4634(2.5196)  Loss 0.0020(0.0547)  Lr 0.00001369  Eta 17:48:45
[2024-03-20 13:53:32 pose_transfer_train.py:518] Train [1/100](50/25491)  Time 2.3433(2.4839)  Loss 0.0026(0.0474)  Lr 0.00001459  Eta 17:33:13
[2024-03-20 13:53:56 pose_transfer_train.py:518] Train [1/100](60/25491)  Time 2.6123(2.4739)  Loss 0.0030(0.0462)  Lr 0.00001549  Eta 17:28:34
[2024-03-20 13:54:18 pose_transfer_train.py:518] Train [1/100](70/25491)  Time 1.9985(2.4332)  Loss 0.0156(0.0444)  Lr 0.00001639  Eta 17:10:53
[2024-03-20 13:54:41 pose_transfer_train.py:518] Train [1/100](80/25491)  Time 2.5844(2.4136)  Loss 0.0031(0.0437)  Lr 0.00001729  Eta 17:02:11
[2024-03-20 13:55:03 pose_transfer_train.py:518] Train [1/100](90/25491)  Time 2.2625(2.3885)  Loss 0.0031(0.0430)  Lr 0.00001819  Eta 16:51:11
[2024-03-20 14:14:40 pose_transfer_train.py:283] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 14:14:40 pose_transfer_train.py:285] preparing datasets...
[2024-03-20 14:14:58 pose_transfer_train.py:308] preparing model...
[2024-03-20 14:15:03 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 14:15:28 pose_transfer_train.py:354] number of trainable parameters: 73112832
[2024-03-20 14:15:28 pose_transfer_train.py:369] preparing optimizer...
[2024-03-20 14:15:28 pose_transfer_train.py:375] preparing accelerator...
[2024-03-20 14:15:34 pose_transfer_train.py:382] loading states from ./checkpoints/
[2024-03-20 14:15:35 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 14:15:43 pose_transfer_train.py:395] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 14:15:43 pose_transfer_train.py:401] preparing lr scheduler...
[2024-03-20 14:15:43 pose_transfer_train.py:407] start training...
[2024-03-20 14:15:43 pose_transfer_train.py:416] epoch 1 start
[2024-03-20 14:17:46 pose_transfer_train.py:283] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: True
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 14:17:46 pose_transfer_train.py:285] preparing datasets...
[2024-03-20 14:18:04 pose_transfer_train.py:308] preparing model...
[2024-03-20 14:18:10 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 14:18:35 pose_transfer_train.py:354] number of trainable parameters: 73112832
[2024-03-20 14:18:35 pose_transfer_train.py:369] preparing optimizer...
[2024-03-20 14:18:35 pose_transfer_train.py:375] preparing accelerator...
[2024-03-20 14:18:42 pose_transfer_train.py:382] loading states from ./checkpoints/
[2024-03-20 14:18:42 pose_transfer_train.py:392] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 14:18:50 pose_transfer_train.py:395] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 14:18:50 pose_transfer_train.py:401] preparing lr scheduler...
[2024-03-20 14:18:50 pose_transfer_train.py:407] start training...
[2024-03-20 14:18:50 pose_transfer_train.py:416] epoch 1 start
[2024-03-20 14:19:17 pose_transfer_train.py:518] Train [1/100](10/25491)  Time 1.5636(2.7182)  Loss 0.0031(0.0341)  Lr 0.00001099  Eta 19:14:23
[2024-03-20 14:19:35 pose_transfer_train.py:518] Train [1/100](20/25491)  Time 1.6826(2.2145)  Loss 0.2355(0.0623)  Lr 0.00001189  Eta 15:40:04
[2024-03-20 14:19:52 pose_transfer_train.py:518] Train [1/100](30/25491)  Time 1.3834(2.0468)  Loss 0.0050(0.0607)  Lr 0.00001279  Eta 14:28:33
[2024-03-20 14:20:09 pose_transfer_train.py:518] Train [1/100](40/25491)  Time 1.6397(1.9726)  Loss 0.0020(0.0547)  Lr 0.00001369  Eta 13:56:43
[2024-03-20 14:20:27 pose_transfer_train.py:518] Train [1/100](50/25491)  Time 1.9602(1.9332)  Loss 0.0026(0.0474)  Lr 0.00001459  Eta 13:39:41
[2024-03-20 14:20:45 pose_transfer_train.py:518] Train [1/100](60/25491)  Time 1.5819(1.9145)  Loss 0.0030(0.0462)  Lr 0.00001549  Eta 13:31:28
[2024-03-20 14:21:02 pose_transfer_train.py:518] Train [1/100](70/25491)  Time 1.7558(1.8771)  Loss 0.0156(0.0444)  Lr 0.00001639  Eta 13:15:17
[2024-03-20 14:21:19 pose_transfer_train.py:518] Train [1/100](80/25491)  Time 1.6806(1.8564)  Loss 0.0031(0.0437)  Lr 0.00001729  Eta 13:06:12
[2024-03-20 14:21:38 pose_transfer_train.py:518] Train [1/100](90/25491)  Time 1.6901(1.8617)  Loss 0.0031(0.0430)  Lr 0.00001819  Eta 13:08:09
[2024-03-20 14:21:55 pose_transfer_train.py:518] Train [1/100](100/25491)  Time 1.8957(1.8481)  Loss 0.0071(0.0428)  Lr 0.00001909  Eta 13:02:05
[2024-03-20 14:22:13 pose_transfer_train.py:518] Train [1/100](110/25491)  Time 1.8987(1.8429)  Loss 0.0037(0.0424)  Lr 0.00001999  Eta 12:59:33
[2024-03-20 14:22:31 pose_transfer_train.py:518] Train [1/100](120/25491)  Time 1.7659(1.8368)  Loss 0.0018(0.0406)  Lr 0.00002089  Eta 12:56:42
[2024-03-20 14:22:48 pose_transfer_train.py:518] Train [1/100](130/25491)  Time 2.0510(1.8285)  Loss 0.0079(0.0396)  Lr 0.00002179  Eta 12:52:51
[2024-03-20 14:23:05 pose_transfer_train.py:518] Train [1/100](140/25491)  Time 1.5242(1.8218)  Loss 0.0078(0.0379)  Lr 0.00002269  Eta 12:49:44
[2024-03-20 14:23:22 pose_transfer_train.py:518] Train [1/100](150/25491)  Time 1.6804(1.8144)  Loss 0.2552(0.0381)  Lr 0.00002359  Eta 12:46:17
[2024-03-20 14:23:39 pose_transfer_train.py:518] Train [1/100](160/25491)  Time 1.7099(1.8039)  Loss 0.0016(0.0379)  Lr 0.00002449  Eta 12:41:35
[2024-03-20 14:23:57 pose_transfer_train.py:518] Train [1/100](170/25491)  Time 1.6151(1.8041)  Loss 0.0019(0.0381)  Lr 0.00002539  Eta 12:41:21
[2024-03-20 14:24:14 pose_transfer_train.py:518] Train [1/100](180/25491)  Time 1.7192(1.8009)  Loss 0.0358(0.0383)  Lr 0.00002629  Eta 12:39:43
[2024-03-20 14:24:33 pose_transfer_train.py:518] Train [1/100](190/25491)  Time 1.6516(1.8028)  Loss 0.0213(0.0378)  Lr 0.00002719  Eta 12:40:12
[2024-03-20 14:24:51 pose_transfer_train.py:518] Train [1/100](200/25491)  Time 1.6602(1.8049)  Loss 0.0130(0.0387)  Lr 0.00002809  Eta 12:40:48
[2024-03-20 14:25:10 pose_transfer_train.py:518] Train [1/100](210/25491)  Time 1.6068(1.8062)  Loss 0.0019(0.0373)  Lr 0.00002899  Eta 12:41:01
[2024-03-20 14:25:27 pose_transfer_train.py:518] Train [1/100](220/25491)  Time 1.7396(1.8053)  Loss 0.0035(0.0369)  Lr 0.00002989  Eta 12:40:21
[2024-03-20 14:25:45 pose_transfer_train.py:518] Train [1/100](230/25491)  Time 1.6899(1.8050)  Loss 0.0059(0.0358)  Lr 0.00003079  Eta 12:39:55
[2024-03-20 14:26:03 pose_transfer_train.py:518] Train [1/100](240/25491)  Time 1.7618(1.8030)  Loss 0.1236(0.0373)  Lr 0.00003169  Eta 12:38:48
[2024-03-20 14:26:20 pose_transfer_train.py:518] Train [1/100](250/25491)  Time 1.6010(1.8006)  Loss 0.0517(0.0382)  Lr 0.00003259  Eta 12:37:27
[2024-03-20 14:26:39 pose_transfer_train.py:518] Train [1/100](260/25491)  Time 1.9070(1.8011)  Loss 0.0102(0.0373)  Lr 0.00003349  Eta 12:37:24
[2024-03-20 14:26:56 pose_transfer_train.py:518] Train [1/100](270/25491)  Time 1.8037(1.7974)  Loss 0.1198(0.0366)  Lr 0.00003439  Eta 12:35:32
[2024-03-20 14:27:14 pose_transfer_train.py:518] Train [1/100](280/25491)  Time 1.8470(1.7986)  Loss 0.1370(0.0380)  Lr 0.00003529  Eta 12:35:44
[2024-03-20 14:27:31 pose_transfer_train.py:518] Train [1/100](290/25491)  Time 1.4920(1.7968)  Loss 0.0015(0.0372)  Lr 0.00003619  Eta 12:34:40
[2024-03-20 14:27:49 pose_transfer_train.py:518] Train [1/100](300/25491)  Time 1.8664(1.7962)  Loss 0.0057(0.0372)  Lr 0.00003709  Eta 12:34:08
[2024-03-20 14:28:08 pose_transfer_train.py:518] Train [1/100](310/25491)  Time 2.0362(1.7989)  Loss 0.0073(0.0377)  Lr 0.00003799  Eta 12:34:57
[2024-03-20 14:28:26 pose_transfer_train.py:518] Train [1/100](320/25491)  Time 1.7158(1.7986)  Loss 0.0176(0.0381)  Lr 0.00003889  Eta 12:34:32
[2024-03-20 14:28:43 pose_transfer_train.py:518] Train [1/100](330/25491)  Time 1.8747(1.7966)  Loss 0.0097(0.0380)  Lr 0.00003979  Eta 12:33:25
[2024-03-20 14:29:01 pose_transfer_train.py:518] Train [1/100](340/25491)  Time 1.9633(1.7967)  Loss 0.0019(0.0376)  Lr 0.00004069  Eta 12:33:07
[2024-03-20 14:29:19 pose_transfer_train.py:518] Train [1/100](350/25491)  Time 1.9074(1.7970)  Loss 0.0018(0.0367)  Lr 0.00004159  Eta 12:32:58
[2024-03-20 14:29:38 pose_transfer_train.py:518] Train [1/100](360/25491)  Time 2.1394(1.7991)  Loss 0.0375(0.0369)  Lr 0.00004249  Eta 12:33:31
[2024-03-20 14:29:55 pose_transfer_train.py:518] Train [1/100](370/25491)  Time 1.3852(1.7966)  Loss 0.0188(0.0362)  Lr 0.00004339  Eta 12:32:12
[2024-03-20 14:30:12 pose_transfer_train.py:518] Train [1/100](380/25491)  Time 1.6985(1.7947)  Loss 0.0240(0.0358)  Lr 0.00004429  Eta 12:31:06
[2024-03-20 14:30:31 pose_transfer_train.py:518] Train [1/100](390/25491)  Time 1.8675(1.7974)  Loss 0.0834(0.0365)  Lr 0.00004519  Eta 12:31:55
[2024-03-20 14:30:49 pose_transfer_train.py:518] Train [1/100](400/25491)  Time 1.6433(1.7968)  Loss 0.0667(0.0370)  Lr 0.00004609  Eta 12:31:22
[2024-03-20 14:31:08 pose_transfer_train.py:518] Train [1/100](410/25491)  Time 1.8133(1.7989)  Loss 0.1424(0.0372)  Lr 0.00004699  Eta 12:31:58
[2024-03-20 14:31:27 pose_transfer_train.py:518] Train [1/100](420/25491)  Time 1.7044(1.8008)  Loss 0.0782(0.0375)  Lr 0.00004789  Eta 12:32:28
[2024-03-20 14:31:45 pose_transfer_train.py:518] Train [1/100](430/25491)  Time 2.0243(1.8028)  Loss 0.0212(0.0373)  Lr 0.00004879  Eta 12:33:00
[2024-03-20 15:44:48 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 15:44:48 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 15:44:57 pose_transfer_train.py:306] preparing model...
[2024-03-20 15:45:06 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 15:46:42 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 8
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 15:46:42 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 15:46:51 pose_transfer_train.py:306] preparing model...
[2024-03-20 15:46:55 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 15:53:09 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 4
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 15:53:09 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 15:53:18 pose_transfer_train.py:306] preparing model...
[2024-03-20 15:53:22 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 16:12:51 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 16:12:51 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 16:13:09 pose_transfer_train.py:306] preparing model...
[2024-03-20 16:13:16 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 16:19:19 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 16:19:19 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 16:19:36 pose_transfer_train.py:306] preparing model...
[2024-03-20 16:19:42 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 16:20:02 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 16:20:02 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 16:20:02 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 16:20:15 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 16:20:16 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 16:20:21 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 16:20:22 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 16:20:22 pose_transfer_train.py:405] start training...
[2024-03-20 16:20:22 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 16:39:19 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 16:39:19 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 16:39:28 pose_transfer_train.py:306] preparing model...
[2024-03-20 16:39:31 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 16:44:38 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 16:44:38 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 16:44:58 pose_transfer_train.py:306] preparing model...
[2024-03-20 16:45:05 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 16:45:27 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 16:45:27 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 16:45:28 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 16:45:32 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 16:45:33 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 16:45:40 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 16:45:40 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 16:45:40 pose_transfer_train.py:405] start training...
[2024-03-20 16:45:40 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 16:50:19 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 16:50:19 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 16:50:35 pose_transfer_train.py:306] preparing model...
[2024-03-20 16:50:43 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 16:51:07 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 16:51:07 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 16:51:07 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 16:51:11 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 16:51:12 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 16:51:18 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 16:51:18 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 16:51:18 pose_transfer_train.py:405] start training...
[2024-03-20 16:51:18 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 16:53:33 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 16:53:33 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 16:53:54 pose_transfer_train.py:306] preparing model...
[2024-03-20 16:54:01 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 16:54:25 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 16:54:25 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 16:54:25 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 16:54:29 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 16:54:30 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 16:54:36 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 16:54:36 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 16:54:36 pose_transfer_train.py:405] start training...
[2024-03-20 16:54:36 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 17:07:07 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 17:07:07 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 17:07:26 pose_transfer_train.py:306] preparing model...
[2024-03-20 17:07:33 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 17:07:56 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 17:07:56 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 17:07:56 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 17:08:05 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 17:08:06 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 17:08:12 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 17:08:12 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 17:08:12 pose_transfer_train.py:405] start training...
[2024-03-20 17:08:12 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 17:12:56 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 17:12:56 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 17:13:14 pose_transfer_train.py:306] preparing model...
[2024-03-20 17:13:20 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 17:13:43 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 17:13:43 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 17:13:43 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 17:13:51 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 17:13:52 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 17:13:58 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 17:13:58 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 17:13:58 pose_transfer_train.py:405] start training...
[2024-03-20 17:13:58 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 17:18:19 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 17:18:19 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 17:18:40 pose_transfer_train.py:306] preparing model...
[2024-03-20 17:18:47 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 17:19:15 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 17:19:15 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 17:19:15 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 17:19:21 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 17:19:22 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 17:19:27 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 17:19:27 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 17:19:27 pose_transfer_train.py:405] start training...
[2024-03-20 17:19:27 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 17:27:45 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 17:27:45 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 17:28:05 pose_transfer_train.py:306] preparing model...
[2024-03-20 17:28:12 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 17:28:35 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 17:28:35 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 17:28:35 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 17:28:40 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 17:28:42 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 17:28:48 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 17:28:48 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 17:28:48 pose_transfer_train.py:405] start training...
[2024-03-20 17:28:48 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 17:33:47 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 5
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 16
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-20 17:33:47 pose_transfer_train.py:283] preparing datasets...
[2024-03-20 17:34:08 pose_transfer_train.py:306] preparing model...
[2024-03-20 17:34:16 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-20 17:34:39 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-20 17:34:39 pose_transfer_train.py:367] preparing optimizer...
[2024-03-20 17:34:39 pose_transfer_train.py:373] preparing accelerator...
[2024-03-20 17:34:45 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-20 17:34:47 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-20 17:34:53 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-20 17:34:53 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-20 17:34:53 pose_transfer_train.py:405] start training...
[2024-03-20 17:34:53 pose_transfer_train.py:414] epoch 1 start
[2024-03-20 17:35:17 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
[2024-03-20 17:35:54 pose_transfer_train.py:516] Train [1/100](10/6372)  Time 4.0844(6.0834)  Loss 0.0100(0.0336)  Lr 0.00001099  Eta 10:45:02
[2024-03-20 17:36:38 pose_transfer_train.py:516] Train [1/100](20/6372)  Time 4.1467(5.2179)  Loss 0.0900(0.0383)  Lr 0.00001189  Eta 9:12:23
[2024-03-20 17:37:20 pose_transfer_train.py:516] Train [1/100](30/6372)  Time 4.0481(4.9028)  Loss 0.0532(0.0421)  Lr 0.00001279  Eta 8:38:13
[2024-03-20 17:38:05 pose_transfer_train.py:516] Train [1/100](40/6372)  Time 4.8790(4.7964)  Loss 0.0028(0.0407)  Lr 0.00001369  Eta 8:26:10
[2024-03-20 17:38:49 pose_transfer_train.py:516] Train [1/100](50/6372)  Time 4.5877(4.7102)  Loss 0.0277(0.0403)  Lr 0.00001459  Eta 8:16:17
[2024-03-20 17:39:34 pose_transfer_train.py:516] Train [1/100](60/6372)  Time 4.1936(4.6832)  Loss 0.0140(0.0401)  Lr 0.00001549  Eta 8:12:40
[2024-03-20 17:40:20 pose_transfer_train.py:516] Train [1/100](70/6372)  Time 4.7225(4.6713)  Loss 0.0076(0.0383)  Lr 0.00001639  Eta 8:10:38
[2024-03-20 17:41:05 pose_transfer_train.py:516] Train [1/100](80/6372)  Time 4.5862(4.6531)  Loss 0.0638(0.0370)  Lr 0.00001729  Eta 8:07:57
[2024-03-20 17:41:48 pose_transfer_train.py:516] Train [1/100](90/6372)  Time 4.0018(4.6104)  Loss 0.0063(0.0359)  Lr 0.00001819  Eta 8:02:42
[2024-03-20 17:42:33 pose_transfer_train.py:516] Train [1/100](100/6372)  Time 4.6253(4.5944)  Loss 0.0398(0.0375)  Lr 0.00001909  Eta 8:00:16
[2024-03-20 17:43:16 pose_transfer_train.py:516] Train [1/100](110/6372)  Time 4.4587(4.5682)  Loss 0.0232(0.0370)  Lr 0.00001999  Eta 7:56:46
[2024-03-20 17:43:59 pose_transfer_train.py:516] Train [1/100](120/6372)  Time 4.6182(4.5519)  Loss 0.0146(0.0363)  Lr 0.00002089  Eta 7:54:18
[2024-03-20 17:44:44 pose_transfer_train.py:516] Train [1/100](130/6372)  Time 4.5640(4.5426)  Loss 0.0659(0.0363)  Lr 0.00002179  Eta 7:52:34
[2024-03-20 17:45:27 pose_transfer_train.py:516] Train [1/100](140/6372)  Time 4.7221(4.5294)  Loss 0.1208(0.0370)  Lr 0.00002269  Eta 7:50:27
[2024-03-20 17:46:10 pose_transfer_train.py:516] Train [1/100](150/6372)  Time 3.7408(4.5136)  Loss 0.0964(0.0366)  Lr 0.00002359  Eta 7:48:03
[2024-03-20 17:46:53 pose_transfer_train.py:516] Train [1/100](160/6372)  Time 4.1536(4.4966)  Loss 0.0169(0.0366)  Lr 0.00002449  Eta 7:45:32
[2024-03-20 17:47:36 pose_transfer_train.py:516] Train [1/100](170/6372)  Time 4.0404(4.4892)  Loss 0.0091(0.0368)  Lr 0.00002539  Eta 7:44:02
[2024-03-20 17:48:21 pose_transfer_train.py:516] Train [1/100](180/6372)  Time 4.7957(4.4861)  Loss 0.0420(0.0367)  Lr 0.00002629  Eta 7:42:58
[2024-03-20 17:49:03 pose_transfer_train.py:516] Train [1/100](190/6372)  Time 4.0859(4.4737)  Loss 0.0124(0.0365)  Lr 0.00002719  Eta 7:40:56
[2024-03-20 17:49:46 pose_transfer_train.py:516] Train [1/100](200/6372)  Time 4.1788(4.4645)  Loss 0.0058(0.0366)  Lr 0.00002809  Eta 7:39:14
[2024-03-20 17:50:31 pose_transfer_train.py:516] Train [1/100](210/6372)  Time 5.1733(4.4657)  Loss 0.0124(0.0358)  Lr 0.00002899  Eta 7:38:37
[2024-03-20 17:51:17 pose_transfer_train.py:516] Train [1/100](220/6372)  Time 4.7203(4.4705)  Loss 0.1121(0.0364)  Lr 0.00002989  Eta 7:38:22
[2024-03-20 17:52:01 pose_transfer_train.py:516] Train [1/100](230/6372)  Time 4.2932(4.4683)  Loss 0.0529(0.0365)  Lr 0.00003079  Eta 7:37:24
[2024-03-20 17:52:45 pose_transfer_train.py:516] Train [1/100](240/6372)  Time 4.7555(4.4662)  Loss 0.0371(0.0366)  Lr 0.00003169  Eta 7:36:26
[2024-03-20 17:53:31 pose_transfer_train.py:516] Train [1/100](250/6372)  Time 4.3012(4.4702)  Loss 0.0408(0.0366)  Lr 0.00003259  Eta 7:36:06
[2024-03-20 17:54:14 pose_transfer_train.py:516] Train [1/100](260/6372)  Time 4.6480(4.4663)  Loss 0.0084(0.0362)  Lr 0.00003349  Eta 7:34:58
[2024-03-20 17:55:01 pose_transfer_train.py:516] Train [1/100](270/6372)  Time 4.8115(4.4736)  Loss 0.0513(0.0360)  Lr 0.00003439  Eta 7:34:58
[2024-03-20 17:55:46 pose_transfer_train.py:516] Train [1/100](280/6372)  Time 4.1662(4.4733)  Loss 0.0720(0.0360)  Lr 0.00003529  Eta 7:34:11
[2024-03-20 17:56:30 pose_transfer_train.py:516] Train [1/100](290/6372)  Time 4.6812(4.4731)  Loss 0.0143(0.0355)  Lr 0.00003619  Eta 7:33:25
[2024-03-20 17:57:12 pose_transfer_train.py:516] Train [1/100](300/6372)  Time 4.4990(4.4628)  Loss 0.0033(0.0356)  Lr 0.00003709  Eta 7:31:38
[2024-03-20 17:57:58 pose_transfer_train.py:516] Train [1/100](310/6372)  Time 4.6549(4.4654)  Loss 0.0436(0.0356)  Lr 0.00003799  Eta 7:31:09
[2024-03-20 17:58:42 pose_transfer_train.py:516] Train [1/100](320/6372)  Time 4.1454(4.4651)  Loss 0.0285(0.0357)  Lr 0.00003889  Eta 7:30:22
[2024-03-20 17:59:25 pose_transfer_train.py:516] Train [1/100](330/6372)  Time 4.5651(4.4600)  Loss 0.0294(0.0355)  Lr 0.00003979  Eta 7:29:07
[2024-03-20 18:00:09 pose_transfer_train.py:516] Train [1/100](340/6372)  Time 4.6898(4.4569)  Loss 0.0149(0.0355)  Lr 0.00004069  Eta 7:28:03
[2024-03-20 18:00:52 pose_transfer_train.py:516] Train [1/100](350/6372)  Time 4.8121(4.4549)  Loss 0.0230(0.0352)  Lr 0.00004159  Eta 7:27:07
[2024-03-20 18:01:35 pose_transfer_train.py:516] Train [1/100](360/6372)  Time 4.4622(4.4480)  Loss 0.0701(0.0356)  Lr 0.00004249  Eta 7:25:41
[2024-03-20 18:02:18 pose_transfer_train.py:516] Train [1/100](370/6372)  Time 4.4237(4.4456)  Loss 0.0421(0.0358)  Lr 0.00004339  Eta 7:24:42
[2024-03-20 18:03:03 pose_transfer_train.py:516] Train [1/100](380/6372)  Time 4.6047(4.4463)  Loss 0.0382(0.0358)  Lr 0.00004429  Eta 7:24:02
[2024-03-20 18:03:46 pose_transfer_train.py:516] Train [1/100](390/6372)  Time 4.3548(4.4437)  Loss 0.0347(0.0361)  Lr 0.00004519  Eta 7:23:02
[2024-03-20 18:04:31 pose_transfer_train.py:516] Train [1/100](400/6372)  Time 4.6395(4.4434)  Loss 0.0269(0.0362)  Lr 0.00004609  Eta 7:22:15
[2024-03-20 18:05:14 pose_transfer_train.py:516] Train [1/100](410/6372)  Time 3.8778(4.4401)  Loss 0.0913(0.0362)  Lr 0.00004699  Eta 7:21:11
[2024-03-20 18:05:58 pose_transfer_train.py:516] Train [1/100](420/6372)  Time 4.0212(4.4401)  Loss 0.0807(0.0361)  Lr 0.00004789  Eta 7:20:27
[2024-03-20 18:06:43 pose_transfer_train.py:516] Train [1/100](430/6372)  Time 4.4627(4.4402)  Loss 0.0206(0.0360)  Lr 0.00004879  Eta 7:19:43
[2024-03-20 18:07:24 pose_transfer_train.py:516] Train [1/100](440/6372)  Time 4.2824(4.4325)  Loss 0.0225(0.0358)  Lr 0.00004969  Eta 7:18:13
[2024-03-20 18:08:07 pose_transfer_train.py:516] Train [1/100](450/6372)  Time 4.0123(4.4310)  Loss 0.0287(0.0359)  Lr 0.00005059  Eta 7:17:20
[2024-03-20 18:08:52 pose_transfer_train.py:516] Train [1/100](460/6372)  Time 4.5997(4.4317)  Loss 0.0582(0.0360)  Lr 0.00005149  Eta 7:16:40
[2024-03-20 18:09:37 pose_transfer_train.py:516] Train [1/100](470/6372)  Time 4.6380(4.4344)  Loss 0.0432(0.0360)  Lr 0.00005239  Eta 7:16:11
[2024-03-20 18:10:22 pose_transfer_train.py:516] Train [1/100](480/6372)  Time 4.3114(4.4340)  Loss 0.0294(0.0358)  Lr 0.00005329  Eta 7:15:24
[2024-03-20 18:11:06 pose_transfer_train.py:516] Train [1/100](490/6372)  Time 4.0513(4.4348)  Loss 0.0246(0.0356)  Lr 0.00005419  Eta 7:14:45
[2024-03-20 18:11:51 pose_transfer_train.py:516] Train [1/100](500/6372)  Time 3.7833(4.4365)  Loss 0.0333(0.0356)  Lr 0.00005509  Eta 7:14:11
[2024-03-20 18:12:35 pose_transfer_train.py:516] Train [1/100](510/6372)  Time 4.3604(4.4354)  Loss 0.0719(0.0356)  Lr 0.00005599  Eta 7:13:20
[2024-03-20 18:13:19 pose_transfer_train.py:516] Train [1/100](520/6372)  Time 4.8355(4.4336)  Loss 0.0131(0.0356)  Lr 0.00005689  Eta 7:12:25
[2024-03-20 18:14:03 pose_transfer_train.py:516] Train [1/100](530/6372)  Time 4.5693(4.4334)  Loss 0.0221(0.0355)  Lr 0.00005779  Eta 7:11:39
[2024-03-20 18:14:48 pose_transfer_train.py:516] Train [1/100](540/6372)  Time 4.3895(4.4349)  Loss 0.0084(0.0354)  Lr 0.00005869  Eta 7:11:04
[2024-03-20 18:15:32 pose_transfer_train.py:516] Train [1/100](550/6372)  Time 4.2312(4.4343)  Loss 0.0765(0.0356)  Lr 0.00005959  Eta 7:10:16
[2024-03-20 18:16:16 pose_transfer_train.py:516] Train [1/100](560/6372)  Time 4.3963(4.4338)  Loss 0.0748(0.0355)  Lr 0.00006049  Eta 7:09:29
[2024-03-20 18:17:00 pose_transfer_train.py:516] Train [1/100](570/6372)  Time 4.6041(4.4322)  Loss 0.0479(0.0358)  Lr 0.00006139  Eta 7:08:35
[2024-03-20 18:17:44 pose_transfer_train.py:516] Train [1/100](580/6372)  Time 4.5364(4.4326)  Loss 0.0951(0.0361)  Lr 0.00006229  Eta 7:07:53
[2024-03-20 18:18:28 pose_transfer_train.py:516] Train [1/100](590/6372)  Time 4.4703(4.4320)  Loss 0.0623(0.0359)  Lr 0.00006319  Eta 7:07:05
[2024-03-20 18:19:13 pose_transfer_train.py:516] Train [1/100](600/6372)  Time 4.4283(4.4321)  Loss 0.0018(0.0357)  Lr 0.00006409  Eta 7:06:22
[2024-03-20 18:19:57 pose_transfer_train.py:516] Train [1/100](610/6372)  Time 4.4054(4.4324)  Loss 0.0569(0.0358)  Lr 0.00006499  Eta 7:05:39
[2024-03-20 18:20:40 pose_transfer_train.py:516] Train [1/100](620/6372)  Time 4.2322(4.4295)  Loss 0.0448(0.0357)  Lr 0.00006589  Eta 7:04:38
[2024-03-20 18:21:22 pose_transfer_train.py:516] Train [1/100](630/6372)  Time 4.1915(4.4261)  Loss 0.0443(0.0357)  Lr 0.00006679  Eta 7:03:34
[2024-03-20 18:22:06 pose_transfer_train.py:516] Train [1/100](640/6372)  Time 4.3944(4.4255)  Loss 0.1595(0.0359)  Lr 0.00006769  Eta 7:02:46
[2024-03-20 18:22:48 pose_transfer_train.py:516] Train [1/100](650/6372)  Time 4.4468(4.4234)  Loss 0.0514(0.0359)  Lr 0.00006859  Eta 7:01:50
[2024-03-20 18:23:33 pose_transfer_train.py:516] Train [1/100](660/6372)  Time 4.2969(4.4242)  Loss 0.0186(0.0357)  Lr 0.00006949  Eta 7:01:10
[2024-03-20 18:24:18 pose_transfer_train.py:516] Train [1/100](670/6372)  Time 3.9757(4.4250)  Loss 0.0160(0.0357)  Lr 0.00007039  Eta 7:00:31
[2024-03-20 18:25:01 pose_transfer_train.py:516] Train [1/100](680/6372)  Time 4.4216(4.4234)  Loss 0.0146(0.0357)  Lr 0.00007129  Eta 6:59:38
[2024-03-20 18:25:46 pose_transfer_train.py:516] Train [1/100](690/6372)  Time 4.6047(4.4245)  Loss 0.0147(0.0359)  Lr 0.00007219  Eta 6:58:59
[2024-03-20 18:26:31 pose_transfer_train.py:516] Train [1/100](700/6372)  Time 4.4137(4.4254)  Loss 0.0235(0.0360)  Lr 0.00007309  Eta 6:58:20
[2024-03-20 18:27:12 pose_transfer_train.py:516] Train [1/100](710/6372)  Time 4.0988(4.4207)  Loss 0.0021(0.0359)  Lr 0.00007399  Eta 6:57:09
[2024-03-20 18:27:58 pose_transfer_train.py:516] Train [1/100](720/6372)  Time 4.5075(4.4229)  Loss 0.0643(0.0358)  Lr 0.00007489  Eta 6:56:38
[2024-03-20 18:28:42 pose_transfer_train.py:516] Train [1/100](730/6372)  Time 4.7710(4.4227)  Loss 0.0065(0.0357)  Lr 0.00007579  Eta 6:55:52
[2024-03-20 18:29:27 pose_transfer_train.py:516] Train [1/100](740/6372)  Time 4.6953(4.4235)  Loss 0.0154(0.0356)  Lr 0.00007669  Eta 6:55:13
[2024-03-20 18:30:11 pose_transfer_train.py:516] Train [1/100](750/6372)  Time 4.5163(4.4234)  Loss 0.0469(0.0357)  Lr 0.00007759  Eta 6:54:28
[2024-03-20 18:30:54 pose_transfer_train.py:516] Train [1/100](760/6372)  Time 4.0690(4.4222)  Loss 0.0049(0.0358)  Lr 0.00007849  Eta 6:53:37
[2024-03-20 18:31:37 pose_transfer_train.py:516] Train [1/100](770/6372)  Time 4.0522(4.4208)  Loss 0.0549(0.0359)  Lr 0.00007939  Eta 6:52:45
[2024-03-20 18:32:21 pose_transfer_train.py:516] Train [1/100](780/6372)  Time 4.1087(4.4208)  Loss 0.0815(0.0358)  Lr 0.00008029  Eta 6:52:01
[2024-03-20 18:33:05 pose_transfer_train.py:516] Train [1/100](790/6372)  Time 4.5428(4.4200)  Loss 0.0287(0.0360)  Lr 0.00008119  Eta 6:51:12
[2024-03-20 18:33:49 pose_transfer_train.py:516] Train [1/100](800/6372)  Time 4.3659(4.4191)  Loss 0.0200(0.0361)  Lr 0.00008209  Eta 6:50:23
[2024-03-20 18:34:34 pose_transfer_train.py:516] Train [1/100](810/6372)  Time 4.8454(4.4204)  Loss 0.0098(0.0362)  Lr 0.00008299  Eta 6:49:46
[2024-03-20 18:35:16 pose_transfer_train.py:516] Train [1/100](820/6372)  Time 4.7946(4.4180)  Loss 0.0202(0.0361)  Lr 0.00008389  Eta 6:48:48
[2024-03-20 18:36:01 pose_transfer_train.py:516] Train [1/100](830/6372)  Time 4.2472(4.4189)  Loss 0.0026(0.0359)  Lr 0.00008479  Eta 6:48:09
[2024-03-20 18:36:44 pose_transfer_train.py:516] Train [1/100](840/6372)  Time 4.3092(4.4176)  Loss 0.0366(0.0359)  Lr 0.00008569  Eta 6:47:17
[2024-03-20 18:37:26 pose_transfer_train.py:516] Train [1/100](850/6372)  Time 3.7663(4.4152)  Loss 0.0145(0.0357)  Lr 0.00008659  Eta 6:46:20
[2024-03-20 18:38:11 pose_transfer_train.py:516] Train [1/100](860/6372)  Time 4.3852(4.4160)  Loss 0.0339(0.0358)  Lr 0.00008749  Eta 6:45:41
[2024-03-20 18:38:54 pose_transfer_train.py:516] Train [1/100](870/6372)  Time 4.2503(4.4149)  Loss 0.0581(0.0357)  Lr 0.00008839  Eta 6:44:50
[2024-03-20 18:39:38 pose_transfer_train.py:516] Train [1/100](880/6372)  Time 4.3206(4.4149)  Loss 0.0186(0.0357)  Lr 0.00008929  Eta 6:44:06
[2024-03-20 18:40:23 pose_transfer_train.py:516] Train [1/100](890/6372)  Time 4.5465(4.4155)  Loss 0.0198(0.0359)  Lr 0.00009019  Eta 6:43:25
[2024-03-20 18:41:07 pose_transfer_train.py:516] Train [1/100](900/6372)  Time 4.0770(4.4151)  Loss 0.0737(0.0361)  Lr 0.00009109  Eta 6:42:39
[2024-03-20 18:41:52 pose_transfer_train.py:516] Train [1/100](910/6372)  Time 4.3303(4.4157)  Loss 0.0352(0.0362)  Lr 0.00009199  Eta 6:41:58
[2024-03-20 18:42:35 pose_transfer_train.py:516] Train [1/100](920/6372)  Time 4.5471(4.4154)  Loss 0.0050(0.0361)  Lr 0.00009289  Eta 6:41:12
[2024-03-20 18:43:20 pose_transfer_train.py:516] Train [1/100](930/6372)  Time 4.7159(4.4156)  Loss 0.0402(0.0363)  Lr 0.00009379  Eta 6:40:29
[2024-03-20 18:44:05 pose_transfer_train.py:516] Train [1/100](940/6372)  Time 4.5924(4.4165)  Loss 0.0334(0.0363)  Lr 0.00009469  Eta 6:39:50
[2024-03-20 18:44:50 pose_transfer_train.py:516] Train [1/100](950/6372)  Time 4.3104(4.4175)  Loss 0.0202(0.0364)  Lr 0.00009559  Eta 6:39:11
[2024-03-20 18:45:34 pose_transfer_train.py:516] Train [1/100](960/6372)  Time 4.7725(4.4172)  Loss 0.0205(0.0364)  Lr 0.00009649  Eta 6:38:25
[2024-03-20 18:46:18 pose_transfer_train.py:516] Train [1/100](970/6372)  Time 4.2963(4.4169)  Loss 0.0347(0.0363)  Lr 0.00009739  Eta 6:37:40
[2024-03-20 18:47:01 pose_transfer_train.py:516] Train [1/100](980/6372)  Time 4.4462(4.4164)  Loss 0.0037(0.0361)  Lr 0.00009829  Eta 6:36:53
[2024-03-20 18:47:45 pose_transfer_train.py:516] Train [1/100](990/6372)  Time 4.4419(4.4156)  Loss 0.0048(0.0361)  Lr 0.00009919  Eta 6:36:04
[2024-03-20 18:48:28 pose_transfer_train.py:516] Train [1/100](1000/6372)  Time 4.3843(4.4148)  Loss 0.0286(0.0362)  Lr 0.00010000  Eta 6:35:16
[2024-03-20 18:49:13 pose_transfer_train.py:516] Train [1/100](1010/6372)  Time 4.8522(4.4152)  Loss 0.0305(0.0361)  Lr 0.00010000  Eta 6:34:34
[2024-03-20 18:49:56 pose_transfer_train.py:516] Train [1/100](1020/6372)  Time 4.0635(4.4148)  Loss 0.0168(0.0360)  Lr 0.00010000  Eta 6:33:47
[2024-03-20 18:50:40 pose_transfer_train.py:516] Train [1/100](1030/6372)  Time 4.5812(4.4140)  Loss 0.0181(0.0360)  Lr 0.00010000  Eta 6:32:59
[2024-03-20 18:51:24 pose_transfer_train.py:516] Train [1/100](1040/6372)  Time 4.6067(4.4141)  Loss 0.0434(0.0360)  Lr 0.00010000  Eta 6:32:15
[2024-03-20 18:52:08 pose_transfer_train.py:516] Train [1/100](1050/6372)  Time 4.6656(4.4141)  Loss 0.0199(0.0360)  Lr 0.00010000  Eta 6:31:31
[2024-03-20 18:52:52 pose_transfer_train.py:516] Train [1/100](1060/6372)  Time 4.7149(4.4144)  Loss 0.0337(0.0360)  Lr 0.00010000  Eta 6:30:49
[2024-03-20 18:53:35 pose_transfer_train.py:516] Train [1/100](1070/6372)  Time 4.2218(4.4124)  Loss 0.0611(0.0360)  Lr 0.00010000  Eta 6:29:54
[2024-03-20 18:54:20 pose_transfer_train.py:516] Train [1/100](1080/6372)  Time 4.6599(4.4132)  Loss 0.0385(0.0359)  Lr 0.00010000  Eta 6:29:14
[2024-03-20 18:55:04 pose_transfer_train.py:516] Train [1/100](1090/6372)  Time 4.5972(4.4136)  Loss 0.0269(0.0358)  Lr 0.00010000  Eta 6:28:32
[2024-03-20 18:55:47 pose_transfer_train.py:516] Train [1/100](1100/6372)  Time 4.4969(4.4127)  Loss 0.0930(0.0359)  Lr 0.00010000  Eta 6:27:43
[2024-03-20 18:56:31 pose_transfer_train.py:516] Train [1/100](1110/6372)  Time 4.4151(4.4124)  Loss 0.0599(0.0359)  Lr 0.00010000  Eta 6:26:57
[2024-03-20 18:57:15 pose_transfer_train.py:516] Train [1/100](1120/6372)  Time 5.0352(4.4125)  Loss 0.0115(0.0358)  Lr 0.00010000  Eta 6:26:14
[2024-03-20 18:58:01 pose_transfer_train.py:516] Train [1/100](1130/6372)  Time 4.6830(4.4138)  Loss 0.0564(0.0358)  Lr 0.00010000  Eta 6:25:36
[2024-03-20 18:58:45 pose_transfer_train.py:516] Train [1/100](1140/6372)  Time 4.3929(4.4142)  Loss 0.0076(0.0359)  Lr 0.00010000  Eta 6:24:55
[2024-03-20 18:59:28 pose_transfer_train.py:516] Train [1/100](1150/6372)  Time 3.9562(4.4125)  Loss 0.0288(0.0359)  Lr 0.00010000  Eta 6:24:02
[2024-03-20 19:00:12 pose_transfer_train.py:516] Train [1/100](1160/6372)  Time 4.4118(4.4125)  Loss 0.0083(0.0358)  Lr 0.00010000  Eta 6:23:17
[2024-03-20 19:00:57 pose_transfer_train.py:516] Train [1/100](1170/6372)  Time 4.7397(4.4136)  Loss 0.0082(0.0357)  Lr 0.00010000  Eta 6:22:39
[2024-03-20 19:01:38 pose_transfer_train.py:516] Train [1/100](1180/6372)  Time 4.5561(4.4109)  Loss 0.0262(0.0357)  Lr 0.00010000  Eta 6:21:41
[2024-03-20 19:02:21 pose_transfer_train.py:516] Train [1/100](1190/6372)  Time 4.5005(4.4103)  Loss 0.0625(0.0357)  Lr 0.00010000  Eta 6:20:54
[2024-03-20 19:03:05 pose_transfer_train.py:516] Train [1/100](1200/6372)  Time 4.1155(4.4099)  Loss 0.0189(0.0356)  Lr 0.00010000  Eta 6:20:08
[2024-03-20 19:03:48 pose_transfer_train.py:516] Train [1/100](1210/6372)  Time 3.9456(4.4086)  Loss 0.0465(0.0356)  Lr 0.00010000  Eta 6:19:17
[2024-03-20 19:04:30 pose_transfer_train.py:516] Train [1/100](1220/6372)  Time 3.8090(4.4070)  Loss 0.0659(0.0355)  Lr 0.00010000  Eta 6:18:25
[2024-03-20 19:05:14 pose_transfer_train.py:516] Train [1/100](1230/6372)  Time 3.9729(4.4072)  Loss 0.0537(0.0356)  Lr 0.00010000  Eta 6:17:41
[2024-03-20 19:05:58 pose_transfer_train.py:516] Train [1/100](1240/6372)  Time 4.4374(4.4067)  Loss 0.0159(0.0357)  Lr 0.00010000  Eta 6:16:55
[2024-03-20 19:06:42 pose_transfer_train.py:516] Train [1/100](1250/6372)  Time 4.3561(4.4066)  Loss 0.0340(0.0357)  Lr 0.00010000  Eta 6:16:10
[2024-03-20 19:07:25 pose_transfer_train.py:516] Train [1/100](1260/6372)  Time 4.2866(4.4060)  Loss 0.0176(0.0356)  Lr 0.00010000  Eta 6:15:23
[2024-03-20 19:08:09 pose_transfer_train.py:516] Train [1/100](1270/6372)  Time 4.1404(4.4060)  Loss 0.0801(0.0356)  Lr 0.00010000  Eta 6:14:39
[2024-03-20 19:08:53 pose_transfer_train.py:516] Train [1/100](1280/6372)  Time 4.0627(4.4061)  Loss 0.0037(0.0356)  Lr 0.00010000  Eta 6:13:56
[2024-03-20 19:09:37 pose_transfer_train.py:516] Train [1/100](1290/6372)  Time 4.2822(4.4057)  Loss 0.0259(0.0357)  Lr 0.00010000  Eta 6:13:09
[2024-03-20 19:10:20 pose_transfer_train.py:516] Train [1/100](1300/6372)  Time 4.4818(4.4051)  Loss 0.0536(0.0356)  Lr 0.00010000  Eta 6:12:22
[2024-03-20 19:11:04 pose_transfer_train.py:516] Train [1/100](1310/6372)  Time 4.1434(4.4051)  Loss 0.0501(0.0354)  Lr 0.00010000  Eta 6:11:38
[2024-03-20 19:11:49 pose_transfer_train.py:516] Train [1/100](1320/6372)  Time 4.7827(4.4058)  Loss 0.0151(0.0355)  Lr 0.00010000  Eta 6:10:57
[2024-03-20 19:12:32 pose_transfer_train.py:516] Train [1/100](1330/6372)  Time 4.1413(4.4053)  Loss 0.0249(0.0356)  Lr 0.00010000  Eta 6:10:11
[2024-03-20 19:13:16 pose_transfer_train.py:516] Train [1/100](1340/6372)  Time 4.1710(4.4049)  Loss 0.0025(0.0356)  Lr 0.00010000  Eta 6:09:25
[2024-03-20 19:13:59 pose_transfer_train.py:516] Train [1/100](1350/6372)  Time 4.3067(4.4043)  Loss 0.0884(0.0356)  Lr 0.00010000  Eta 6:08:38
[2024-03-20 19:14:43 pose_transfer_train.py:516] Train [1/100](1360/6372)  Time 4.1195(4.4045)  Loss 0.0070(0.0357)  Lr 0.00010000  Eta 6:07:55
[2024-03-20 19:15:28 pose_transfer_train.py:516] Train [1/100](1370/6372)  Time 4.4652(4.4047)  Loss 0.0271(0.0357)  Lr 0.00010000  Eta 6:07:12
[2024-03-20 19:16:11 pose_transfer_train.py:516] Train [1/100](1380/6372)  Time 4.1029(4.4044)  Loss 0.0509(0.0357)  Lr 0.00010000  Eta 6:06:26
[2024-03-20 19:16:56 pose_transfer_train.py:516] Train [1/100](1390/6372)  Time 4.4993(4.4045)  Loss 0.0387(0.0357)  Lr 0.00010000  Eta 6:05:43
[2024-03-20 19:17:40 pose_transfer_train.py:516] Train [1/100](1400/6372)  Time 4.9502(4.4045)  Loss 0.0051(0.0356)  Lr 0.00010000  Eta 6:04:59
[2024-03-20 19:18:24 pose_transfer_train.py:516] Train [1/100](1410/6372)  Time 4.4669(4.4044)  Loss 0.0364(0.0357)  Lr 0.00010000  Eta 6:04:14
[2024-03-20 19:19:06 pose_transfer_train.py:516] Train [1/100](1420/6372)  Time 3.9084(4.4032)  Loss 0.0132(0.0357)  Lr 0.00010000  Eta 6:03:24
[2024-03-20 19:19:48 pose_transfer_train.py:516] Train [1/100](1430/6372)  Time 4.3658(4.4023)  Loss 0.0468(0.0357)  Lr 0.00010000  Eta 6:02:35
[2024-03-20 19:20:34 pose_transfer_train.py:516] Train [1/100](1440/6372)  Time 4.5119(4.4035)  Loss 0.0169(0.0357)  Lr 0.00010000  Eta 6:01:57
[2024-03-20 19:21:18 pose_transfer_train.py:516] Train [1/100](1450/6372)  Time 4.1657(4.4031)  Loss 0.0464(0.0359)  Lr 0.00010000  Eta 6:01:11
[2024-03-20 19:22:00 pose_transfer_train.py:516] Train [1/100](1460/6372)  Time 3.6786(4.4021)  Loss 0.0186(0.0358)  Lr 0.00010000  Eta 6:00:22
[2024-03-20 19:22:44 pose_transfer_train.py:516] Train [1/100](1470/6372)  Time 5.0456(4.4021)  Loss 0.0253(0.0358)  Lr 0.00010000  Eta 5:59:38
[2024-03-20 19:23:29 pose_transfer_train.py:516] Train [1/100](1480/6372)  Time 4.6380(4.4026)  Loss 0.0459(0.0357)  Lr 0.00010000  Eta 5:58:57
[2024-03-20 19:24:13 pose_transfer_train.py:516] Train [1/100](1490/6372)  Time 4.7329(4.4025)  Loss 0.0301(0.0358)  Lr 0.00010000  Eta 5:58:12
[2024-03-20 19:24:58 pose_transfer_train.py:516] Train [1/100](1500/6372)  Time 4.4080(4.4028)  Loss 0.0386(0.0358)  Lr 0.00010000  Eta 5:57:30
[2024-03-20 19:25:40 pose_transfer_train.py:516] Train [1/100](1510/6372)  Time 4.2660(4.4018)  Loss 0.0829(0.0359)  Lr 0.00010000  Eta 5:56:41
[2024-03-20 19:26:23 pose_transfer_train.py:516] Train [1/100](1520/6372)  Time 4.2975(4.4014)  Loss 0.0397(0.0358)  Lr 0.00010000  Eta 5:55:55
[2024-03-20 19:27:06 pose_transfer_train.py:516] Train [1/100](1530/6372)  Time 4.5473(4.4005)  Loss 0.0646(0.0359)  Lr 0.00010000  Eta 5:55:07
[2024-03-20 19:27:50 pose_transfer_train.py:516] Train [1/100](1540/6372)  Time 4.5257(4.4007)  Loss 0.0168(0.0358)  Lr 0.00010000  Eta 5:54:24
[2024-03-20 19:28:33 pose_transfer_train.py:516] Train [1/100](1550/6372)  Time 4.5658(4.4000)  Loss 0.0048(0.0357)  Lr 0.00010000  Eta 5:53:36
[2024-03-20 19:29:18 pose_transfer_train.py:516] Train [1/100](1560/6372)  Time 4.3608(4.4003)  Loss 0.0483(0.0358)  Lr 0.00010000  Eta 5:52:54
[2024-03-20 19:30:00 pose_transfer_train.py:516] Train [1/100](1570/6372)  Time 4.2537(4.3994)  Loss 0.0346(0.0359)  Lr 0.00010000  Eta 5:52:06
[2024-03-20 19:30:45 pose_transfer_train.py:516] Train [1/100](1580/6372)  Time 4.5930(4.3999)  Loss 0.0236(0.0359)  Lr 0.00010000  Eta 5:51:24
[2024-03-20 19:31:27 pose_transfer_train.py:516] Train [1/100](1590/6372)  Time 3.5641(4.3985)  Loss 0.0091(0.0358)  Lr 0.00010000  Eta 5:50:33
[2024-03-20 19:32:10 pose_transfer_train.py:516] Train [1/100](1600/6372)  Time 4.3783(4.3977)  Loss 0.0406(0.0357)  Lr 0.00010000  Eta 5:49:45
[2024-03-20 19:32:55 pose_transfer_train.py:516] Train [1/100](1610/6372)  Time 4.4419(4.3983)  Loss 0.0318(0.0357)  Lr 0.00010000  Eta 5:49:04
[2024-03-20 19:33:39 pose_transfer_train.py:516] Train [1/100](1620/6372)  Time 4.4172(4.3983)  Loss 0.0084(0.0356)  Lr 0.00010000  Eta 5:48:20
[2024-03-20 19:34:22 pose_transfer_train.py:516] Train [1/100](1630/6372)  Time 4.4480(4.3979)  Loss 0.0239(0.0356)  Lr 0.00010000  Eta 5:47:34
[2024-03-20 19:35:05 pose_transfer_train.py:516] Train [1/100](1640/6372)  Time 3.9672(4.3973)  Loss 0.0023(0.0355)  Lr 0.00010000  Eta 5:46:48
[2024-03-20 19:35:47 pose_transfer_train.py:516] Train [1/100](1650/6372)  Time 4.2982(4.3964)  Loss 0.0452(0.0355)  Lr 0.00010000  Eta 5:45:59
[2024-03-20 19:36:31 pose_transfer_train.py:516] Train [1/100](1660/6372)  Time 4.3551(4.3965)  Loss 0.0087(0.0354)  Lr 0.00010000  Eta 5:45:16
[2024-03-20 19:37:15 pose_transfer_train.py:516] Train [1/100](1670/6372)  Time 4.7851(4.3964)  Loss 0.0057(0.0353)  Lr 0.00010000  Eta 5:44:32
[2024-03-20 19:38:00 pose_transfer_train.py:516] Train [1/100](1680/6372)  Time 4.2508(4.3969)  Loss 0.0061(0.0353)  Lr 0.00010000  Eta 5:43:50
[2024-03-20 19:38:42 pose_transfer_train.py:516] Train [1/100](1690/6372)  Time 3.9999(4.3959)  Loss 0.0751(0.0353)  Lr 0.00010000  Eta 5:43:01
[2024-03-20 19:39:27 pose_transfer_train.py:516] Train [1/100](1700/6372)  Time 4.7009(4.3963)  Loss 0.0283(0.0353)  Lr 0.00010000  Eta 5:42:19
[2024-03-20 19:40:10 pose_transfer_train.py:516] Train [1/100](1710/6372)  Time 4.5456(4.3956)  Loss 0.0730(0.0353)  Lr 0.00010000  Eta 5:41:32
[2024-03-20 19:40:53 pose_transfer_train.py:516] Train [1/100](1720/6372)  Time 4.1382(4.3955)  Loss 0.0188(0.0353)  Lr 0.00010000  Eta 5:40:47
[2024-03-20 19:41:37 pose_transfer_train.py:516] Train [1/100](1730/6372)  Time 4.2782(4.3953)  Loss 0.0593(0.0354)  Lr 0.00010000  Eta 5:40:02
[2024-03-20 19:42:21 pose_transfer_train.py:516] Train [1/100](1740/6372)  Time 4.7024(4.3955)  Loss 0.0351(0.0354)  Lr 0.00010000  Eta 5:39:20
[2024-03-20 19:43:06 pose_transfer_train.py:516] Train [1/100](1750/6372)  Time 4.3296(4.3956)  Loss 0.0083(0.0354)  Lr 0.00010000  Eta 5:38:36
[2024-03-20 19:43:50 pose_transfer_train.py:516] Train [1/100](1760/6372)  Time 4.1919(4.3956)  Loss 0.0265(0.0353)  Lr 0.00010000  Eta 5:37:52
[2024-03-20 19:44:33 pose_transfer_train.py:516] Train [1/100](1770/6372)  Time 4.0717(4.3953)  Loss 0.0290(0.0353)  Lr 0.00010000  Eta 5:37:07
[2024-03-20 19:45:16 pose_transfer_train.py:516] Train [1/100](1780/6372)  Time 4.8245(4.3947)  Loss 0.0154(0.0352)  Lr 0.00010000  Eta 5:36:20
[2024-03-20 19:45:59 pose_transfer_train.py:516] Train [1/100](1790/6372)  Time 3.7797(4.3944)  Loss 0.0090(0.0352)  Lr 0.00010000  Eta 5:35:35
[2024-03-20 19:46:41 pose_transfer_train.py:516] Train [1/100](1800/6372)  Time 4.0432(4.3931)  Loss 0.0222(0.0352)  Lr 0.00010000  Eta 5:34:45
[2024-03-20 19:47:23 pose_transfer_train.py:516] Train [1/100](1810/6372)  Time 4.5549(4.3923)  Loss 0.0063(0.0352)  Lr 0.00010000  Eta 5:33:57
[2024-03-20 19:48:08 pose_transfer_train.py:516] Train [1/100](1820/6372)  Time 4.1465(4.3925)  Loss 0.0856(0.0353)  Lr 0.00010000  Eta 5:33:14
[2024-03-20 19:48:51 pose_transfer_train.py:516] Train [1/100](1830/6372)  Time 4.0629(4.3922)  Loss 0.0155(0.0353)  Lr 0.00010000  Eta 5:32:29
[2024-03-20 19:49:35 pose_transfer_train.py:516] Train [1/100](1840/6372)  Time 4.5816(4.3923)  Loss 0.0191(0.0353)  Lr 0.00010000  Eta 5:31:45
[2024-03-20 19:50:20 pose_transfer_train.py:516] Train [1/100](1850/6372)  Time 4.2073(4.3926)  Loss 0.0668(0.0353)  Lr 0.00010000  Eta 5:31:03
[2024-03-20 19:51:04 pose_transfer_train.py:516] Train [1/100](1860/6372)  Time 4.6912(4.3928)  Loss 0.0188(0.0353)  Lr 0.00010000  Eta 5:30:20
[2024-03-20 19:51:47 pose_transfer_train.py:516] Train [1/100](1870/6372)  Time 4.1760(4.3924)  Loss 0.0419(0.0353)  Lr 0.00010000  Eta 5:29:34
[2024-03-20 19:52:31 pose_transfer_train.py:516] Train [1/100](1880/6372)  Time 4.7626(4.3925)  Loss 0.0745(0.0353)  Lr 0.00010000  Eta 5:28:51
[2024-03-20 19:53:14 pose_transfer_train.py:516] Train [1/100](1890/6372)  Time 4.3273(4.3918)  Loss 0.0206(0.0353)  Lr 0.00010000  Eta 5:28:03
[2024-03-20 19:53:57 pose_transfer_train.py:516] Train [1/100](1900/6372)  Time 4.3920(4.3913)  Loss 0.0502(0.0354)  Lr 0.00010000  Eta 5:27:17
[2024-03-20 19:54:40 pose_transfer_train.py:516] Train [1/100](1910/6372)  Time 4.0635(4.3908)  Loss 0.0109(0.0353)  Lr 0.00010000  Eta 5:26:31
[2024-03-20 19:55:22 pose_transfer_train.py:516] Train [1/100](1920/6372)  Time 4.0447(4.3899)  Loss 0.0296(0.0353)  Lr 0.00010000  Eta 5:25:43
[2024-03-20 19:56:05 pose_transfer_train.py:516] Train [1/100](1930/6372)  Time 4.4399(4.3896)  Loss 0.0452(0.0353)  Lr 0.00010000  Eta 5:24:58
[2024-03-20 19:56:49 pose_transfer_train.py:516] Train [1/100](1940/6372)  Time 4.4036(4.3895)  Loss 0.0574(0.0353)  Lr 0.00010000  Eta 5:24:14
[2024-03-20 19:57:33 pose_transfer_train.py:516] Train [1/100](1950/6372)  Time 4.2261(4.3898)  Loss 0.0279(0.0353)  Lr 0.00010000  Eta 5:23:31
[2024-03-20 19:58:17 pose_transfer_train.py:516] Train [1/100](1960/6372)  Time 4.5521(4.3898)  Loss 0.0456(0.0352)  Lr 0.00010000  Eta 5:22:47
[2024-03-20 19:58:59 pose_transfer_train.py:516] Train [1/100](1970/6372)  Time 4.6458(4.3886)  Loss 0.0023(0.0352)  Lr 0.00010000  Eta 5:21:58
[2024-03-20 19:59:43 pose_transfer_train.py:516] Train [1/100](1980/6372)  Time 4.0088(4.3887)  Loss 0.0277(0.0352)  Lr 0.00010000  Eta 5:21:15
[2024-03-20 20:00:29 pose_transfer_train.py:516] Train [1/100](1990/6372)  Time 4.2725(4.3896)  Loss 0.0064(0.0351)  Lr 0.00010000  Eta 5:20:35
[2024-03-20 20:01:12 pose_transfer_train.py:516] Train [1/100](2000/6372)  Time 4.2666(4.3896)  Loss 0.0612(0.0351)  Lr 0.00010000  Eta 5:19:51
[2024-03-20 20:01:57 pose_transfer_train.py:516] Train [1/100](2010/6372)  Time 4.6085(4.3899)  Loss 0.0049(0.0351)  Lr 0.00010000  Eta 5:19:08
[2024-03-20 20:02:39 pose_transfer_train.py:516] Train [1/100](2020/6372)  Time 4.1338(4.3892)  Loss 0.1075(0.0351)  Lr 0.00010000  Eta 5:18:21
[2024-03-20 20:03:24 pose_transfer_train.py:516] Train [1/100](2030/6372)  Time 4.7165(4.3893)  Loss 0.0090(0.0351)  Lr 0.00010000  Eta 5:17:38
[2024-03-20 20:04:06 pose_transfer_train.py:516] Train [1/100](2040/6372)  Time 4.3208(4.3886)  Loss 0.0327(0.0350)  Lr 0.00010000  Eta 5:16:51
[2024-03-20 20:04:49 pose_transfer_train.py:516] Train [1/100](2050/6372)  Time 4.3036(4.3880)  Loss 0.0918(0.0350)  Lr 0.00010000  Eta 5:16:04
[2024-03-20 20:05:32 pose_transfer_train.py:516] Train [1/100](2060/6372)  Time 4.2441(4.3876)  Loss 0.0049(0.0350)  Lr 0.00010000  Eta 5:15:19
[2024-03-20 20:06:16 pose_transfer_train.py:516] Train [1/100](2070/6372)  Time 4.5435(4.3876)  Loss 0.0139(0.0349)  Lr 0.00010000  Eta 5:14:35
[2024-03-20 20:06:59 pose_transfer_train.py:516] Train [1/100](2080/6372)  Time 3.9552(4.3875)  Loss 0.0024(0.0349)  Lr 0.00010000  Eta 5:13:51
[2024-03-20 20:07:43 pose_transfer_train.py:516] Train [1/100](2090/6372)  Time 4.7087(4.3876)  Loss 0.0630(0.0349)  Lr 0.00010000  Eta 5:13:07
[2024-03-20 20:08:27 pose_transfer_train.py:516] Train [1/100](2100/6372)  Time 4.4124(4.3877)  Loss 0.0064(0.0349)  Lr 0.00010000  Eta 5:12:24
[2024-03-20 20:09:11 pose_transfer_train.py:516] Train [1/100](2110/6372)  Time 4.8169(4.3876)  Loss 0.0087(0.0350)  Lr 0.00010000  Eta 5:11:39
[2024-03-20 20:09:55 pose_transfer_train.py:516] Train [1/100](2120/6372)  Time 4.3203(4.3876)  Loss 0.0541(0.0350)  Lr 0.00010000  Eta 5:10:56
[2024-03-20 20:10:39 pose_transfer_train.py:516] Train [1/100](2130/6372)  Time 4.0724(4.3876)  Loss 0.0109(0.0350)  Lr 0.00010000  Eta 5:10:12
[2024-03-20 20:11:22 pose_transfer_train.py:516] Train [1/100](2140/6372)  Time 3.7793(4.3873)  Loss 0.0151(0.0349)  Lr 0.00010000  Eta 5:09:26
[2024-03-20 20:12:05 pose_transfer_train.py:516] Train [1/100](2150/6372)  Time 4.5295(4.3871)  Loss 0.0222(0.0349)  Lr 0.00010000  Eta 5:08:42
[2024-03-20 20:12:50 pose_transfer_train.py:516] Train [1/100](2160/6372)  Time 4.4411(4.3874)  Loss 0.0691(0.0349)  Lr 0.00010000  Eta 5:07:59
[2024-03-20 20:13:34 pose_transfer_train.py:516] Train [1/100](2170/6372)  Time 4.5833(4.3874)  Loss 0.0640(0.0349)  Lr 0.00010000  Eta 5:07:15
[2024-03-20 20:14:20 pose_transfer_train.py:516] Train [1/100](2180/6372)  Time 4.7755(4.3885)  Loss 0.0548(0.0349)  Lr 0.00010000  Eta 5:06:36
[2024-03-20 20:15:03 pose_transfer_train.py:516] Train [1/100](2190/6372)  Time 4.0061(4.3878)  Loss 0.0117(0.0348)  Lr 0.00010000  Eta 5:05:49
[2024-03-20 20:15:46 pose_transfer_train.py:516] Train [1/100](2200/6372)  Time 4.6777(4.3878)  Loss 0.0301(0.0348)  Lr 0.00010000  Eta 5:05:05
[2024-03-20 20:16:30 pose_transfer_train.py:516] Train [1/100](2210/6372)  Time 4.3328(4.3878)  Loss 0.0381(0.0348)  Lr 0.00010000  Eta 5:04:22
[2024-03-20 20:17:15 pose_transfer_train.py:516] Train [1/100](2220/6372)  Time 3.6418(4.3880)  Loss 0.0353(0.0348)  Lr 0.00010000  Eta 5:03:39
[2024-03-20 20:18:00 pose_transfer_train.py:516] Train [1/100](2230/6372)  Time 4.6305(4.3885)  Loss 0.0092(0.0347)  Lr 0.00010000  Eta 5:02:57
[2024-03-20 20:18:43 pose_transfer_train.py:516] Train [1/100](2240/6372)  Time 3.7788(4.3885)  Loss 0.0506(0.0347)  Lr 0.00010000  Eta 5:02:13
[2024-03-20 20:19:27 pose_transfer_train.py:516] Train [1/100](2250/6372)  Time 4.1071(4.3884)  Loss 0.0057(0.0347)  Lr 0.00010000  Eta 5:01:28
[2024-03-20 20:20:12 pose_transfer_train.py:516] Train [1/100](2260/6372)  Time 4.4286(4.3886)  Loss 0.0442(0.0346)  Lr 0.00010000  Eta 5:00:46
[2024-03-20 20:20:55 pose_transfer_train.py:516] Train [1/100](2270/6372)  Time 3.9602(4.3884)  Loss 0.0602(0.0347)  Lr 0.00010000  Eta 5:00:01
[2024-03-20 20:21:37 pose_transfer_train.py:516] Train [1/100](2280/6372)  Time 4.1414(4.3874)  Loss 0.0349(0.0347)  Lr 0.00010000  Eta 4:59:13
[2024-03-20 20:22:21 pose_transfer_train.py:516] Train [1/100](2290/6372)  Time 4.8718(4.3875)  Loss 0.0325(0.0347)  Lr 0.00010000  Eta 4:58:29
[2024-03-20 20:23:06 pose_transfer_train.py:516] Train [1/100](2300/6372)  Time 4.4609(4.3880)  Loss 0.0024(0.0347)  Lr 0.00010000  Eta 4:57:47
[2024-03-20 20:23:47 pose_transfer_train.py:516] Train [1/100](2310/6372)  Time 4.5063(4.3869)  Loss 0.0177(0.0346)  Lr 0.00010000  Eta 4:56:59
[2024-03-20 20:24:31 pose_transfer_train.py:516] Train [1/100](2320/6372)  Time 4.4214(4.3869)  Loss 0.0259(0.0346)  Lr 0.00010000  Eta 4:56:15
[2024-03-20 20:25:14 pose_transfer_train.py:516] Train [1/100](2330/6372)  Time 4.8143(4.3867)  Loss 0.0649(0.0346)  Lr 0.00010000  Eta 4:55:31
[2024-03-20 20:25:58 pose_transfer_train.py:516] Train [1/100](2340/6372)  Time 4.2584(4.3865)  Loss 0.0262(0.0345)  Lr 0.00010000  Eta 4:54:46
[2024-03-20 20:26:40 pose_transfer_train.py:516] Train [1/100](2350/6372)  Time 4.1880(4.3860)  Loss 0.0453(0.0345)  Lr 0.00010000  Eta 4:54:00
[2024-03-20 20:27:23 pose_transfer_train.py:516] Train [1/100](2360/6372)  Time 3.9537(4.3856)  Loss 0.0119(0.0346)  Lr 0.00010000  Eta 4:53:14
[2024-03-20 20:28:07 pose_transfer_train.py:516] Train [1/100](2370/6372)  Time 4.3988(4.3855)  Loss 0.0328(0.0346)  Lr 0.00010000  Eta 4:52:30
[2024-03-20 20:28:51 pose_transfer_train.py:516] Train [1/100](2380/6372)  Time 4.1894(4.3854)  Loss 0.0052(0.0346)  Lr 0.00010000  Eta 4:51:46
[2024-03-20 20:29:33 pose_transfer_train.py:516] Train [1/100](2390/6372)  Time 4.2647(4.3848)  Loss 0.0030(0.0346)  Lr 0.00010000  Eta 4:51:00
[2024-03-20 20:30:17 pose_transfer_train.py:516] Train [1/100](2400/6372)  Time 4.2284(4.3848)  Loss 0.0307(0.0345)  Lr 0.00010000  Eta 4:50:16
[2024-03-20 20:30:59 pose_transfer_train.py:516] Train [1/100](2410/6372)  Time 4.5261(4.3843)  Loss 0.0509(0.0345)  Lr 0.00010000  Eta 4:49:30
[2024-03-20 20:31:44 pose_transfer_train.py:516] Train [1/100](2420/6372)  Time 4.6213(4.3845)  Loss 0.0485(0.0345)  Lr 0.00010000  Eta 4:48:47
[2024-03-20 20:32:26 pose_transfer_train.py:516] Train [1/100](2430/6372)  Time 4.0054(4.3840)  Loss 0.0068(0.0345)  Lr 0.00010000  Eta 4:48:01
[2024-03-20 20:33:08 pose_transfer_train.py:516] Train [1/100](2440/6372)  Time 4.2919(4.3831)  Loss 0.0238(0.0344)  Lr 0.00010000  Eta 4:47:14
[2024-03-20 20:33:52 pose_transfer_train.py:516] Train [1/100](2450/6372)  Time 4.2560(4.3831)  Loss 0.0601(0.0345)  Lr 0.00010000  Eta 4:46:30
[2024-03-20 20:34:37 pose_transfer_train.py:516] Train [1/100](2460/6372)  Time 4.3787(4.3836)  Loss 0.0093(0.0346)  Lr 0.00010000  Eta 4:45:48
[2024-03-20 20:35:20 pose_transfer_train.py:516] Train [1/100](2470/6372)  Time 4.0330(4.3834)  Loss 0.0488(0.0346)  Lr 0.00010000  Eta 4:45:03
[2024-03-20 20:36:04 pose_transfer_train.py:516] Train [1/100](2480/6372)  Time 4.3884(4.3833)  Loss 0.0215(0.0345)  Lr 0.00010000  Eta 4:44:19
[2024-03-20 20:36:48 pose_transfer_train.py:516] Train [1/100](2490/6372)  Time 5.1592(4.3835)  Loss 0.0199(0.0345)  Lr 0.00010000  Eta 4:43:36
[2024-03-20 20:37:33 pose_transfer_train.py:516] Train [1/100](2500/6372)  Time 4.3413(4.3840)  Loss 0.1096(0.0345)  Lr 0.00010000  Eta 4:42:54
[2024-03-20 20:38:16 pose_transfer_train.py:516] Train [1/100](2510/6372)  Time 4.0536(4.3835)  Loss 0.2149(0.0346)  Lr 0.00010000  Eta 4:42:09
[2024-03-20 20:38:59 pose_transfer_train.py:516] Train [1/100](2520/6372)  Time 4.3963(4.3833)  Loss 0.0444(0.0346)  Lr 0.00010000  Eta 4:41:24
[2024-03-20 20:39:42 pose_transfer_train.py:516] Train [1/100](2530/6372)  Time 4.2900(4.3831)  Loss 0.0079(0.0345)  Lr 0.00010000  Eta 4:40:39
[2024-03-20 20:40:26 pose_transfer_train.py:516] Train [1/100](2540/6372)  Time 4.4231(4.3831)  Loss 0.0826(0.0345)  Lr 0.00010000  Eta 4:39:55
[2024-03-20 20:41:10 pose_transfer_train.py:516] Train [1/100](2550/6372)  Time 4.6405(4.3830)  Loss 0.0389(0.0345)  Lr 0.00010000  Eta 4:39:12
[2024-03-20 20:41:53 pose_transfer_train.py:516] Train [1/100](2560/6372)  Time 4.3478(4.3827)  Loss 0.0206(0.0345)  Lr 0.00010000  Eta 4:38:27
[2024-03-20 20:42:37 pose_transfer_train.py:516] Train [1/100](2570/6372)  Time 4.6873(4.3830)  Loss 0.0090(0.0345)  Lr 0.00010000  Eta 4:37:44
[2024-03-20 20:43:20 pose_transfer_train.py:516] Train [1/100](2580/6372)  Time 3.9096(4.3826)  Loss 0.0585(0.0345)  Lr 0.00010000  Eta 4:36:58
[2024-03-20 20:44:04 pose_transfer_train.py:516] Train [1/100](2590/6372)  Time 4.5681(4.3826)  Loss 0.0533(0.0345)  Lr 0.00010000  Eta 4:36:14
[2024-03-20 20:44:49 pose_transfer_train.py:516] Train [1/100](2600/6372)  Time 4.7494(4.3829)  Loss 0.0126(0.0345)  Lr 0.00010000  Eta 4:35:32
[2024-03-20 20:45:34 pose_transfer_train.py:516] Train [1/100](2610/6372)  Time 4.1684(4.3835)  Loss 0.1607(0.0346)  Lr 0.00010000  Eta 4:34:50
[2024-03-20 20:46:16 pose_transfer_train.py:516] Train [1/100](2620/6372)  Time 4.5485(4.3829)  Loss 0.0328(0.0346)  Lr 0.00010000  Eta 4:34:04
[2024-03-20 20:47:00 pose_transfer_train.py:516] Train [1/100](2630/6372)  Time 4.7189(4.3829)  Loss 0.0224(0.0346)  Lr 0.00010000  Eta 4:33:20
[2024-03-20 20:47:44 pose_transfer_train.py:516] Train [1/100](2640/6372)  Time 4.6358(4.3830)  Loss 0.0448(0.0346)  Lr 0.00010000  Eta 4:32:37
[2024-03-20 20:48:27 pose_transfer_train.py:516] Train [1/100](2650/6372)  Time 3.8872(4.3825)  Loss 0.0149(0.0346)  Lr 0.00010000  Eta 4:31:51
[2024-03-20 20:49:10 pose_transfer_train.py:516] Train [1/100](2660/6372)  Time 4.5209(4.3822)  Loss 0.0088(0.0346)  Lr 0.00010000  Eta 4:31:06
[2024-03-20 20:49:55 pose_transfer_train.py:516] Train [1/100](2670/6372)  Time 4.6120(4.3825)  Loss 0.1218(0.0346)  Lr 0.00010000  Eta 4:30:24
[2024-03-20 20:50:38 pose_transfer_train.py:516] Train [1/100](2680/6372)  Time 4.1654(4.3822)  Loss 0.0152(0.0346)  Lr 0.00010000  Eta 4:29:39
[2024-03-20 20:51:21 pose_transfer_train.py:516] Train [1/100](2690/6372)  Time 4.2956(4.3821)  Loss 0.0333(0.0346)  Lr 0.00010000  Eta 4:28:54
[2024-03-20 20:52:03 pose_transfer_train.py:516] Train [1/100](2700/6372)  Time 4.5528(4.3815)  Loss 0.0086(0.0346)  Lr 0.00010000  Eta 4:28:08
[2024-03-20 20:52:46 pose_transfer_train.py:516] Train [1/100](2710/6372)  Time 4.5456(4.3810)  Loss 0.0269(0.0346)  Lr 0.00010000  Eta 4:27:23
[2024-03-20 20:53:30 pose_transfer_train.py:516] Train [1/100](2720/6372)  Time 4.6966(4.3812)  Loss 0.0350(0.0346)  Lr 0.00010000  Eta 4:26:40
[2024-03-20 20:54:13 pose_transfer_train.py:516] Train [1/100](2730/6372)  Time 4.5150(4.3810)  Loss 0.0489(0.0346)  Lr 0.00010000  Eta 4:25:55
[2024-03-20 20:54:56 pose_transfer_train.py:516] Train [1/100](2740/6372)  Time 4.6658(4.3807)  Loss 0.0540(0.0346)  Lr 0.00010000  Eta 4:25:10
[2024-03-20 20:55:40 pose_transfer_train.py:516] Train [1/100](2750/6372)  Time 4.1006(4.3806)  Loss 0.0269(0.0346)  Lr 0.00010000  Eta 4:24:26
[2024-03-20 20:56:23 pose_transfer_train.py:516] Train [1/100](2760/6372)  Time 4.5915(4.3804)  Loss 0.0261(0.0346)  Lr 0.00010000  Eta 4:23:42
[2024-03-20 20:57:09 pose_transfer_train.py:516] Train [1/100](2770/6372)  Time 4.5356(4.3811)  Loss 0.0342(0.0346)  Lr 0.00010000  Eta 4:23:00
[2024-03-20 20:57:54 pose_transfer_train.py:516] Train [1/100](2780/6372)  Time 4.3684(4.3817)  Loss 0.0082(0.0346)  Lr 0.00010000  Eta 4:22:18
[2024-03-20 20:58:37 pose_transfer_train.py:516] Train [1/100](2790/6372)  Time 3.8245(4.3814)  Loss 0.0229(0.0346)  Lr 0.00010000  Eta 4:21:34
[2024-03-20 20:59:23 pose_transfer_train.py:516] Train [1/100](2800/6372)  Time 4.7630(4.3820)  Loss 0.0430(0.0346)  Lr 0.00010000  Eta 4:20:52
[2024-03-20 21:00:10 pose_transfer_train.py:516] Train [1/100](2810/6372)  Time 4.5225(4.3831)  Loss 0.0021(0.0345)  Lr 0.00010000  Eta 4:20:12
[2024-03-20 21:00:54 pose_transfer_train.py:516] Train [1/100](2820/6372)  Time 4.5376(4.3833)  Loss 0.0016(0.0345)  Lr 0.00010000  Eta 4:19:29
[2024-03-20 21:01:37 pose_transfer_train.py:516] Train [1/100](2830/6372)  Time 4.3491(4.3830)  Loss 0.0501(0.0345)  Lr 0.00010000  Eta 4:18:44
[2024-03-20 21:02:21 pose_transfer_train.py:516] Train [1/100](2840/6372)  Time 4.4263(4.3830)  Loss 0.0098(0.0345)  Lr 0.00010000  Eta 4:18:00
[2024-03-20 21:03:06 pose_transfer_train.py:516] Train [1/100](2850/6372)  Time 4.5674(4.3834)  Loss 0.0097(0.0345)  Lr 0.00010000  Eta 4:17:18
[2024-03-20 21:03:49 pose_transfer_train.py:516] Train [1/100](2860/6372)  Time 4.5057(4.3833)  Loss 0.0122(0.0345)  Lr 0.00010000  Eta 4:16:34
[2024-03-20 21:04:33 pose_transfer_train.py:516] Train [1/100](2870/6372)  Time 4.0971(4.3831)  Loss 0.0287(0.0345)  Lr 0.00010000  Eta 4:15:49
[2024-03-20 21:05:18 pose_transfer_train.py:516] Train [1/100](2880/6372)  Time 4.1624(4.3836)  Loss 0.0123(0.0345)  Lr 0.00010000  Eta 4:15:07
[2024-03-20 21:06:00 pose_transfer_train.py:516] Train [1/100](2890/6372)  Time 4.4962(4.3831)  Loss 0.0028(0.0344)  Lr 0.00010000  Eta 4:14:21
[2024-03-20 21:06:44 pose_transfer_train.py:516] Train [1/100](2900/6372)  Time 4.5442(4.3832)  Loss 0.0313(0.0344)  Lr 0.00010000  Eta 4:13:38
[2024-03-20 21:07:27 pose_transfer_train.py:516] Train [1/100](2910/6372)  Time 4.4467(4.3826)  Loss 0.0074(0.0343)  Lr 0.00010000  Eta 4:12:52
[2024-03-20 21:08:10 pose_transfer_train.py:516] Train [1/100](2920/6372)  Time 3.9969(4.3824)  Loss 0.0389(0.0343)  Lr 0.00010000  Eta 4:12:07
[2024-03-20 21:08:53 pose_transfer_train.py:516] Train [1/100](2930/6372)  Time 3.9004(4.3820)  Loss 0.0070(0.0343)  Lr 0.00010000  Eta 4:11:22
[2024-03-20 21:09:35 pose_transfer_train.py:516] Train [1/100](2940/6372)  Time 4.3915(4.3815)  Loss 0.0049(0.0342)  Lr 0.00010000  Eta 4:10:37
[2024-03-20 21:10:19 pose_transfer_train.py:516] Train [1/100](2950/6372)  Time 4.3959(4.3817)  Loss 0.0248(0.0343)  Lr 0.00010000  Eta 4:09:54
[2024-03-20 21:11:02 pose_transfer_train.py:516] Train [1/100](2960/6372)  Time 4.5352(4.3814)  Loss 0.0104(0.0343)  Lr 0.00010000  Eta 4:09:09
[2024-03-20 21:11:47 pose_transfer_train.py:516] Train [1/100](2970/6372)  Time 4.3765(4.3817)  Loss 0.0651(0.0342)  Lr 0.00010000  Eta 4:08:26
[2024-03-20 21:12:30 pose_transfer_train.py:516] Train [1/100](2980/6372)  Time 4.6580(4.3814)  Loss 0.0663(0.0342)  Lr 0.00010000  Eta 4:07:41
[2024-03-20 21:13:13 pose_transfer_train.py:516] Train [1/100](2990/6372)  Time 4.5547(4.3812)  Loss 0.0124(0.0342)  Lr 0.00010000  Eta 4:06:57
[2024-03-20 21:13:58 pose_transfer_train.py:516] Train [1/100](3000/6372)  Time 4.4070(4.3816)  Loss 0.1190(0.0342)  Lr 0.00010000  Eta 4:06:14
[2024-03-20 21:14:41 pose_transfer_train.py:516] Train [1/100](3010/6372)  Time 4.6171(4.3812)  Loss 0.0317(0.0342)  Lr 0.00010000  Eta 4:05:29
[2024-03-20 21:15:24 pose_transfer_train.py:516] Train [1/100](3020/6372)  Time 4.1400(4.3812)  Loss 0.0242(0.0342)  Lr 0.00010000  Eta 4:04:45
[2024-03-20 21:16:08 pose_transfer_train.py:516] Train [1/100](3030/6372)  Time 4.5803(4.3812)  Loss 0.0012(0.0342)  Lr 0.00010000  Eta 4:04:01
[2024-03-20 21:16:52 pose_transfer_train.py:516] Train [1/100](3040/6372)  Time 4.4344(4.3811)  Loss 0.0075(0.0341)  Lr 0.00010000  Eta 4:03:17
[2024-03-20 21:17:36 pose_transfer_train.py:516] Train [1/100](3050/6372)  Time 4.1544(4.3811)  Loss 0.0694(0.0341)  Lr 0.00010000  Eta 4:02:34
[2024-03-20 21:18:18 pose_transfer_train.py:516] Train [1/100](3060/6372)  Time 4.1769(4.3805)  Loss 0.0311(0.0341)  Lr 0.00010000  Eta 4:01:48
[2024-03-20 21:19:00 pose_transfer_train.py:516] Train [1/100](3070/6372)  Time 4.0888(4.3801)  Loss 0.0048(0.0341)  Lr 0.00010000  Eta 4:01:03
[2024-03-20 21:19:45 pose_transfer_train.py:516] Train [1/100](3080/6372)  Time 4.8988(4.3803)  Loss 0.0205(0.0341)  Lr 0.00010000  Eta 4:00:20
[2024-03-20 21:20:28 pose_transfer_train.py:516] Train [1/100](3090/6372)  Time 4.4267(4.3803)  Loss 0.0019(0.0341)  Lr 0.00010000  Eta 3:59:36
[2024-03-20 21:21:12 pose_transfer_train.py:516] Train [1/100](3100/6372)  Time 4.6657(4.3801)  Loss 0.0014(0.0341)  Lr 0.00010000  Eta 3:58:51
[2024-03-20 21:21:56 pose_transfer_train.py:516] Train [1/100](3110/6372)  Time 4.6273(4.3802)  Loss 0.1201(0.0341)  Lr 0.00010000  Eta 3:58:08
[2024-03-20 21:22:39 pose_transfer_train.py:516] Train [1/100](3120/6372)  Time 4.5460(4.3802)  Loss 0.0270(0.0340)  Lr 0.00010000  Eta 3:57:24
[2024-03-20 21:23:23 pose_transfer_train.py:516] Train [1/100](3130/6372)  Time 4.5431(4.3801)  Loss 0.0260(0.0340)  Lr 0.00010000  Eta 3:56:40
[2024-03-20 21:24:06 pose_transfer_train.py:516] Train [1/100](3140/6372)  Time 4.5718(4.3798)  Loss 0.0531(0.0340)  Lr 0.00010000  Eta 3:55:55
[2024-03-20 21:24:51 pose_transfer_train.py:516] Train [1/100](3150/6372)  Time 4.5703(4.3802)  Loss 0.0624(0.0340)  Lr 0.00010000  Eta 3:55:13
[2024-03-20 21:25:34 pose_transfer_train.py:516] Train [1/100](3160/6372)  Time 3.7201(4.3800)  Loss 0.0392(0.0340)  Lr 0.00010000  Eta 3:54:28
[2024-03-20 21:26:18 pose_transfer_train.py:516] Train [1/100](3170/6372)  Time 4.2358(4.3801)  Loss 0.0273(0.0340)  Lr 0.00010000  Eta 3:53:44
[2024-03-20 21:27:03 pose_transfer_train.py:516] Train [1/100](3180/6372)  Time 4.2421(4.3804)  Loss 0.0041(0.0341)  Lr 0.00010000  Eta 3:53:02
[2024-03-20 21:27:46 pose_transfer_train.py:516] Train [1/100](3190/6372)  Time 3.8275(4.3803)  Loss 0.0480(0.0341)  Lr 0.00010000  Eta 3:52:18
[2024-03-20 21:28:30 pose_transfer_train.py:516] Train [1/100](3200/6372)  Time 4.2167(4.3801)  Loss 0.0516(0.0341)  Lr 0.00010000  Eta 3:51:33
[2024-03-20 21:29:16 pose_transfer_train.py:516] Train [1/100](3210/6372)  Time 4.3617(4.3808)  Loss 0.0415(0.0341)  Lr 0.00010000  Eta 3:50:52
[2024-03-20 21:30:01 pose_transfer_train.py:516] Train [1/100](3220/6372)  Time 4.3351(4.3812)  Loss 0.0512(0.0341)  Lr 0.00010000  Eta 3:50:09
[2024-03-20 21:30:45 pose_transfer_train.py:516] Train [1/100](3230/6372)  Time 4.7812(4.3813)  Loss 0.0048(0.0341)  Lr 0.00010000  Eta 3:49:25
[2024-03-20 21:31:30 pose_transfer_train.py:516] Train [1/100](3240/6372)  Time 4.4807(4.3818)  Loss 0.0308(0.0341)  Lr 0.00010000  Eta 3:48:43
[2024-03-20 21:32:14 pose_transfer_train.py:516] Train [1/100](3250/6372)  Time 3.4883(4.3817)  Loss 0.0212(0.0340)  Lr 0.00010000  Eta 3:47:59
[2024-03-20 21:32:56 pose_transfer_train.py:516] Train [1/100](3260/6372)  Time 4.0592(4.3812)  Loss 0.0262(0.0340)  Lr 0.00010000  Eta 3:47:14
[2024-03-20 21:33:41 pose_transfer_train.py:516] Train [1/100](3270/6372)  Time 4.0773(4.3815)  Loss 0.0095(0.0340)  Lr 0.00010000  Eta 3:46:31
[2024-03-20 21:34:26 pose_transfer_train.py:516] Train [1/100](3280/6372)  Time 4.6393(4.3820)  Loss 0.0243(0.0340)  Lr 0.00010000  Eta 3:45:49
[2024-03-20 21:35:10 pose_transfer_train.py:516] Train [1/100](3290/6372)  Time 4.1397(4.3820)  Loss 0.0401(0.0340)  Lr 0.00010000  Eta 3:45:05
[2024-03-20 21:35:55 pose_transfer_train.py:516] Train [1/100](3300/6372)  Time 4.7282(4.3824)  Loss 0.1195(0.0340)  Lr 0.00010000  Eta 3:44:22
[2024-03-20 21:36:38 pose_transfer_train.py:516] Train [1/100](3310/6372)  Time 4.1559(4.3822)  Loss 0.0327(0.0340)  Lr 0.00010000  Eta 3:43:38
[2024-03-20 21:37:22 pose_transfer_train.py:516] Train [1/100](3320/6372)  Time 3.8318(4.3820)  Loss 0.0103(0.0340)  Lr 0.00010000  Eta 3:42:53
[2024-03-20 21:38:05 pose_transfer_train.py:516] Train [1/100](3330/6372)  Time 4.5282(4.3820)  Loss 0.0084(0.0340)  Lr 0.00010000  Eta 3:42:10
[2024-03-20 21:38:49 pose_transfer_train.py:516] Train [1/100](3340/6372)  Time 4.4964(4.3820)  Loss 0.0166(0.0340)  Lr 0.00010000  Eta 3:41:26
[2024-03-20 21:39:33 pose_transfer_train.py:516] Train [1/100](3350/6372)  Time 5.0581(4.3820)  Loss 0.0547(0.0340)  Lr 0.00010000  Eta 3:40:42
[2024-03-20 21:40:15 pose_transfer_train.py:516] Train [1/100](3360/6372)  Time 4.5445(4.3816)  Loss 0.0257(0.0340)  Lr 0.00010000  Eta 3:39:57
[2024-03-20 21:40:59 pose_transfer_train.py:516] Train [1/100](3370/6372)  Time 4.5056(4.3814)  Loss 0.0722(0.0340)  Lr 0.00010000  Eta 3:39:13
[2024-03-20 21:41:42 pose_transfer_train.py:516] Train [1/100](3380/6372)  Time 4.5941(4.3812)  Loss 0.0015(0.0340)  Lr 0.00010000  Eta 3:38:28
[2024-03-20 21:42:25 pose_transfer_train.py:516] Train [1/100](3390/6372)  Time 4.1957(4.3810)  Loss 0.0102(0.0340)  Lr 0.00010000  Eta 3:37:44
[2024-03-20 21:43:07 pose_transfer_train.py:516] Train [1/100](3400/6372)  Time 4.4761(4.3807)  Loss 0.0038(0.0339)  Lr 0.00010000  Eta 3:36:59
[2024-03-20 21:43:50 pose_transfer_train.py:516] Train [1/100](3410/6372)  Time 4.6013(4.3803)  Loss 0.0414(0.0339)  Lr 0.00010000  Eta 3:36:14
[2024-03-20 21:44:33 pose_transfer_train.py:516] Train [1/100](3420/6372)  Time 4.7883(4.3801)  Loss 0.0374(0.0339)  Lr 0.00010000  Eta 3:35:30
[2024-03-20 21:45:16 pose_transfer_train.py:516] Train [1/100](3430/6372)  Time 4.4004(4.3798)  Loss 0.0083(0.0339)  Lr 0.00010000  Eta 3:34:45
[2024-03-20 21:46:01 pose_transfer_train.py:516] Train [1/100](3440/6372)  Time 4.2325(4.3801)  Loss 0.0082(0.0338)  Lr 0.00010000  Eta 3:34:02
[2024-03-20 21:46:43 pose_transfer_train.py:516] Train [1/100](3450/6372)  Time 4.5011(4.3796)  Loss 0.0882(0.0339)  Lr 0.00010000  Eta 3:33:17
[2024-03-20 21:47:26 pose_transfer_train.py:516] Train [1/100](3460/6372)  Time 4.4314(4.3793)  Loss 0.0162(0.0338)  Lr 0.00010000  Eta 3:32:32
[2024-03-20 21:48:09 pose_transfer_train.py:516] Train [1/100](3470/6372)  Time 4.6307(4.3792)  Loss 0.0172(0.0338)  Lr 0.00010000  Eta 3:31:48
[2024-03-20 21:48:52 pose_transfer_train.py:516] Train [1/100](3480/6372)  Time 4.3927(4.3789)  Loss 0.0598(0.0338)  Lr 0.00010000  Eta 3:31:03
[2024-03-20 21:49:36 pose_transfer_train.py:516] Train [1/100](3490/6372)  Time 4.2001(4.3790)  Loss 0.0133(0.0337)  Lr 0.00010000  Eta 3:30:20
[2024-03-20 21:50:20 pose_transfer_train.py:516] Train [1/100](3500/6372)  Time 4.2669(4.3791)  Loss 0.0106(0.0338)  Lr 0.00010000  Eta 3:29:36
[2024-03-20 21:51:04 pose_transfer_train.py:516] Train [1/100](3510/6372)  Time 4.2005(4.3791)  Loss 0.0197(0.0337)  Lr 0.00010000  Eta 3:28:53
[2024-03-20 21:51:48 pose_transfer_train.py:516] Train [1/100](3520/6372)  Time 4.0837(4.3791)  Loss 0.0516(0.0337)  Lr 0.00010000  Eta 3:28:09
[2024-03-20 21:52:32 pose_transfer_train.py:516] Train [1/100](3530/6372)  Time 4.4269(4.3792)  Loss 0.0428(0.0337)  Lr 0.00010000  Eta 3:27:25
[2024-03-20 21:53:16 pose_transfer_train.py:516] Train [1/100](3540/6372)  Time 4.3600(4.3794)  Loss 0.0180(0.0337)  Lr 0.00010000  Eta 3:26:42
[2024-03-20 21:54:00 pose_transfer_train.py:516] Train [1/100](3550/6372)  Time 4.4232(4.3793)  Loss 0.0260(0.0337)  Lr 0.00010000  Eta 3:25:58
[2024-03-20 21:54:43 pose_transfer_train.py:516] Train [1/100](3560/6372)  Time 4.3214(4.3792)  Loss 0.0134(0.0336)  Lr 0.00010000  Eta 3:25:14
[2024-03-20 21:55:26 pose_transfer_train.py:516] Train [1/100](3570/6372)  Time 4.4549(4.3789)  Loss 0.0169(0.0336)  Lr 0.00010000  Eta 3:24:29
[2024-03-20 21:56:10 pose_transfer_train.py:516] Train [1/100](3580/6372)  Time 4.5249(4.3789)  Loss 0.0155(0.0336)  Lr 0.00010000  Eta 3:23:45
[2024-03-20 21:56:53 pose_transfer_train.py:516] Train [1/100](3590/6372)  Time 4.2282(4.3789)  Loss 0.0062(0.0336)  Lr 0.00010000  Eta 3:23:01
[2024-03-20 21:57:38 pose_transfer_train.py:516] Train [1/100](3600/6372)  Time 4.5710(4.3790)  Loss 0.0529(0.0336)  Lr 0.00010000  Eta 3:22:18
[2024-03-20 21:58:22 pose_transfer_train.py:516] Train [1/100](3610/6372)  Time 4.4021(4.3792)  Loss 0.1387(0.0336)  Lr 0.00010000  Eta 3:21:35
[2024-03-20 21:59:06 pose_transfer_train.py:516] Train [1/100](3620/6372)  Time 3.8732(4.3791)  Loss 0.0735(0.0336)  Lr 0.00010000  Eta 3:20:51
[2024-03-20 21:59:50 pose_transfer_train.py:516] Train [1/100](3630/6372)  Time 4.5192(4.3794)  Loss 0.0115(0.0336)  Lr 0.00010000  Eta 3:20:08
[2024-03-20 22:00:35 pose_transfer_train.py:516] Train [1/100](3640/6372)  Time 4.7865(4.3796)  Loss 0.0190(0.0336)  Lr 0.00010000  Eta 3:19:24
[2024-03-20 22:01:19 pose_transfer_train.py:516] Train [1/100](3650/6372)  Time 4.7210(4.3798)  Loss 0.0296(0.0336)  Lr 0.00010000  Eta 3:18:41
[2024-03-20 22:02:03 pose_transfer_train.py:516] Train [1/100](3660/6372)  Time 4.9796(4.3797)  Loss 0.0866(0.0335)  Lr 0.00010000  Eta 3:17:57
[2024-03-20 22:02:47 pose_transfer_train.py:516] Train [1/100](3670/6372)  Time 4.6658(4.3798)  Loss 0.0272(0.0335)  Lr 0.00010000  Eta 3:17:14
[2024-03-20 22:03:31 pose_transfer_train.py:516] Train [1/100](3680/6372)  Time 4.6722(4.3800)  Loss 0.0572(0.0336)  Lr 0.00010000  Eta 3:16:30
[2024-03-20 22:04:17 pose_transfer_train.py:516] Train [1/100](3690/6372)  Time 4.5201(4.3803)  Loss 0.0583(0.0336)  Lr 0.00010000  Eta 3:15:48
[2024-03-20 22:05:00 pose_transfer_train.py:516] Train [1/100](3700/6372)  Time 4.5405(4.3801)  Loss 0.0231(0.0335)  Lr 0.00010000  Eta 3:15:03
[2024-03-20 22:05:43 pose_transfer_train.py:516] Train [1/100](3710/6372)  Time 4.6824(4.3801)  Loss 0.0308(0.0335)  Lr 0.00010000  Eta 3:14:19
[2024-03-20 22:06:26 pose_transfer_train.py:516] Train [1/100](3720/6372)  Time 4.9388(4.3799)  Loss 0.0073(0.0335)  Lr 0.00010000  Eta 3:13:35
[2024-03-20 22:07:11 pose_transfer_train.py:516] Train [1/100](3730/6372)  Time 4.6338(4.3800)  Loss 0.1263(0.0335)  Lr 0.00010000  Eta 3:12:52
[2024-03-20 22:07:54 pose_transfer_train.py:516] Train [1/100](3740/6372)  Time 4.6255(4.3800)  Loss 0.1055(0.0335)  Lr 0.00010000  Eta 3:12:08
[2024-03-20 22:08:39 pose_transfer_train.py:516] Train [1/100](3750/6372)  Time 4.1693(4.3803)  Loss 0.0328(0.0335)  Lr 0.00010000  Eta 3:11:25
[2024-03-20 22:09:25 pose_transfer_train.py:516] Train [1/100](3760/6372)  Time 4.6500(4.3807)  Loss 0.0090(0.0335)  Lr 0.00010000  Eta 3:10:42
[2024-03-20 22:10:09 pose_transfer_train.py:516] Train [1/100](3770/6372)  Time 4.1376(4.3809)  Loss 0.0373(0.0335)  Lr 0.00010000  Eta 3:09:58
[2024-03-20 22:10:54 pose_transfer_train.py:516] Train [1/100](3780/6372)  Time 4.3007(4.3812)  Loss 0.0468(0.0335)  Lr 0.00010000  Eta 3:09:16
[2024-03-20 22:11:38 pose_transfer_train.py:516] Train [1/100](3790/6372)  Time 4.6653(4.3812)  Loss 0.0037(0.0335)  Lr 0.00010000  Eta 3:08:32
[2024-03-20 22:12:22 pose_transfer_train.py:516] Train [1/100](3800/6372)  Time 4.6226(4.3813)  Loss 0.0482(0.0335)  Lr 0.00010000  Eta 3:07:48
[2024-03-20 22:13:07 pose_transfer_train.py:516] Train [1/100](3810/6372)  Time 4.6114(4.3815)  Loss 0.0049(0.0335)  Lr 0.00010000  Eta 3:07:05
[2024-03-20 22:13:50 pose_transfer_train.py:516] Train [1/100](3820/6372)  Time 4.1567(4.3813)  Loss 0.0629(0.0335)  Lr 0.00010000  Eta 3:06:21
[2024-03-20 22:14:34 pose_transfer_train.py:516] Train [1/100](3830/6372)  Time 4.2764(4.3813)  Loss 0.0117(0.0335)  Lr 0.00010000  Eta 3:05:37
[2024-03-20 22:15:17 pose_transfer_train.py:516] Train [1/100](3840/6372)  Time 4.5251(4.3811)  Loss 0.0742(0.0336)  Lr 0.00010000  Eta 3:04:53
[2024-03-20 22:16:02 pose_transfer_train.py:516] Train [1/100](3850/6372)  Time 4.7975(4.3815)  Loss 0.0349(0.0335)  Lr 0.00010000  Eta 3:04:10
[2024-03-20 22:16:49 pose_transfer_train.py:516] Train [1/100](3860/6372)  Time 4.8676(4.3824)  Loss 0.0315(0.0336)  Lr 0.00010000  Eta 3:03:28
[2024-03-20 22:17:31 pose_transfer_train.py:516] Train [1/100](3870/6372)  Time 3.9777(4.3818)  Loss 0.0097(0.0336)  Lr 0.00010000  Eta 3:02:43
[2024-03-20 22:18:14 pose_transfer_train.py:516] Train [1/100](3880/6372)  Time 4.4625(4.3816)  Loss 0.0303(0.0336)  Lr 0.00010000  Eta 3:01:59
[2024-03-20 22:18:57 pose_transfer_train.py:516] Train [1/100](3890/6372)  Time 3.8645(4.3815)  Loss 0.0210(0.0336)  Lr 0.00010000  Eta 3:01:14
[2024-03-20 22:19:43 pose_transfer_train.py:516] Train [1/100](3900/6372)  Time 4.3852(4.3819)  Loss 0.0221(0.0336)  Lr 0.00010000  Eta 3:00:32
[2024-03-20 22:20:27 pose_transfer_train.py:516] Train [1/100](3910/6372)  Time 4.3257(4.3819)  Loss 0.0455(0.0336)  Lr 0.00010000  Eta 2:59:48
[2024-03-20 22:21:11 pose_transfer_train.py:516] Train [1/100](3920/6372)  Time 4.0210(4.3821)  Loss 0.0318(0.0336)  Lr 0.00010000  Eta 2:59:05
[2024-03-20 22:21:57 pose_transfer_train.py:516] Train [1/100](3930/6372)  Time 4.7177(4.3825)  Loss 0.0038(0.0336)  Lr 0.00010000  Eta 2:58:22
[2024-03-20 22:22:42 pose_transfer_train.py:516] Train [1/100](3940/6372)  Time 4.4241(4.3828)  Loss 0.0497(0.0336)  Lr 0.00010000  Eta 2:57:39
[2024-03-20 22:23:25 pose_transfer_train.py:516] Train [1/100](3950/6372)  Time 4.5545(4.3828)  Loss 0.0245(0.0335)  Lr 0.00010000  Eta 2:56:55
[2024-03-20 22:24:09 pose_transfer_train.py:516] Train [1/100](3960/6372)  Time 4.1785(4.3828)  Loss 0.0291(0.0336)  Lr 0.00010000  Eta 2:56:11
[2024-03-20 22:24:53 pose_transfer_train.py:516] Train [1/100](3970/6372)  Time 4.0873(4.3829)  Loss 0.0635(0.0336)  Lr 0.00010000  Eta 2:55:27
[2024-03-20 22:25:36 pose_transfer_train.py:516] Train [1/100](3980/6372)  Time 4.6247(4.3827)  Loss 0.0842(0.0336)  Lr 0.00010000  Eta 2:54:43
[2024-03-20 22:26:19 pose_transfer_train.py:516] Train [1/100](3990/6372)  Time 4.6855(4.3825)  Loss 0.0843(0.0336)  Lr 0.00010000  Eta 2:53:59
[2024-03-20 22:27:04 pose_transfer_train.py:516] Train [1/100](4000/6372)  Time 4.5054(4.3828)  Loss 0.0546(0.0336)  Lr 0.00010000  Eta 2:53:15
[2024-03-20 22:27:48 pose_transfer_train.py:516] Train [1/100](4010/6372)  Time 4.3578(4.3827)  Loss 0.0034(0.0336)  Lr 0.00010000  Eta 2:52:31
[2024-03-20 22:28:31 pose_transfer_train.py:516] Train [1/100](4020/6372)  Time 3.9365(4.3826)  Loss 0.0556(0.0335)  Lr 0.00010000  Eta 2:51:47
[2024-03-20 22:29:16 pose_transfer_train.py:516] Train [1/100](4030/6372)  Time 3.9526(4.3828)  Loss 0.1356(0.0336)  Lr 0.00010000  Eta 2:51:04
[2024-03-20 22:29:59 pose_transfer_train.py:516] Train [1/100](4040/6372)  Time 4.4496(4.3825)  Loss 0.0111(0.0335)  Lr 0.00010000  Eta 2:50:20
[2024-03-20 22:30:44 pose_transfer_train.py:516] Train [1/100](4050/6372)  Time 3.9315(4.3828)  Loss 0.0279(0.0335)  Lr 0.00010000  Eta 2:49:36
[2024-03-20 22:31:27 pose_transfer_train.py:516] Train [1/100](4060/6372)  Time 4.1584(4.3828)  Loss 0.0223(0.0335)  Lr 0.00010000  Eta 2:48:52
[2024-03-20 22:32:11 pose_transfer_train.py:516] Train [1/100](4070/6372)  Time 4.1108(4.3827)  Loss 0.0090(0.0335)  Lr 0.00010000  Eta 2:48:09
[2024-03-20 22:32:54 pose_transfer_train.py:516] Train [1/100](4080/6372)  Time 3.9212(4.3825)  Loss 0.0315(0.0335)  Lr 0.00010000  Eta 2:47:24
[2024-03-20 22:33:38 pose_transfer_train.py:516] Train [1/100](4090/6372)  Time 3.9019(4.3826)  Loss 0.0112(0.0335)  Lr 0.00010000  Eta 2:46:41
[2024-03-20 22:34:23 pose_transfer_train.py:516] Train [1/100](4100/6372)  Time 4.7139(4.3828)  Loss 0.0019(0.0335)  Lr 0.00010000  Eta 2:45:57
[2024-03-20 22:35:05 pose_transfer_train.py:516] Train [1/100](4110/6372)  Time 4.5413(4.3825)  Loss 0.1535(0.0335)  Lr 0.00010000  Eta 2:45:13
[2024-03-20 22:35:49 pose_transfer_train.py:516] Train [1/100](4120/6372)  Time 3.8266(4.3826)  Loss 0.0112(0.0335)  Lr 0.00010000  Eta 2:44:29
[2024-03-20 22:36:33 pose_transfer_train.py:516] Train [1/100](4130/6372)  Time 4.4319(4.3824)  Loss 0.0396(0.0335)  Lr 0.00010000  Eta 2:43:45
[2024-03-20 22:37:16 pose_transfer_train.py:516] Train [1/100](4140/6372)  Time 4.4186(4.3824)  Loss 0.0148(0.0335)  Lr 0.00010000  Eta 2:43:01
[2024-03-20 22:37:59 pose_transfer_train.py:516] Train [1/100](4150/6372)  Time 4.6334(4.3822)  Loss 0.0155(0.0335)  Lr 0.00010000  Eta 2:42:17
[2024-03-20 22:38:43 pose_transfer_train.py:516] Train [1/100](4160/6372)  Time 3.9879(4.3822)  Loss 0.0490(0.0335)  Lr 0.00010000  Eta 2:41:33
[2024-03-20 22:39:28 pose_transfer_train.py:516] Train [1/100](4170/6372)  Time 4.3750(4.3824)  Loss 0.0193(0.0335)  Lr 0.00010000  Eta 2:40:50
[2024-03-20 22:40:13 pose_transfer_train.py:516] Train [1/100](4180/6372)  Time 4.5799(4.3828)  Loss 0.0103(0.0335)  Lr 0.00010000  Eta 2:40:07
[2024-03-20 22:40:58 pose_transfer_train.py:516] Train [1/100](4190/6372)  Time 4.2706(4.3830)  Loss 0.0178(0.0335)  Lr 0.00010000  Eta 2:39:23
[2024-03-20 22:41:41 pose_transfer_train.py:516] Train [1/100](4200/6372)  Time 4.0814(4.3828)  Loss 0.0073(0.0335)  Lr 0.00010000  Eta 2:38:39
[2024-03-20 22:42:25 pose_transfer_train.py:516] Train [1/100](4210/6372)  Time 4.1101(4.3828)  Loss 0.0461(0.0334)  Lr 0.00010000  Eta 2:37:55
[2024-03-20 22:43:08 pose_transfer_train.py:516] Train [1/100](4220/6372)  Time 4.2558(4.3826)  Loss 0.0119(0.0334)  Lr 0.00010000  Eta 2:37:11
[2024-03-20 22:43:53 pose_transfer_train.py:516] Train [1/100](4230/6372)  Time 4.3701(4.3828)  Loss 0.0127(0.0334)  Lr 0.00010000  Eta 2:36:27
[2024-03-20 22:44:35 pose_transfer_train.py:516] Train [1/100](4240/6372)  Time 4.5054(4.3825)  Loss 0.0135(0.0334)  Lr 0.00010000  Eta 2:35:43
[2024-03-20 22:45:18 pose_transfer_train.py:516] Train [1/100](4250/6372)  Time 4.6207(4.3824)  Loss 0.0026(0.0334)  Lr 0.00010000  Eta 2:34:59
[2024-03-20 22:46:02 pose_transfer_train.py:516] Train [1/100](4260/6372)  Time 4.4145(4.3824)  Loss 0.0749(0.0334)  Lr 0.00010000  Eta 2:34:15
[2024-03-20 22:46:46 pose_transfer_train.py:516] Train [1/100](4270/6372)  Time 4.4427(4.3824)  Loss 0.0354(0.0334)  Lr 0.00010000  Eta 2:33:31
[2024-03-20 22:47:29 pose_transfer_train.py:516] Train [1/100](4280/6372)  Time 4.2050(4.3823)  Loss 0.0175(0.0334)  Lr 0.00010000  Eta 2:32:47
[2024-03-20 22:48:14 pose_transfer_train.py:516] Train [1/100](4290/6372)  Time 4.3352(4.3824)  Loss 0.0265(0.0334)  Lr 0.00010000  Eta 2:32:04
[2024-03-20 22:48:58 pose_transfer_train.py:516] Train [1/100](4300/6372)  Time 4.4799(4.3824)  Loss 0.0310(0.0334)  Lr 0.00010000  Eta 2:31:20
[2024-03-20 22:49:41 pose_transfer_train.py:516] Train [1/100](4310/6372)  Time 4.3440(4.3823)  Loss 0.0059(0.0334)  Lr 0.00010000  Eta 2:30:36
[2024-03-20 22:50:25 pose_transfer_train.py:516] Train [1/100](4320/6372)  Time 4.4158(4.3824)  Loss 0.0727(0.0334)  Lr 0.00010000  Eta 2:29:52
[2024-03-20 22:51:09 pose_transfer_train.py:516] Train [1/100](4330/6372)  Time 3.8649(4.3825)  Loss 0.0063(0.0333)  Lr 0.00010000  Eta 2:29:09
[2024-03-20 22:51:53 pose_transfer_train.py:516] Train [1/100](4340/6372)  Time 4.2145(4.3824)  Loss 0.0444(0.0333)  Lr 0.00010000  Eta 2:28:25
[2024-03-20 22:52:39 pose_transfer_train.py:516] Train [1/100](4350/6372)  Time 4.6719(4.3828)  Loss 0.0170(0.0333)  Lr 0.00010000  Eta 2:27:42
[2024-03-20 22:53:23 pose_transfer_train.py:516] Train [1/100](4360/6372)  Time 4.4932(4.3829)  Loss 0.0563(0.0333)  Lr 0.00010000  Eta 2:26:58
[2024-03-20 22:54:07 pose_transfer_train.py:516] Train [1/100](4370/6372)  Time 4.1301(4.3831)  Loss 0.0812(0.0333)  Lr 0.00010000  Eta 2:26:14
[2024-03-20 22:54:51 pose_transfer_train.py:516] Train [1/100](4380/6372)  Time 3.8182(4.3831)  Loss 0.0255(0.0333)  Lr 0.00010000  Eta 2:25:31
[2024-03-20 22:55:35 pose_transfer_train.py:516] Train [1/100](4390/6372)  Time 4.4395(4.3832)  Loss 0.1194(0.0333)  Lr 0.00010000  Eta 2:24:47
[2024-03-20 22:56:20 pose_transfer_train.py:516] Train [1/100](4400/6372)  Time 4.4610(4.3834)  Loss 0.0167(0.0333)  Lr 0.00010000  Eta 2:24:04
[2024-03-20 22:57:07 pose_transfer_train.py:516] Train [1/100](4410/6372)  Time 4.1889(4.3840)  Loss 0.0748(0.0333)  Lr 0.00010000  Eta 2:23:21
[2024-03-20 22:57:51 pose_transfer_train.py:516] Train [1/100](4420/6372)  Time 4.6941(4.3842)  Loss 0.0398(0.0333)  Lr 0.00010000  Eta 2:22:37
[2024-03-20 22:58:34 pose_transfer_train.py:516] Train [1/100](4430/6372)  Time 4.3188(4.3839)  Loss 0.0097(0.0333)  Lr 0.00010000  Eta 2:21:53
[2024-03-20 22:59:19 pose_transfer_train.py:516] Train [1/100](4440/6372)  Time 4.6649(4.3841)  Loss 0.0197(0.0333)  Lr 0.00010000  Eta 2:21:10
[2024-03-20 23:00:03 pose_transfer_train.py:516] Train [1/100](4450/6372)  Time 4.4960(4.3843)  Loss 0.0518(0.0333)  Lr 0.00010000  Eta 2:20:26
[2024-03-20 23:00:48 pose_transfer_train.py:516] Train [1/100](4460/6372)  Time 4.2683(4.3844)  Loss 0.0541(0.0333)  Lr 0.00010000  Eta 2:19:42
[2024-03-20 23:01:30 pose_transfer_train.py:516] Train [1/100](4470/6372)  Time 4.3600(4.3840)  Loss 0.0309(0.0332)  Lr 0.00010000  Eta 2:18:58
[2024-03-20 23:02:14 pose_transfer_train.py:516] Train [1/100](4480/6372)  Time 4.9661(4.3841)  Loss 0.0545(0.0332)  Lr 0.00010000  Eta 2:18:14
[2024-03-20 23:02:59 pose_transfer_train.py:516] Train [1/100](4490/6372)  Time 4.5832(4.3843)  Loss 0.0200(0.0332)  Lr 0.00010000  Eta 2:17:31
[2024-03-20 23:03:44 pose_transfer_train.py:516] Train [1/100](4500/6372)  Time 4.4909(4.3845)  Loss 0.0194(0.0332)  Lr 0.00010000  Eta 2:16:47
[2024-03-20 23:04:29 pose_transfer_train.py:516] Train [1/100](4510/6372)  Time 5.0385(4.3849)  Loss 0.0375(0.0332)  Lr 0.00010000  Eta 2:16:04
[2024-03-20 23:05:15 pose_transfer_train.py:516] Train [1/100](4520/6372)  Time 4.8268(4.3853)  Loss 0.0111(0.0332)  Lr 0.00010000  Eta 2:15:21
[2024-03-20 23:05:59 pose_transfer_train.py:516] Train [1/100](4530/6372)  Time 4.6402(4.3854)  Loss 0.0125(0.0332)  Lr 0.00010000  Eta 2:14:37
[2024-03-20 23:06:44 pose_transfer_train.py:516] Train [1/100](4540/6372)  Time 4.4142(4.3855)  Loss 0.0274(0.0332)  Lr 0.00010000  Eta 2:13:54
[2024-03-20 23:07:28 pose_transfer_train.py:516] Train [1/100](4550/6372)  Time 4.3196(4.3857)  Loss 0.0146(0.0332)  Lr 0.00010000  Eta 2:13:10
[2024-03-20 23:08:11 pose_transfer_train.py:516] Train [1/100](4560/6372)  Time 4.0149(4.3855)  Loss 0.0134(0.0331)  Lr 0.00010000  Eta 2:12:26
[2024-03-20 23:08:56 pose_transfer_train.py:516] Train [1/100](4570/6372)  Time 4.3327(4.3858)  Loss 0.0443(0.0331)  Lr 0.00010000  Eta 2:11:43
[2024-03-20 23:09:39 pose_transfer_train.py:516] Train [1/100](4580/6372)  Time 3.9763(4.3855)  Loss 0.0490(0.0331)  Lr 0.00010000  Eta 2:10:58
[2024-03-20 23:10:25 pose_transfer_train.py:516] Train [1/100](4590/6372)  Time 4.7529(4.3859)  Loss 0.0541(0.0331)  Lr 0.00010000  Eta 2:10:15
[2024-03-20 23:11:10 pose_transfer_train.py:516] Train [1/100](4600/6372)  Time 4.4212(4.3862)  Loss 0.1768(0.0331)  Lr 0.00010000  Eta 2:09:32
[2024-03-20 23:11:53 pose_transfer_train.py:516] Train [1/100](4610/6372)  Time 4.0486(4.3860)  Loss 0.0340(0.0331)  Lr 0.00010000  Eta 2:08:48
[2024-03-20 23:12:36 pose_transfer_train.py:516] Train [1/100](4620/6372)  Time 4.8284(4.3859)  Loss 0.0200(0.0331)  Lr 0.00010000  Eta 2:08:04
[2024-03-20 23:13:20 pose_transfer_train.py:516] Train [1/100](4630/6372)  Time 4.6629(4.3859)  Loss 0.0035(0.0331)  Lr 0.00010000  Eta 2:07:20
[2024-03-20 23:14:03 pose_transfer_train.py:516] Train [1/100](4640/6372)  Time 4.1524(4.3857)  Loss 0.0534(0.0331)  Lr 0.00010000  Eta 2:06:36
[2024-03-20 23:14:48 pose_transfer_train.py:516] Train [1/100](4650/6372)  Time 4.4340(4.3860)  Loss 0.0463(0.0331)  Lr 0.00010000  Eta 2:05:52
[2024-03-20 23:15:30 pose_transfer_train.py:516] Train [1/100](4660/6372)  Time 4.2566(4.3857)  Loss 0.0055(0.0331)  Lr 0.00010000  Eta 2:05:08
[2024-03-20 23:16:14 pose_transfer_train.py:516] Train [1/100](4670/6372)  Time 4.6000(4.3857)  Loss 0.0916(0.0331)  Lr 0.00010000  Eta 2:04:24
[2024-03-20 23:16:58 pose_transfer_train.py:516] Train [1/100](4680/6372)  Time 4.0383(4.3857)  Loss 0.0349(0.0331)  Lr 0.00010000  Eta 2:03:40
[2024-03-20 23:17:43 pose_transfer_train.py:516] Train [1/100](4690/6372)  Time 4.7140(4.3860)  Loss 0.0104(0.0331)  Lr 0.00010000  Eta 2:02:57
[2024-03-20 23:18:28 pose_transfer_train.py:516] Train [1/100](4700/6372)  Time 4.5202(4.3860)  Loss 0.0184(0.0331)  Lr 0.00010000  Eta 2:02:13
[2024-03-20 23:19:12 pose_transfer_train.py:516] Train [1/100](4710/6372)  Time 4.1971(4.3861)  Loss 0.0630(0.0331)  Lr 0.00010000  Eta 2:01:29
[2024-03-20 23:19:57 pose_transfer_train.py:516] Train [1/100](4720/6372)  Time 4.4092(4.3863)  Loss 0.0046(0.0331)  Lr 0.00010000  Eta 2:00:46
[2024-03-20 23:20:42 pose_transfer_train.py:516] Train [1/100](4730/6372)  Time 4.5190(4.3866)  Loss 0.0090(0.0331)  Lr 0.00010000  Eta 2:00:02
[2024-03-20 23:21:25 pose_transfer_train.py:516] Train [1/100](4740/6372)  Time 4.3915(4.3865)  Loss 0.0664(0.0331)  Lr 0.00010000  Eta 1:59:18
[2024-03-20 23:22:10 pose_transfer_train.py:516] Train [1/100](4750/6372)  Time 4.6534(4.3866)  Loss 0.0176(0.0331)  Lr 0.00010000  Eta 1:58:35
[2024-03-20 23:22:53 pose_transfer_train.py:516] Train [1/100](4760/6372)  Time 4.5108(4.3866)  Loss 0.0115(0.0331)  Lr 0.00010000  Eta 1:57:51
[2024-03-20 23:23:38 pose_transfer_train.py:516] Train [1/100](4770/6372)  Time 4.4100(4.3867)  Loss 0.0026(0.0331)  Lr 0.00010000  Eta 1:57:07
[2024-03-20 23:24:20 pose_transfer_train.py:516] Train [1/100](4780/6372)  Time 4.5261(4.3865)  Loss 0.0106(0.0330)  Lr 0.00010000  Eta 1:56:23
[2024-03-20 23:25:05 pose_transfer_train.py:516] Train [1/100](4790/6372)  Time 4.4073(4.3866)  Loss 0.0161(0.0330)  Lr 0.00010000  Eta 1:55:39
[2024-03-20 23:25:49 pose_transfer_train.py:516] Train [1/100](4800/6372)  Time 4.3005(4.3865)  Loss 0.0123(0.0330)  Lr 0.00010000  Eta 1:54:55
[2024-03-20 23:26:33 pose_transfer_train.py:516] Train [1/100](4810/6372)  Time 4.4478(4.3866)  Loss 0.0154(0.0330)  Lr 0.00010000  Eta 1:54:11
[2024-03-20 23:27:16 pose_transfer_train.py:516] Train [1/100](4820/6372)  Time 3.7070(4.3865)  Loss 0.0070(0.0330)  Lr 0.00010000  Eta 1:53:27
[2024-03-20 23:28:00 pose_transfer_train.py:516] Train [1/100](4830/6372)  Time 4.3912(4.3865)  Loss 0.0284(0.0330)  Lr 0.00010000  Eta 1:52:43
[2024-03-20 23:28:44 pose_transfer_train.py:516] Train [1/100](4840/6372)  Time 4.5083(4.3865)  Loss 0.0115(0.0329)  Lr 0.00010000  Eta 1:52:00
[2024-03-20 23:29:27 pose_transfer_train.py:516] Train [1/100](4850/6372)  Time 3.6128(4.3863)  Loss 0.0316(0.0329)  Lr 0.00010000  Eta 1:51:15
[2024-03-20 23:30:12 pose_transfer_train.py:516] Train [1/100](4860/6372)  Time 4.4410(4.3865)  Loss 0.0176(0.0329)  Lr 0.00010000  Eta 1:50:32
[2024-03-20 23:30:56 pose_transfer_train.py:516] Train [1/100](4870/6372)  Time 4.9845(4.3865)  Loss 0.0022(0.0329)  Lr 0.00010000  Eta 1:49:48
[2024-03-20 23:31:40 pose_transfer_train.py:516] Train [1/100](4880/6372)  Time 4.6407(4.3865)  Loss 0.0077(0.0329)  Lr 0.00010000  Eta 1:49:04
[2024-03-20 23:32:23 pose_transfer_train.py:516] Train [1/100](4890/6372)  Time 4.3725(4.3865)  Loss 0.0642(0.0329)  Lr 0.00010000  Eta 1:48:20
[2024-03-20 23:33:07 pose_transfer_train.py:516] Train [1/100](4900/6372)  Time 4.6238(4.3864)  Loss 0.0063(0.0329)  Lr 0.00010000  Eta 1:47:36
[2024-03-20 23:33:51 pose_transfer_train.py:516] Train [1/100](4910/6372)  Time 4.3274(4.3866)  Loss 0.0570(0.0329)  Lr 0.00010000  Eta 1:46:53
[2024-03-20 23:34:35 pose_transfer_train.py:516] Train [1/100](4920/6372)  Time 4.2543(4.3865)  Loss 0.0105(0.0329)  Lr 0.00010000  Eta 1:46:09
[2024-03-20 23:35:18 pose_transfer_train.py:516] Train [1/100](4930/6372)  Time 4.2398(4.3864)  Loss 0.0159(0.0329)  Lr 0.00010000  Eta 1:45:25
[2024-03-20 23:36:02 pose_transfer_train.py:516] Train [1/100](4940/6372)  Time 4.1315(4.3864)  Loss 0.0039(0.0329)  Lr 0.00010000  Eta 1:44:41
[2024-03-20 23:36:47 pose_transfer_train.py:516] Train [1/100](4950/6372)  Time 4.6220(4.3867)  Loss 0.0066(0.0328)  Lr 0.00010000  Eta 1:43:57
[2024-03-20 23:37:33 pose_transfer_train.py:516] Train [1/100](4960/6372)  Time 4.4054(4.3871)  Loss 0.0159(0.0328)  Lr 0.00010000  Eta 1:43:14
[2024-03-20 23:38:16 pose_transfer_train.py:516] Train [1/100](4970/6372)  Time 4.3619(4.3869)  Loss 0.0064(0.0328)  Lr 0.00010000  Eta 1:42:30
[2024-03-20 23:39:00 pose_transfer_train.py:516] Train [1/100](4980/6372)  Time 4.6334(4.3869)  Loss 0.0374(0.0328)  Lr 0.00010000  Eta 1:41:46
[2024-03-20 23:39:44 pose_transfer_train.py:516] Train [1/100](4990/6372)  Time 4.5592(4.3870)  Loss 0.0018(0.0328)  Lr 0.00010000  Eta 1:41:02
[2024-03-20 23:40:28 pose_transfer_train.py:516] Train [1/100](5000/6372)  Time 4.5628(4.3870)  Loss 0.0011(0.0328)  Lr 0.00010000  Eta 1:40:19
[2024-03-20 23:41:10 pose_transfer_train.py:516] Train [1/100](5010/6372)  Time 4.6251(4.3866)  Loss 0.0102(0.0328)  Lr 0.00010000  Eta 1:39:34
[2024-03-20 23:41:53 pose_transfer_train.py:516] Train [1/100](5020/6372)  Time 4.6325(4.3863)  Loss 0.0210(0.0328)  Lr 0.00010000  Eta 1:38:50
[2024-03-20 23:42:38 pose_transfer_train.py:516] Train [1/100](5030/6372)  Time 4.5746(4.3867)  Loss 0.0247(0.0328)  Lr 0.00010000  Eta 1:38:06
[2024-03-20 23:43:22 pose_transfer_train.py:516] Train [1/100](5040/6372)  Time 4.0196(4.3866)  Loss 0.0225(0.0328)  Lr 0.00010000  Eta 1:37:22
[2024-03-20 23:44:05 pose_transfer_train.py:516] Train [1/100](5050/6372)  Time 4.0545(4.3864)  Loss 0.0171(0.0328)  Lr 0.00010000  Eta 1:36:38
[2024-03-20 23:44:49 pose_transfer_train.py:516] Train [1/100](5060/6372)  Time 4.6520(4.3864)  Loss 0.0534(0.0328)  Lr 0.00010000  Eta 1:35:55
[2024-03-20 23:45:32 pose_transfer_train.py:516] Train [1/100](5070/6372)  Time 4.1836(4.3863)  Loss 0.0017(0.0327)  Lr 0.00010000  Eta 1:35:10
[2024-03-20 23:46:16 pose_transfer_train.py:516] Train [1/100](5080/6372)  Time 4.5901(4.3864)  Loss 0.0581(0.0327)  Lr 0.00010000  Eta 1:34:27
[2024-03-20 23:47:01 pose_transfer_train.py:516] Train [1/100](5090/6372)  Time 4.6038(4.3865)  Loss 0.0893(0.0327)  Lr 0.00010000  Eta 1:33:43
[2024-03-20 23:47:46 pose_transfer_train.py:516] Train [1/100](5100/6372)  Time 4.2942(4.3868)  Loss 0.0037(0.0327)  Lr 0.00010000  Eta 1:32:59
[2024-03-20 23:48:29 pose_transfer_train.py:516] Train [1/100](5110/6372)  Time 4.3839(4.3867)  Loss 0.0258(0.0327)  Lr 0.00010000  Eta 1:32:16
[2024-03-20 23:49:13 pose_transfer_train.py:516] Train [1/100](5120/6372)  Time 4.1333(4.3866)  Loss 0.0085(0.0327)  Lr 0.00010000  Eta 1:31:31
[2024-03-20 23:49:56 pose_transfer_train.py:516] Train [1/100](5130/6372)  Time 3.9129(4.3865)  Loss 0.0111(0.0327)  Lr 0.00010000  Eta 1:30:48
[2024-03-20 23:50:40 pose_transfer_train.py:516] Train [1/100](5140/6372)  Time 4.3530(4.3866)  Loss 0.0106(0.0327)  Lr 0.00010000  Eta 1:30:04
[2024-03-20 23:51:25 pose_transfer_train.py:516] Train [1/100](5150/6372)  Time 4.2459(4.3867)  Loss 0.0033(0.0326)  Lr 0.00010000  Eta 1:29:20
[2024-03-20 23:52:09 pose_transfer_train.py:516] Train [1/100](5160/6372)  Time 4.1435(4.3868)  Loss 0.0550(0.0326)  Lr 0.00010000  Eta 1:28:36
[2024-03-20 23:52:53 pose_transfer_train.py:516] Train [1/100](5170/6372)  Time 4.6318(4.3868)  Loss 0.0202(0.0326)  Lr 0.00010000  Eta 1:27:52
[2024-03-20 23:53:38 pose_transfer_train.py:516] Train [1/100](5180/6372)  Time 4.1953(4.3871)  Loss 0.0137(0.0327)  Lr 0.00010000  Eta 1:27:09
[2024-03-20 23:54:23 pose_transfer_train.py:516] Train [1/100](5190/6372)  Time 3.6362(4.3872)  Loss 0.0792(0.0326)  Lr 0.00010000  Eta 1:26:25
[2024-03-20 23:55:08 pose_transfer_train.py:516] Train [1/100](5200/6372)  Time 4.5566(4.3874)  Loss 0.0120(0.0326)  Lr 0.00010000  Eta 1:25:42
[2024-03-20 23:55:49 pose_transfer_train.py:516] Train [1/100](5210/6372)  Time 3.8294(4.3869)  Loss 0.0176(0.0326)  Lr 0.00010000  Eta 1:24:57
[2024-03-20 23:56:34 pose_transfer_train.py:516] Train [1/100](5220/6372)  Time 3.9828(4.3870)  Loss 0.0041(0.0326)  Lr 0.00010000  Eta 1:24:13
[2024-03-20 23:57:18 pose_transfer_train.py:516] Train [1/100](5230/6372)  Time 4.5237(4.3871)  Loss 0.0375(0.0326)  Lr 0.00010000  Eta 1:23:30
[2024-03-20 23:58:02 pose_transfer_train.py:516] Train [1/100](5240/6372)  Time 4.6469(4.3871)  Loss 0.0119(0.0326)  Lr 0.00010000  Eta 1:22:46
[2024-03-20 23:58:46 pose_transfer_train.py:516] Train [1/100](5250/6372)  Time 4.1461(4.3872)  Loss 0.0362(0.0326)  Lr 0.00010000  Eta 1:22:02
[2024-03-20 23:59:29 pose_transfer_train.py:516] Train [1/100](5260/6372)  Time 4.4088(4.3870)  Loss 0.0047(0.0326)  Lr 0.00010000  Eta 1:21:18
[2024-03-21 00:00:13 pose_transfer_train.py:516] Train [1/100](5270/6372)  Time 4.4620(4.3871)  Loss 0.0589(0.0326)  Lr 0.00010000  Eta 1:20:34
[2024-03-21 00:00:58 pose_transfer_train.py:516] Train [1/100](5280/6372)  Time 4.4729(4.3873)  Loss 0.0090(0.0326)  Lr 0.00010000  Eta 1:19:50
[2024-03-21 00:01:41 pose_transfer_train.py:516] Train [1/100](5290/6372)  Time 3.7770(4.3871)  Loss 0.0138(0.0325)  Lr 0.00010000  Eta 1:19:06
[2024-03-21 00:02:25 pose_transfer_train.py:516] Train [1/100](5300/6372)  Time 4.4946(4.3872)  Loss 0.0504(0.0325)  Lr 0.00010000  Eta 1:18:23
[2024-03-21 00:03:08 pose_transfer_train.py:516] Train [1/100](5310/6372)  Time 4.0224(4.3869)  Loss 0.0097(0.0325)  Lr 0.00010000  Eta 1:17:38
[2024-03-21 00:03:49 pose_transfer_train.py:516] Train [1/100](5320/6372)  Time 4.3672(4.3865)  Loss 0.0809(0.0325)  Lr 0.00010000  Eta 1:16:54
[2024-03-21 00:04:32 pose_transfer_train.py:516] Train [1/100](5330/6372)  Time 4.1583(4.3862)  Loss 0.0373(0.0326)  Lr 0.00010000  Eta 1:16:10
[2024-03-21 00:05:14 pose_transfer_train.py:516] Train [1/100](5340/6372)  Time 3.9648(4.3860)  Loss 0.0272(0.0326)  Lr 0.00010000  Eta 1:15:26
[2024-03-21 00:05:58 pose_transfer_train.py:516] Train [1/100](5350/6372)  Time 4.1091(4.3859)  Loss 0.0310(0.0325)  Lr 0.00010000  Eta 1:14:42
[2024-03-21 00:06:40 pose_transfer_train.py:516] Train [1/100](5360/6372)  Time 4.4876(4.3856)  Loss 0.0098(0.0325)  Lr 0.00010000  Eta 1:13:58
[2024-03-21 00:07:23 pose_transfer_train.py:516] Train [1/100](5370/6372)  Time 4.1405(4.3855)  Loss 0.0028(0.0325)  Lr 0.00010000  Eta 1:13:14
[2024-03-21 00:08:08 pose_transfer_train.py:516] Train [1/100](5380/6372)  Time 4.7714(4.3856)  Loss 0.0166(0.0325)  Lr 0.00010000  Eta 1:12:30
[2024-03-21 00:08:53 pose_transfer_train.py:516] Train [1/100](5390/6372)  Time 4.3404(4.3858)  Loss 0.0642(0.0325)  Lr 0.00010000  Eta 1:11:46
[2024-03-21 00:09:37 pose_transfer_train.py:516] Train [1/100](5400/6372)  Time 4.2624(4.3859)  Loss 0.0077(0.0325)  Lr 0.00010000  Eta 1:11:03
[2024-03-21 00:10:20 pose_transfer_train.py:516] Train [1/100](5410/6372)  Time 4.2679(4.3858)  Loss 0.1313(0.0326)  Lr 0.00010000  Eta 1:10:19
[2024-03-21 00:11:03 pose_transfer_train.py:516] Train [1/100](5420/6372)  Time 4.2637(4.3856)  Loss 0.0068(0.0325)  Lr 0.00010000  Eta 1:09:35
[2024-03-21 00:11:47 pose_transfer_train.py:516] Train [1/100](5430/6372)  Time 4.3891(4.3856)  Loss 0.0597(0.0325)  Lr 0.00010000  Eta 1:08:51
[2024-03-21 00:12:32 pose_transfer_train.py:516] Train [1/100](5440/6372)  Time 4.2583(4.3857)  Loss 0.0114(0.0325)  Lr 0.00010000  Eta 1:08:07
[2024-03-21 00:13:17 pose_transfer_train.py:516] Train [1/100](5450/6372)  Time 4.8175(4.3861)  Loss 0.0566(0.0325)  Lr 0.00010000  Eta 1:07:23
[2024-03-21 00:14:01 pose_transfer_train.py:516] Train [1/100](5460/6372)  Time 4.5942(4.3860)  Loss 0.0100(0.0325)  Lr 0.00010000  Eta 1:06:40
[2024-03-21 00:14:46 pose_transfer_train.py:516] Train [1/100](5470/6372)  Time 4.7685(4.3862)  Loss 0.0096(0.0325)  Lr 0.00010000  Eta 1:05:56
[2024-03-21 00:15:30 pose_transfer_train.py:516] Train [1/100](5480/6372)  Time 4.5555(4.3863)  Loss 0.0493(0.0325)  Lr 0.00010000  Eta 1:05:12
[2024-03-21 00:16:14 pose_transfer_train.py:516] Train [1/100](5490/6372)  Time 4.3690(4.3864)  Loss 0.0522(0.0326)  Lr 0.00010000  Eta 1:04:28
[2024-03-21 00:17:00 pose_transfer_train.py:516] Train [1/100](5500/6372)  Time 4.6688(4.3866)  Loss 0.0019(0.0325)  Lr 0.00010000  Eta 1:03:45
[2024-03-21 00:17:43 pose_transfer_train.py:516] Train [1/100](5510/6372)  Time 4.6928(4.3865)  Loss 0.0122(0.0325)  Lr 0.00010000  Eta 1:03:01
[2024-03-21 00:18:28 pose_transfer_train.py:516] Train [1/100](5520/6372)  Time 4.6389(4.3867)  Loss 0.0488(0.0325)  Lr 0.00010000  Eta 1:02:17
[2024-03-21 00:19:12 pose_transfer_train.py:516] Train [1/100](5530/6372)  Time 4.4427(4.3867)  Loss 0.0029(0.0325)  Lr 0.00010000  Eta 1:01:33
[2024-03-21 00:19:56 pose_transfer_train.py:516] Train [1/100](5540/6372)  Time 3.8364(4.3867)  Loss 0.0807(0.0325)  Lr 0.00010000  Eta 1:00:49
[2024-03-21 00:20:40 pose_transfer_train.py:516] Train [1/100](5550/6372)  Time 3.9308(4.3867)  Loss 0.0574(0.0325)  Lr 0.00010000  Eta 1:00:05
[2024-03-21 00:21:25 pose_transfer_train.py:516] Train [1/100](5560/6372)  Time 4.3403(4.3870)  Loss 0.0733(0.0325)  Lr 0.00010000  Eta 0:59:22
[2024-03-21 00:22:10 pose_transfer_train.py:516] Train [1/100](5570/6372)  Time 4.5486(4.3872)  Loss 0.0018(0.0325)  Lr 0.00010000  Eta 0:58:38
[2024-03-21 00:22:55 pose_transfer_train.py:516] Train [1/100](5580/6372)  Time 4.3638(4.3875)  Loss 0.1016(0.0325)  Lr 0.00010000  Eta 0:57:54
[2024-03-21 00:23:39 pose_transfer_train.py:516] Train [1/100](5590/6372)  Time 4.4581(4.3874)  Loss 0.0447(0.0325)  Lr 0.00010000  Eta 0:57:10
[2024-03-21 00:24:22 pose_transfer_train.py:516] Train [1/100](5600/6372)  Time 4.8166(4.3872)  Loss 0.0300(0.0325)  Lr 0.00010000  Eta 0:56:26
[2024-03-21 00:25:06 pose_transfer_train.py:516] Train [1/100](5610/6372)  Time 4.3089(4.3873)  Loss 0.0047(0.0325)  Lr 0.00010000  Eta 0:55:43
[2024-03-21 00:25:49 pose_transfer_train.py:516] Train [1/100](5620/6372)  Time 4.2802(4.3871)  Loss 0.0821(0.0325)  Lr 0.00010000  Eta 0:54:59
[2024-03-21 00:26:35 pose_transfer_train.py:516] Train [1/100](5630/6372)  Time 4.7145(4.3876)  Loss 0.0138(0.0325)  Lr 0.00010000  Eta 0:54:15
[2024-03-21 00:27:20 pose_transfer_train.py:516] Train [1/100](5640/6372)  Time 4.6486(4.3878)  Loss 0.0557(0.0325)  Lr 0.00010000  Eta 0:53:31
[2024-03-21 00:28:03 pose_transfer_train.py:516] Train [1/100](5650/6372)  Time 4.0073(4.3876)  Loss 0.0222(0.0325)  Lr 0.00010000  Eta 0:52:47
[2024-03-21 00:28:46 pose_transfer_train.py:516] Train [1/100](5660/6372)  Time 4.0682(4.3874)  Loss 0.0043(0.0325)  Lr 0.00010000  Eta 0:52:03
[2024-03-21 00:29:28 pose_transfer_train.py:516] Train [1/100](5670/6372)  Time 4.2510(4.3871)  Loss 0.0235(0.0325)  Lr 0.00010000  Eta 0:51:19
[2024-03-21 00:30:14 pose_transfer_train.py:516] Train [1/100](5680/6372)  Time 4.7358(4.3874)  Loss 0.1392(0.0325)  Lr 0.00010000  Eta 0:50:36
[2024-03-21 00:31:00 pose_transfer_train.py:516] Train [1/100](5690/6372)  Time 4.7083(4.3878)  Loss 0.0068(0.0325)  Lr 0.00010000  Eta 0:49:52
[2024-03-21 00:31:44 pose_transfer_train.py:516] Train [1/100](5700/6372)  Time 4.3412(4.3878)  Loss 0.0249(0.0325)  Lr 0.00010000  Eta 0:49:08
[2024-03-21 00:32:29 pose_transfer_train.py:516] Train [1/100](5710/6372)  Time 4.6334(4.3880)  Loss 0.0189(0.0325)  Lr 0.00010000  Eta 0:48:24
[2024-03-21 00:33:15 pose_transfer_train.py:516] Train [1/100](5720/6372)  Time 4.6234(4.3884)  Loss 0.0218(0.0325)  Lr 0.00010000  Eta 0:47:41
[2024-03-21 00:33:59 pose_transfer_train.py:516] Train [1/100](5730/6372)  Time 4.1162(4.3884)  Loss 0.0378(0.0325)  Lr 0.00010000  Eta 0:46:57
[2024-03-21 00:34:43 pose_transfer_train.py:516] Train [1/100](5740/6372)  Time 4.5744(4.3885)  Loss 0.0868(0.0325)  Lr 0.00010000  Eta 0:46:13
[2024-03-21 00:35:26 pose_transfer_train.py:516] Train [1/100](5750/6372)  Time 4.4283(4.3884)  Loss 0.0319(0.0325)  Lr 0.00010000  Eta 0:45:29
[2024-03-21 00:36:08 pose_transfer_train.py:516] Train [1/100](5760/6372)  Time 4.2415(4.3880)  Loss 0.0218(0.0325)  Lr 0.00010000  Eta 0:44:45
[2024-03-21 00:36:53 pose_transfer_train.py:516] Train [1/100](5770/6372)  Time 4.4373(4.3881)  Loss 0.0038(0.0325)  Lr 0.00010000  Eta 0:44:01
[2024-03-21 00:37:39 pose_transfer_train.py:516] Train [1/100](5780/6372)  Time 4.4941(4.3885)  Loss 0.0145(0.0325)  Lr 0.00010000  Eta 0:43:18
[2024-03-21 00:38:23 pose_transfer_train.py:516] Train [1/100](5790/6372)  Time 4.3481(4.3885)  Loss 0.0141(0.0325)  Lr 0.00010000  Eta 0:42:34
[2024-03-21 00:39:06 pose_transfer_train.py:516] Train [1/100](5800/6372)  Time 4.5138(4.3885)  Loss 0.0121(0.0325)  Lr 0.00010000  Eta 0:41:50
[2024-03-21 00:39:52 pose_transfer_train.py:516] Train [1/100](5810/6372)  Time 5.1233(4.3888)  Loss 0.0016(0.0325)  Lr 0.00010000  Eta 0:41:06
[2024-03-21 00:40:38 pose_transfer_train.py:516] Train [1/100](5820/6372)  Time 3.8505(4.3890)  Loss 0.0165(0.0324)  Lr 0.00010000  Eta 0:40:22
[2024-03-21 00:41:22 pose_transfer_train.py:516] Train [1/100](5830/6372)  Time 4.3867(4.3891)  Loss 0.0210(0.0324)  Lr 0.00010000  Eta 0:39:38
[2024-03-21 00:42:07 pose_transfer_train.py:516] Train [1/100](5840/6372)  Time 4.5363(4.3894)  Loss 0.0105(0.0324)  Lr 0.00010000  Eta 0:38:55
[2024-03-21 00:42:50 pose_transfer_train.py:516] Train [1/100](5850/6372)  Time 4.6978(4.3893)  Loss 0.0069(0.0324)  Lr 0.00010000  Eta 0:38:11
[2024-03-21 00:43:35 pose_transfer_train.py:516] Train [1/100](5860/6372)  Time 3.9768(4.3894)  Loss 0.0388(0.0324)  Lr 0.00010000  Eta 0:37:27
[2024-03-21 00:44:18 pose_transfer_train.py:516] Train [1/100](5870/6372)  Time 4.4213(4.3893)  Loss 0.0304(0.0324)  Lr 0.00010000  Eta 0:36:43
[2024-03-21 00:45:04 pose_transfer_train.py:516] Train [1/100](5880/6372)  Time 4.4458(4.3896)  Loss 0.0415(0.0324)  Lr 0.00010000  Eta 0:35:59
[2024-03-21 00:45:48 pose_transfer_train.py:516] Train [1/100](5890/6372)  Time 4.3189(4.3897)  Loss 0.0340(0.0324)  Lr 0.00010000  Eta 0:35:15
[2024-03-21 00:46:32 pose_transfer_train.py:516] Train [1/100](5900/6372)  Time 4.6039(4.3896)  Loss 0.0220(0.0324)  Lr 0.00010000  Eta 0:34:31
[2024-03-21 00:47:16 pose_transfer_train.py:516] Train [1/100](5910/6372)  Time 4.7409(4.3896)  Loss 0.0188(0.0324)  Lr 0.00010000  Eta 0:33:47
[2024-03-21 00:48:00 pose_transfer_train.py:516] Train [1/100](5920/6372)  Time 3.9576(4.3896)  Loss 0.0056(0.0324)  Lr 0.00010000  Eta 0:33:04
[2024-03-21 00:48:44 pose_transfer_train.py:516] Train [1/100](5930/6372)  Time 4.1937(4.3896)  Loss 0.0344(0.0324)  Lr 0.00010000  Eta 0:32:20
[2024-03-21 00:49:29 pose_transfer_train.py:516] Train [1/100](5940/6372)  Time 4.3378(4.3898)  Loss 0.0388(0.0324)  Lr 0.00010000  Eta 0:31:36
[2024-03-21 00:50:12 pose_transfer_train.py:516] Train [1/100](5950/6372)  Time 4.2376(4.3897)  Loss 0.0465(0.0324)  Lr 0.00010000  Eta 0:30:52
[2024-03-21 00:50:58 pose_transfer_train.py:516] Train [1/100](5960/6372)  Time 4.5475(4.3900)  Loss 0.0042(0.0324)  Lr 0.00010000  Eta 0:30:08
[2024-03-21 00:51:43 pose_transfer_train.py:516] Train [1/100](5970/6372)  Time 4.5580(4.3902)  Loss 0.0277(0.0324)  Lr 0.00010000  Eta 0:29:24
[2024-03-21 00:52:28 pose_transfer_train.py:516] Train [1/100](5980/6372)  Time 4.7377(4.3905)  Loss 0.0322(0.0324)  Lr 0.00010000  Eta 0:28:41
[2024-03-21 00:53:12 pose_transfer_train.py:516] Train [1/100](5990/6372)  Time 4.2907(4.3904)  Loss 0.0080(0.0324)  Lr 0.00010000  Eta 0:27:57
[2024-03-21 00:53:56 pose_transfer_train.py:516] Train [1/100](6000/6372)  Time 4.3361(4.3905)  Loss 0.1214(0.0324)  Lr 0.00010000  Eta 0:27:13
[2024-03-21 00:54:40 pose_transfer_train.py:516] Train [1/100](6010/6372)  Time 4.3172(4.3904)  Loss 0.0077(0.0324)  Lr 0.00010000  Eta 0:26:29
[2024-03-21 00:55:24 pose_transfer_train.py:516] Train [1/100](6020/6372)  Time 4.2060(4.3905)  Loss 0.0225(0.0324)  Lr 0.00010000  Eta 0:25:45
[2024-03-21 00:56:08 pose_transfer_train.py:516] Train [1/100](6030/6372)  Time 4.3934(4.3904)  Loss 0.0119(0.0324)  Lr 0.00010000  Eta 0:25:01
[2024-03-21 00:56:52 pose_transfer_train.py:516] Train [1/100](6040/6372)  Time 4.5823(4.3906)  Loss 0.0412(0.0324)  Lr 0.00010000  Eta 0:24:17
[2024-03-21 00:57:37 pose_transfer_train.py:516] Train [1/100](6050/6372)  Time 4.3964(4.3907)  Loss 0.0189(0.0324)  Lr 0.00010000  Eta 0:23:33
[2024-03-21 00:58:20 pose_transfer_train.py:516] Train [1/100](6060/6372)  Time 3.9638(4.3906)  Loss 0.0245(0.0324)  Lr 0.00010000  Eta 0:22:49
[2024-03-21 00:59:05 pose_transfer_train.py:516] Train [1/100](6070/6372)  Time 4.5474(4.3907)  Loss 0.0970(0.0324)  Lr 0.00010000  Eta 0:22:05
[2024-03-21 00:59:50 pose_transfer_train.py:516] Train [1/100](6080/6372)  Time 4.3748(4.3909)  Loss 0.0290(0.0324)  Lr 0.00010000  Eta 0:21:22
[2024-03-21 01:00:36 pose_transfer_train.py:516] Train [1/100](6090/6372)  Time 4.2282(4.3913)  Loss 0.0028(0.0324)  Lr 0.00010000  Eta 0:20:38
[2024-03-21 01:01:22 pose_transfer_train.py:516] Train [1/100](6100/6372)  Time 4.2429(4.3915)  Loss 0.0745(0.0324)  Lr 0.00010000  Eta 0:19:54
[2024-03-21 01:02:07 pose_transfer_train.py:516] Train [1/100](6110/6372)  Time 4.4245(4.3917)  Loss 0.0140(0.0324)  Lr 0.00010000  Eta 0:19:10
[2024-03-21 01:02:52 pose_transfer_train.py:516] Train [1/100](6120/6372)  Time 4.4777(4.3920)  Loss 0.0699(0.0325)  Lr 0.00010000  Eta 0:18:26
[2024-03-21 01:03:36 pose_transfer_train.py:516] Train [1/100](6130/6372)  Time 4.4786(4.3919)  Loss 0.0167(0.0324)  Lr 0.00010000  Eta 0:17:42
[2024-03-21 01:04:20 pose_transfer_train.py:516] Train [1/100](6140/6372)  Time 4.4658(4.3919)  Loss 0.0013(0.0324)  Lr 0.00010000  Eta 0:16:58
[2024-03-21 01:05:03 pose_transfer_train.py:516] Train [1/100](6150/6372)  Time 4.9860(4.3918)  Loss 0.0292(0.0324)  Lr 0.00010000  Eta 0:16:14
[2024-03-21 01:05:46 pose_transfer_train.py:516] Train [1/100](6160/6372)  Time 4.3792(4.3917)  Loss 0.0097(0.0324)  Lr 0.00010000  Eta 0:15:31
[2024-03-21 01:06:29 pose_transfer_train.py:516] Train [1/100](6170/6372)  Time 4.1790(4.3916)  Loss 0.0504(0.0324)  Lr 0.00010000  Eta 0:14:47
[2024-03-21 01:07:15 pose_transfer_train.py:516] Train [1/100](6180/6372)  Time 4.6568(4.3918)  Loss 0.0365(0.0324)  Lr 0.00010000  Eta 0:14:03
[2024-03-21 01:08:00 pose_transfer_train.py:516] Train [1/100](6190/6372)  Time 4.6250(4.3921)  Loss 0.0184(0.0324)  Lr 0.00010000  Eta 0:13:19
[2024-03-21 01:08:46 pose_transfer_train.py:516] Train [1/100](6200/6372)  Time 4.4874(4.3924)  Loss 0.0433(0.0324)  Lr 0.00010000  Eta 0:12:35
[2024-03-21 01:09:29 pose_transfer_train.py:516] Train [1/100](6210/6372)  Time 4.9740(4.3923)  Loss 0.0232(0.0324)  Lr 0.00010000  Eta 0:11:51
[2024-03-21 01:10:14 pose_transfer_train.py:516] Train [1/100](6220/6372)  Time 4.3944(4.3925)  Loss 0.0407(0.0324)  Lr 0.00010000  Eta 0:11:07
[2024-03-21 01:10:59 pose_transfer_train.py:516] Train [1/100](6230/6372)  Time 4.5328(4.3925)  Loss 0.0306(0.0324)  Lr 0.00010000  Eta 0:10:23
[2024-03-21 01:11:45 pose_transfer_train.py:516] Train [1/100](6240/6372)  Time 4.5693(4.3928)  Loss 0.0333(0.0324)  Lr 0.00010000  Eta 0:09:39
[2024-03-21 01:12:30 pose_transfer_train.py:516] Train [1/100](6250/6372)  Time 4.0878(4.3930)  Loss 0.0123(0.0324)  Lr 0.00010000  Eta 0:08:55
[2024-03-21 01:13:15 pose_transfer_train.py:516] Train [1/100](6260/6372)  Time 4.2696(4.3932)  Loss 0.0357(0.0324)  Lr 0.00010000  Eta 0:08:12
[2024-03-21 01:14:00 pose_transfer_train.py:516] Train [1/100](6270/6372)  Time 4.6858(4.3935)  Loss 0.0239(0.0324)  Lr 0.00010000  Eta 0:07:28
[2024-03-21 01:14:45 pose_transfer_train.py:516] Train [1/100](6280/6372)  Time 4.3699(4.3936)  Loss 0.0160(0.0324)  Lr 0.00010000  Eta 0:06:44
[2024-03-21 01:15:29 pose_transfer_train.py:516] Train [1/100](6290/6372)  Time 4.4676(4.3936)  Loss 0.0922(0.0325)  Lr 0.00010000  Eta 0:06:00
[2024-03-21 01:16:15 pose_transfer_train.py:516] Train [1/100](6300/6372)  Time 4.6655(4.3939)  Loss 0.0547(0.0325)  Lr 0.00010000  Eta 0:05:16
[2024-03-21 01:16:59 pose_transfer_train.py:516] Train [1/100](6310/6372)  Time 4.4416(4.3940)  Loss 0.0391(0.0325)  Lr 0.00010000  Eta 0:04:32
[2024-03-21 01:17:44 pose_transfer_train.py:516] Train [1/100](6320/6372)  Time 4.6659(4.3941)  Loss 0.0871(0.0325)  Lr 0.00010000  Eta 0:03:48
[2024-03-21 01:18:28 pose_transfer_train.py:516] Train [1/100](6330/6372)  Time 4.6886(4.3941)  Loss 0.0842(0.0324)  Lr 0.00010000  Eta 0:03:04
[2024-03-21 01:19:12 pose_transfer_train.py:516] Train [1/100](6340/6372)  Time 4.7722(4.3940)  Loss 0.0037(0.0325)  Lr 0.00010000  Eta 0:02:20
[2024-03-21 01:19:56 pose_transfer_train.py:516] Train [1/100](6350/6372)  Time 4.7793(4.3942)  Loss 0.0239(0.0324)  Lr 0.00010000  Eta 0:01:36
[2024-03-21 01:20:44 pose_transfer_train.py:516] Train [1/100](6360/6372)  Time 4.8341(4.3947)  Loss 0.0420(0.0324)  Lr 0.00010000  Eta 0:00:52
[2024-03-21 01:21:28 pose_transfer_train.py:516] Train [1/100](6370/6372)  Time 4.6770(4.3948)  Loss 0.0351(0.0324)  Lr 0.00010000  Eta 0:00:08
[2024-03-21 01:21:37 pose_transfer_train.py:516] Train [1/100](6372/6372)  Time 5.2367(4.3949)  Loss 0.0258(0.0324)  Lr 0.00010000  Eta 0:00:00
[2024-03-21 01:21:38 pose_transfer_train.py:526] epoch 1 finished, running time 7:46:44
[2024-03-21 01:21:38 pose_transfer_train.py:414] epoch 2 start
[2024-03-21 01:22:27 pose_transfer_train.py:516] Train [2/100](10/6372)  Time 4.6183(4.9748)  Loss 0.0108(0.0359)  Lr 0.00010000  Eta 8:47:29
[2024-03-21 01:23:11 pose_transfer_train.py:516] Train [2/100](20/6372)  Time 4.7554(4.6598)  Loss 0.0070(0.0321)  Lr 0.00010000  Eta 8:13:19
[2024-03-21 01:23:55 pose_transfer_train.py:516] Train [2/100](30/6372)  Time 4.4699(4.5787)  Loss 0.0625(0.0355)  Lr 0.00010000  Eta 8:03:58
[2024-03-21 01:24:40 pose_transfer_train.py:516] Train [2/100](40/6372)  Time 3.8712(4.5554)  Loss 0.0107(0.0352)  Lr 0.00010000  Eta 8:00:44
[2024-03-21 01:25:24 pose_transfer_train.py:516] Train [2/100](50/6372)  Time 4.5432(4.5228)  Loss 0.0146(0.0336)  Lr 0.00010000  Eta 7:56:32
[2024-03-21 01:26:09 pose_transfer_train.py:516] Train [2/100](60/6372)  Time 4.4380(4.5317)  Loss 0.0364(0.0327)  Lr 0.00010000  Eta 7:56:44
[2024-03-21 01:26:53 pose_transfer_train.py:516] Train [2/100](70/6372)  Time 4.2619(4.5145)  Loss 0.0124(0.0326)  Lr 0.00010000  Eta 7:54:10
[2024-03-21 01:27:38 pose_transfer_train.py:516] Train [2/100](80/6372)  Time 4.7072(4.5086)  Loss 0.0119(0.0309)  Lr 0.00010000  Eta 7:52:48
[2024-03-21 01:28:23 pose_transfer_train.py:516] Train [2/100](90/6372)  Time 4.6748(4.5045)  Loss 0.0040(0.0291)  Lr 0.00010000  Eta 7:51:37
[2024-03-21 01:29:07 pose_transfer_train.py:516] Train [2/100](100/6372)  Time 4.4927(4.4990)  Loss 0.0535(0.0285)  Lr 0.00010000  Eta 7:50:17
[2024-03-21 01:29:52 pose_transfer_train.py:516] Train [2/100](110/6372)  Time 4.1060(4.4956)  Loss 0.0249(0.0286)  Lr 0.00010000  Eta 7:49:11
[2024-03-21 01:30:36 pose_transfer_train.py:516] Train [2/100](120/6372)  Time 4.6084(4.4899)  Loss 0.0143(0.0298)  Lr 0.00010000  Eta 7:47:50
[2024-03-21 01:31:20 pose_transfer_train.py:516] Train [2/100](130/6372)  Time 4.2137(4.4805)  Loss 0.0618(0.0301)  Lr 0.00010000  Eta 7:46:07
[2024-03-21 01:32:03 pose_transfer_train.py:516] Train [2/100](140/6372)  Time 4.7138(4.4685)  Loss 0.0450(0.0299)  Lr 0.00010000  Eta 7:44:07
[2024-03-21 01:32:48 pose_transfer_train.py:516] Train [2/100](150/6372)  Time 4.5941(4.4732)  Loss 0.0487(0.0306)  Lr 0.00010000  Eta 7:43:52
[2024-03-21 01:33:34 pose_transfer_train.py:516] Train [2/100](160/6372)  Time 4.0196(4.4764)  Loss 0.0621(0.0308)  Lr 0.00010000  Eta 7:43:27
[2024-03-21 01:34:20 pose_transfer_train.py:516] Train [2/100](170/6372)  Time 4.3558(4.4826)  Loss 0.0572(0.0318)  Lr 0.00010000  Eta 7:43:21
[2024-03-21 01:35:04 pose_transfer_train.py:516] Train [2/100](180/6372)  Time 4.6226(4.4828)  Loss 0.0116(0.0318)  Lr 0.00010000  Eta 7:42:37
[2024-03-21 01:35:47 pose_transfer_train.py:516] Train [2/100](190/6372)  Time 4.1653(4.4688)  Loss 0.0250(0.0318)  Lr 0.00010000  Eta 7:40:26
[2024-03-21 01:36:29 pose_transfer_train.py:516] Train [2/100](200/6372)  Time 4.6232(4.4599)  Loss 0.0844(0.0328)  Lr 0.00010000  Eta 7:38:46
[2024-03-21 01:37:15 pose_transfer_train.py:516] Train [2/100](210/6372)  Time 4.3356(4.4621)  Loss 0.0126(0.0323)  Lr 0.00010000  Eta 7:38:15
[2024-03-21 01:37:58 pose_transfer_train.py:516] Train [2/100](220/6372)  Time 4.5563(4.4572)  Loss 0.0137(0.0320)  Lr 0.00010000  Eta 7:37:00
[2024-03-21 01:38:41 pose_transfer_train.py:516] Train [2/100](230/6372)  Time 4.4650(4.4514)  Loss 0.0012(0.0314)  Lr 0.00010000  Eta 7:35:40
[2024-03-21 01:39:27 pose_transfer_train.py:516] Train [2/100](240/6372)  Time 4.6229(4.4543)  Loss 0.0641(0.0311)  Lr 0.00010000  Eta 7:35:13
[2024-03-21 01:40:12 pose_transfer_train.py:516] Train [2/100](250/6372)  Time 4.3899(4.4576)  Loss 0.0181(0.0311)  Lr 0.00010000  Eta 7:34:49
[2024-03-21 01:40:56 pose_transfer_train.py:516] Train [2/100](260/6372)  Time 4.2170(4.4570)  Loss 0.0288(0.0315)  Lr 0.00010000  Eta 7:34:01
[2024-03-21 01:41:39 pose_transfer_train.py:516] Train [2/100](270/6372)  Time 3.8433(4.4498)  Loss 0.0353(0.0318)  Lr 0.00010000  Eta 7:32:32
[2024-03-21 01:42:23 pose_transfer_train.py:516] Train [2/100](280/6372)  Time 4.6418(4.4473)  Loss 0.0042(0.0323)  Lr 0.00010000  Eta 7:31:33
[2024-03-21 01:43:08 pose_transfer_train.py:516] Train [2/100](290/6372)  Time 4.6564(4.4488)  Loss 0.0527(0.0323)  Lr 0.00010000  Eta 7:30:57
[2024-03-21 01:43:51 pose_transfer_train.py:516] Train [2/100](300/6372)  Time 3.7116(4.4450)  Loss 0.0021(0.0320)  Lr 0.00010000  Eta 7:29:50
[2024-03-21 01:44:34 pose_transfer_train.py:516] Train [2/100](310/6372)  Time 4.4528(4.4418)  Loss 0.0109(0.0321)  Lr 0.00010000  Eta 7:28:46
[2024-03-21 01:45:20 pose_transfer_train.py:516] Train [2/100](320/6372)  Time 4.5684(4.4463)  Loss 0.0162(0.0321)  Lr 0.00010000  Eta 7:28:28
[2024-03-21 01:46:06 pose_transfer_train.py:516] Train [2/100](330/6372)  Time 4.3936(4.4509)  Loss 0.0184(0.0320)  Lr 0.00010000  Eta 7:28:12
[2024-03-21 01:46:50 pose_transfer_train.py:516] Train [2/100](340/6372)  Time 4.3428(4.4478)  Loss 0.0155(0.0323)  Lr 0.00010000  Eta 7:27:08
[2024-03-21 01:47:34 pose_transfer_train.py:516] Train [2/100](350/6372)  Time 4.3661(4.4483)  Loss 0.0077(0.0322)  Lr 0.00010000  Eta 7:26:27
[2024-03-21 01:48:17 pose_transfer_train.py:516] Train [2/100](360/6372)  Time 4.5507(4.4423)  Loss 0.0483(0.0325)  Lr 0.00010000  Eta 7:25:06
[2024-03-21 01:49:02 pose_transfer_train.py:516] Train [2/100](370/6372)  Time 4.5511(4.4437)  Loss 0.0193(0.0324)  Lr 0.00010000  Eta 7:24:30
[2024-03-21 01:49:44 pose_transfer_train.py:516] Train [2/100](380/6372)  Time 4.1158(4.4390)  Loss 0.0262(0.0323)  Lr 0.00010000  Eta 7:23:18
[2024-03-21 01:50:30 pose_transfer_train.py:516] Train [2/100](390/6372)  Time 4.2398(4.4418)  Loss 0.0478(0.0322)  Lr 0.00010000  Eta 7:22:50
[2024-03-21 01:51:15 pose_transfer_train.py:516] Train [2/100](400/6372)  Time 4.7416(4.4446)  Loss 0.0185(0.0319)  Lr 0.00010000  Eta 7:22:23
[2024-03-21 01:51:59 pose_transfer_train.py:516] Train [2/100](410/6372)  Time 4.8633(4.4430)  Loss 0.0513(0.0316)  Lr 0.00010000  Eta 7:21:29
[2024-03-21 01:52:43 pose_transfer_train.py:516] Train [2/100](420/6372)  Time 4.6315(4.4404)  Loss 0.0381(0.0317)  Lr 0.00010000  Eta 7:20:29
[2024-03-21 01:53:28 pose_transfer_train.py:516] Train [2/100](430/6372)  Time 4.5363(4.4421)  Loss 0.0164(0.0316)  Lr 0.00010000  Eta 7:19:54
[2024-03-21 01:54:11 pose_transfer_train.py:516] Train [2/100](440/6372)  Time 4.2623(4.4409)  Loss 0.0353(0.0314)  Lr 0.00010000  Eta 7:19:03
[2024-03-21 01:54:56 pose_transfer_train.py:516] Train [2/100](450/6372)  Time 4.7692(4.4420)  Loss 0.0428(0.0314)  Lr 0.00010000  Eta 7:18:25
[2024-03-21 01:55:39 pose_transfer_train.py:516] Train [2/100](460/6372)  Time 4.7672(4.4390)  Loss 0.0173(0.0312)  Lr 0.00010000  Eta 7:17:23
[2024-03-21 01:56:25 pose_transfer_train.py:516] Train [2/100](470/6372)  Time 4.4045(4.4417)  Loss 0.1350(0.0316)  Lr 0.00010000  Eta 7:16:54
[2024-03-21 01:57:11 pose_transfer_train.py:516] Train [2/100](480/6372)  Time 5.0625(4.4439)  Loss 0.0108(0.0313)  Lr 0.00010000  Eta 7:16:23
[2024-03-21 01:57:57 pose_transfer_train.py:516] Train [2/100](490/6372)  Time 4.4312(4.4485)  Loss 0.0126(0.0311)  Lr 0.00010000  Eta 7:16:06
[2024-03-21 01:58:42 pose_transfer_train.py:516] Train [2/100](500/6372)  Time 4.6834(4.4488)  Loss 0.0755(0.0313)  Lr 0.00010000  Eta 7:15:23
[2024-03-21 02:02:49 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 1
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 64
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-21 02:02:49 pose_transfer_train.py:283] preparing datasets...
[2024-03-21 02:03:08 pose_transfer_train.py:306] preparing model...
[2024-03-21 02:03:15 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-21 02:03:39 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-21 02:03:39 pose_transfer_train.py:367] preparing optimizer...
[2024-03-21 02:03:39 pose_transfer_train.py:373] preparing accelerator...
[2024-03-21 02:03:43 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-21 02:03:44 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-21 02:03:49 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-21 02:03:49 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-21 02:03:49 pose_transfer_train.py:405] start training...
[2024-03-21 02:03:49 pose_transfer_train.py:414] epoch 1 start
[2024-03-21 02:04:58 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
[2024-03-21 02:06:33 pose_transfer_train.py:516] Train [1/100](10/1618)  Time 10.9370(16.3377)  Loss 0.0464(0.0390)  Lr 0.00001099  Eta 7:17:51
[2024-03-21 02:08:52 pose_transfer_train.py:281] running with config:
ACCELERATE:
  ALLOW_TF32: True
  EVAL_PERIOD: 1
  GRADIENT_ACCUMULATION_STEPS: 1
  LOG_PERIOD: 10
  MIXED_PRECISION: fp16
  PROJECT_NAME: CFLD
  RUN_NAME: debug
  SEED: 3407
INPUT:
  BATCH_SIZE: 64
  COND:
    IMG_SIZE: [256, 256]
    MASK_PATCH_SIZE: 8
    MIN_SCALE: 1.0
    PRED_ASPECT_RATIO: [0.3, 3.3333333333333335]
    PRED_RATIO: []
    PRED_RATIO_VAR: []
  GT:
    IMG_SIZE: [512, 512]
  NUM_WORKERS: 8
  POSE:
    IMG_SIZE: [256, 256]
  ROOT_DIR: ./fashion
MODEL:
  APPEARANCE_GUIDANCE_CONFIG:
    ATTN_RESIDUAL_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    CONVIN_KERNEL_SIZE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CONVIN_PADDING: [0, 0, 0, 0, 0, 0, 0, 0, 0]
    CONVIN_STRIDE: [1, 1, 1, 1, 1, 1, 1, 1, 1]
    CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
    DEPTH: 4
    DETACH_INPUT: False
    EMBED_DIMS: [64, 64, 64, 128, 128, 128, 256, 256, 256]
    HEADS: [2, 2, 2, 4, 4, 4, 8, 8, 8]
    INNER_DIMS: [128, 128, 128, 256, 256, 256, 512, 512, 512]
    TO_KEYS: False
    TO_QUERIES: True
    TO_SELF_ATTN: False
    TO_VALUES: False
  COND_STAGE_CONFIG:
    CLIP_ENCODER:
      CTX_DIM: 768
      N_CTX: 50
      PRETRAINED_PATH: pretrained_models/openai/clip-vit-base-patch32
    DEPTHS: [2, 2, 18, 2]
    DROP_PATH_RATE: 0.2
    EMBED_DIM: 128
    LAST_NORM: False
    NUM_HEADS: [4, 8, 16, 32]
    PRETRAINED_PATH: pretrained_models/swin/swin_base_patch4_window12_384_22kto1k.pth
    RIE:
      CTX_DIMS: [320, 320, 320, 640, 640, 640, 1280, 1280, 1280]
      PRETRAINED_PATH: 
    WINDOW_SIZE: 16
    ZERO_CROSS_ATTENTION:
      USE_ATV_LOSS: False
      USE_ZERO_CROSS_ATTN: True
  DECODER_CONFIG:
    CTX_DIM: 768
    DEPTH: 8
    HEADS: 24
    N_CTX: 16
    POSE_QUERY: False
  FIRST_STAGE_CONFIG:
    PRETRAINED_PATH: pretrained_models/vae
  LAST_EPOCH: 0
  POSE_GUIDANCE_CONFIG:
    CHANNELS: [320, 640, 1280]
    DOWNSCALE_FACTOR: 4
    IN_CHANNELS: 320
    POSE_CHANNELS: 21
  PRETRAINED_PATH: ./checkpoints/
  SCHEDULER_CONFIG:
    CUBIC_SAMPLING: True
    NAME: ddpm
    PRETRAINED_PATH: pretrained_models/scheduler
  UNET_CONFIG:
    PRETRAINED_PATH: pretrained_models/unet
    TRAINABLE_BLOCK_IDX: [11, 10, 9, 8, 7, 6, 5, 4, 3]
    TRAIN_CROSS_ATTN_K: True
    TRAIN_CROSS_ATTN_Q: False
    TRAIN_CROSS_ATTN_V: True
    TRAIN_SELF_ATTN_K: False
    TRAIN_SELF_ATTN_Q: False
    TRAIN_SELF_ATTN_V: False
  U_COND_DOWN_BLOCK_GUIDANCE: False
  U_COND_PERCENT: 0.2
  U_COND_UP_BLOCK_GUIDANCE: False
OPTIMIZER:
  DECAY_EPOCHS: [50]
  DECAY_RATE: 0.1
  EPOCHS: 100
  LR: 0.0001
  NAME: adam
  OVERRIDE_LR: 0.0
  SCALE_LR: False
  WARMUP_RATE: 0.1
  WARMUP_STEPS: 1000
TEST:
  ALL_BLOCK_GUIDANCE_SCALE: 2.0
  DDIM_INVERSION_DOWN_BLOCK_GUIDANCE: False
  DDIM_INVERSION_STEPS: 0
  DDIM_INVERSION_UNCONDITIONAL: True
  DDIM_INVERSION_UP_BLOCK_GUIDANCE: False
  DOWN_BLOCK_GUIDANCE_SCALE: 2.0
  FULL_GUIDANCE_SCALE: 2.0
  GUIDANCE_SCALE: 2.0
  GUIDANCE_TYPE: uc_down_full
  IMG_SIZE: [256, 176]
  MICRO_BATCH_SIZE: 16
  NUM_INFERENCE_STEPS: 50
  NUM_WORKERS: 8
  UP_BLOCK_GUIDANCE_SCALE: 2.0
[2024-03-21 02:08:52 pose_transfer_train.py:283] preparing datasets...
[2024-03-21 02:09:13 pose_transfer_train.py:306] preparing model...
[2024-03-21 02:09:21 swin_transformer.py:702] _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=['norm.weight', 'norm.bias', 'head.weight', 'head.bias'])
[2024-03-21 02:09:44 pose_transfer_train.py:352] number of trainable parameters: 73112832
[2024-03-21 02:09:44 pose_transfer_train.py:367] preparing optimizer...
[2024-03-21 02:09:44 pose_transfer_train.py:373] preparing accelerator...
[2024-03-21 02:09:48 pose_transfer_train.py:380] loading states from ./checkpoints/
[2024-03-21 02:09:49 pose_transfer_train.py:390] _IncompatibleKeys(missing_keys=['learnable_vector', 'image_encoder_g.vision_model.embeddings.class_embedding', 'image_encoder_g.vision_model.embeddings.patch_embedding.weight', 'image_encoder_g.vision_model.embeddings.position_embedding.weight', 'image_encoder_g.vision_model.pre_layrnorm.weight', 'image_encoder_g.vision_model.pre_layrnorm.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight', 'image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight', 'image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias', 'image_encoder_g.vision_model.post_layernorm.weight', 'image_encoder_g.vision_model.post_layernorm.bias', 'image_encoder_g.visual_projection.weight'], unexpected_keys=[])
[2024-03-21 02:09:55 pose_transfer_train.py:393] _IncompatibleKeys(missing_keys=['model.up_blocks.1.warpflows.0.norm.weight', 'model.up_blocks.1.warpflows.0.norm.bias', 'model.up_blocks.1.warpflows.0.proj_in.weight', 'model.up_blocks.1.warpflows.0.proj_in.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.0.proj_out.weight', 'model.up_blocks.1.warpflows.0.proj_out.bias', 'model.up_blocks.1.warpflows.1.norm.weight', 'model.up_blocks.1.warpflows.1.norm.bias', 'model.up_blocks.1.warpflows.1.proj_in.weight', 'model.up_blocks.1.warpflows.1.proj_in.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.1.proj_out.weight', 'model.up_blocks.1.warpflows.1.proj_out.bias', 'model.up_blocks.1.warpflows.2.norm.weight', 'model.up_blocks.1.warpflows.2.norm.bias', 'model.up_blocks.1.warpflows.2.proj_in.weight', 'model.up_blocks.1.warpflows.2.proj_in.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.1.warpflows.2.proj_out.weight', 'model.up_blocks.1.warpflows.2.proj_out.bias', 'model.up_blocks.1.warpzc.0.weight', 'model.up_blocks.1.warpzc.0.bias', 'model.up_blocks.1.warpzc.1.weight', 'model.up_blocks.1.warpzc.1.bias', 'model.up_blocks.1.warpzc.2.weight', 'model.up_blocks.1.warpzc.2.bias', 'model.up_blocks.2.warpflows.0.norm.weight', 'model.up_blocks.2.warpflows.0.norm.bias', 'model.up_blocks.2.warpflows.0.proj_in.weight', 'model.up_blocks.2.warpflows.0.proj_in.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.0.proj_out.weight', 'model.up_blocks.2.warpflows.0.proj_out.bias', 'model.up_blocks.2.warpflows.1.norm.weight', 'model.up_blocks.2.warpflows.1.norm.bias', 'model.up_blocks.2.warpflows.1.proj_in.weight', 'model.up_blocks.2.warpflows.1.proj_in.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.1.proj_out.weight', 'model.up_blocks.2.warpflows.1.proj_out.bias', 'model.up_blocks.2.warpflows.2.norm.weight', 'model.up_blocks.2.warpflows.2.norm.bias', 'model.up_blocks.2.warpflows.2.proj_in.weight', 'model.up_blocks.2.warpflows.2.proj_in.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.2.warpflows.2.proj_out.weight', 'model.up_blocks.2.warpflows.2.proj_out.bias', 'model.up_blocks.2.warpzc.0.weight', 'model.up_blocks.2.warpzc.0.bias', 'model.up_blocks.2.warpzc.1.weight', 'model.up_blocks.2.warpzc.1.bias', 'model.up_blocks.2.warpzc.2.weight', 'model.up_blocks.2.warpzc.2.bias', 'model.up_blocks.3.warpflows.0.norm.weight', 'model.up_blocks.3.warpflows.0.norm.bias', 'model.up_blocks.3.warpflows.0.proj_in.weight', 'model.up_blocks.3.warpflows.0.proj_in.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.0.proj_out.weight', 'model.up_blocks.3.warpflows.0.proj_out.bias', 'model.up_blocks.3.warpflows.1.norm.weight', 'model.up_blocks.3.warpflows.1.norm.bias', 'model.up_blocks.3.warpflows.1.proj_in.weight', 'model.up_blocks.3.warpflows.1.proj_in.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.1.proj_out.weight', 'model.up_blocks.3.warpflows.1.proj_out.bias', 'model.up_blocks.3.warpflows.2.norm.weight', 'model.up_blocks.3.warpflows.2.norm.bias', 'model.up_blocks.3.warpflows.2.proj_in.weight', 'model.up_blocks.3.warpflows.2.proj_in.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight', 'model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias', 'model.up_blocks.3.warpflows.2.proj_out.weight', 'model.up_blocks.3.warpflows.2.proj_out.bias', 'model.up_blocks.3.warpzc.0.weight', 'model.up_blocks.3.warpzc.0.bias', 'model.up_blocks.3.warpzc.1.weight', 'model.up_blocks.3.warpzc.1.bias', 'model.up_blocks.3.warpzc.2.weight', 'model.up_blocks.3.warpzc.2.bias'], unexpected_keys=[])
[2024-03-21 02:09:55 pose_transfer_train.py:399] preparing lr scheduler...
[2024-03-21 02:09:55 pose_transfer_train.py:405] start training...
[2024-03-21 02:09:55 pose_transfer_train.py:414] epoch 1 start
[2024-03-21 02:11:00 distributed.py:1140] Reducer buckets have been rebuilt in this iteration.
[2024-03-21 02:12:39 pose_transfer_train.py:516] Train [1/100](10/1618)  Time 11.5081(16.3868)  Loss 0.0464(0.0390)  Lr 0.00001099  Eta 7:19:10
[2024-03-21 02:14:31 pose_transfer_train.py:516] Train [1/100](20/1618)  Time 11.4664(13.7841)  Loss 0.0375(0.0427)  Lr 0.00001189  Eta 6:07:06
[2024-03-21 02:16:22 pose_transfer_train.py:516] Train [1/100](30/1618)  Time 10.5278(12.8982)  Loss 0.0568(0.0423)  Lr 0.00001279  Eta 5:41:22
[2024-03-21 02:18:13 pose_transfer_train.py:516] Train [1/100](40/1618)  Time 11.1631(12.4618)  Loss 0.0360(0.0402)  Lr 0.00001369  Eta 5:27:44
[2024-03-21 02:20:04 pose_transfer_train.py:516] Train [1/100](50/1618)  Time 11.3252(12.1805)  Loss 0.0281(0.0386)  Lr 0.00001459  Eta 5:18:19
[2024-03-21 02:21:56 pose_transfer_train.py:516] Train [1/100](60/1618)  Time 10.5610(12.0227)  Loss 0.0289(0.0376)  Lr 0.00001549  Eta 5:12:11
[2024-03-21 02:23:46 pose_transfer_train.py:516] Train [1/100](70/1618)  Time 11.2393(11.8762)  Loss 0.0405(0.0374)  Lr 0.00001639  Eta 5:06:24
[2024-03-21 02:25:38 pose_transfer_train.py:516] Train [1/100](80/1618)  Time 10.9925(11.7891)  Loss 0.0436(0.0369)  Lr 0.00001729  Eta 5:02:11
[2024-03-21 02:27:27 pose_transfer_train.py:516] Train [1/100](90/1618)  Time 11.0519(11.6927)  Loss 0.0324(0.0364)  Lr 0.00001819  Eta 4:57:46
[2024-03-21 02:29:17 pose_transfer_train.py:516] Train [1/100](100/1618)  Time 11.0459(11.6197)  Loss 0.0222(0.0362)  Lr 0.00001909  Eta 4:53:58
[2024-03-21 02:31:06 pose_transfer_train.py:516] Train [1/100](110/1618)  Time 10.0805(11.5519)  Loss 0.0276(0.0363)  Lr 0.00001999  Eta 4:50:20
[2024-03-21 02:32:56 pose_transfer_train.py:516] Train [1/100](120/1618)  Time 10.7508(11.5109)  Loss 0.0373(0.0360)  Lr 0.00002089  Eta 4:47:23
[2024-03-21 02:34:48 pose_transfer_train.py:516] Train [1/100](130/1618)  Time 11.5507(11.4814)  Loss 0.0491(0.0361)  Lr 0.00002179  Eta 4:44:44
[2024-03-21 02:36:41 pose_transfer_train.py:516] Train [1/100](140/1618)  Time 11.6813(11.4738)  Loss 0.0196(0.0360)  Lr 0.00002269  Eta 4:42:38
[2024-03-21 02:38:31 pose_transfer_train.py:516] Train [1/100](150/1618)  Time 10.4060(11.4431)  Loss 0.0254(0.0358)  Lr 0.00002359  Eta 4:39:58
[2024-03-21 02:40:30 pose_transfer_train.py:516] Train [1/100](160/1618)  Time 11.9894(11.4663)  Loss 0.0260(0.0352)  Lr 0.00002449  Eta 4:38:37
[2024-03-21 02:42:28 pose_transfer_train.py:516] Train [1/100](170/1618)  Time 11.9336(11.4880)  Loss 0.0386(0.0355)  Lr 0.00002539  Eta 4:37:14
[2024-03-21 02:44:24 pose_transfer_train.py:516] Train [1/100](180/1618)  Time 11.8548(11.4927)  Loss 0.0348(0.0356)  Lr 0.00002629  Eta 4:35:26
[2024-03-21 02:46:20 pose_transfer_train.py:516] Train [1/100](190/1618)  Time 11.6113(11.4995)  Loss 0.0392(0.0355)  Lr 0.00002719  Eta 4:33:41
[2024-03-21 02:48:13 pose_transfer_train.py:516] Train [1/100](200/1618)  Time 11.0017(11.4910)  Loss 0.0262(0.0354)  Lr 0.00002809  Eta 4:31:34
[2024-03-21 02:50:03 pose_transfer_train.py:516] Train [1/100](210/1618)  Time 11.6219(11.4671)  Loss 0.0301(0.0352)  Lr 0.00002899  Eta 4:29:05
[2024-03-21 02:51:53 pose_transfer_train.py:516] Train [1/100](220/1618)  Time 11.1802(11.4437)  Loss 0.0322(0.0351)  Lr 0.00002989  Eta 4:26:38
[2024-03-21 02:53:44 pose_transfer_train.py:516] Train [1/100](230/1618)  Time 11.1620(11.4299)  Loss 0.0652(0.0353)  Lr 0.00003079  Eta 4:24:24
[2024-03-21 02:55:34 pose_transfer_train.py:516] Train [1/100](240/1618)  Time 11.0229(11.4127)  Loss 0.0334(0.0356)  Lr 0.00003169  Eta 4:22:06
[2024-03-21 02:57:25 pose_transfer_train.py:516] Train [1/100](250/1618)  Time 11.4308(11.4009)  Loss 0.0270(0.0353)  Lr 0.00003259  Eta 4:19:56
[2024-03-21 02:59:16 pose_transfer_train.py:516] Train [1/100](260/1618)  Time 10.7436(11.3899)  Loss 0.0304(0.0351)  Lr 0.00003349  Eta 4:17:47
[2024-03-21 03:01:07 pose_transfer_train.py:516] Train [1/100](270/1618)  Time 11.4077(11.3793)  Loss 0.0258(0.0349)  Lr 0.00003439  Eta 4:15:39
[2024-03-21 03:02:58 pose_transfer_train.py:516] Train [1/100](280/1618)  Time 11.0803(11.3677)  Loss 0.0196(0.0348)  Lr 0.00003529  Eta 4:13:29
[2024-03-21 03:04:50 pose_transfer_train.py:516] Train [1/100](290/1618)  Time 11.5193(11.3607)  Loss 0.0326(0.0351)  Lr 0.00003619  Eta 4:11:26
[2024-03-21 03:06:40 pose_transfer_train.py:516] Train [1/100](300/1618)  Time 10.8158(11.3492)  Loss 0.0425(0.0350)  Lr 0.00003709  Eta 4:09:18
[2024-03-21 03:08:37 pose_transfer_train.py:516] Train [1/100](310/1618)  Time 14.3682(11.3616)  Loss 0.0199(0.0350)  Lr 0.00003799  Eta 4:07:40
[2024-03-21 03:10:34 pose_transfer_train.py:516] Train [1/100](320/1618)  Time 10.9145(11.3709)  Loss 0.0506(0.0351)  Lr 0.00003889  Eta 4:05:59
[2024-03-21 03:12:23 pose_transfer_train.py:516] Train [1/100](330/1618)  Time 10.8956(11.3590)  Loss 0.0304(0.0351)  Lr 0.00003979  Eta 4:03:50
[2024-03-21 03:14:14 pose_transfer_train.py:516] Train [1/100](340/1618)  Time 11.7651(11.3507)  Loss 0.0252(0.0351)  Lr 0.00004069  Eta 4:01:46
[2024-03-21 03:16:06 pose_transfer_train.py:516] Train [1/100](350/1618)  Time 11.1522(11.3454)  Loss 0.0298(0.0349)  Lr 0.00004159  Eta 3:59:45
[2024-03-21 03:17:56 pose_transfer_train.py:516] Train [1/100](360/1618)  Time 11.3132(11.3362)  Loss 0.0355(0.0350)  Lr 0.00004249  Eta 3:57:40
[2024-03-21 03:19:49 pose_transfer_train.py:516] Train [1/100](370/1618)  Time 10.8350(11.3342)  Loss 0.0358(0.0349)  Lr 0.00004339  Eta 3:55:45
[2024-03-21 03:21:40 pose_transfer_train.py:516] Train [1/100](380/1618)  Time 11.6265(11.3279)  Loss 0.0377(0.0349)  Lr 0.00004429  Eta 3:53:43
[2024-03-21 03:23:30 pose_transfer_train.py:516] Train [1/100](390/1618)  Time 11.6862(11.3207)  Loss 0.0322(0.0348)  Lr 0.00004519  Eta 3:51:41
[2024-03-21 03:25:19 pose_transfer_train.py:516] Train [1/100](400/1618)  Time 11.0933(11.3108)  Loss 0.0358(0.0349)  Lr 0.00004609  Eta 3:49:36
[2024-03-21 03:27:09 pose_transfer_train.py:516] Train [1/100](410/1618)  Time 11.0320(11.3027)  Loss 0.0327(0.0350)  Lr 0.00004699  Eta 3:47:33
[2024-03-21 03:28:59 pose_transfer_train.py:516] Train [1/100](420/1618)  Time 10.3234(11.2958)  Loss 0.0335(0.0350)  Lr 0.00004789  Eta 3:45:32
[2024-03-21 03:30:50 pose_transfer_train.py:516] Train [1/100](430/1618)  Time 10.9938(11.2908)  Loss 0.0449(0.0351)  Lr 0.00004879  Eta 3:43:33
[2024-03-21 03:32:39 pose_transfer_train.py:516] Train [1/100](440/1618)  Time 11.0936(11.2823)  Loss 0.0371(0.0352)  Lr 0.00004969  Eta 3:41:30
[2024-03-21 03:34:30 pose_transfer_train.py:516] Train [1/100](450/1618)  Time 11.2814(11.2783)  Loss 0.0209(0.0351)  Lr 0.00005059  Eta 3:39:33
[2024-03-21 03:36:18 pose_transfer_train.py:516] Train [1/100](460/1618)  Time 11.4636(11.2686)  Loss 0.0402(0.0351)  Lr 0.00005149  Eta 3:37:29
[2024-03-21 03:38:10 pose_transfer_train.py:516] Train [1/100](470/1618)  Time 10.6359(11.2665)  Loss 0.0344(0.0351)  Lr 0.00005239  Eta 3:35:33
[2024-03-21 03:40:01 pose_transfer_train.py:516] Train [1/100](480/1618)  Time 11.6541(11.2626)  Loss 0.0527(0.0351)  Lr 0.00005329  Eta 3:33:36
[2024-03-21 03:41:52 pose_transfer_train.py:516] Train [1/100](490/1618)  Time 10.9785(11.2584)  Loss 0.0288(0.0351)  Lr 0.00005419  Eta 3:31:39
[2024-03-21 03:43:41 pose_transfer_train.py:516] Train [1/100](500/1618)  Time 10.8842(11.2516)  Loss 0.0548(0.0350)  Lr 0.00005509  Eta 3:29:39
[2024-03-21 03:45:29 pose_transfer_train.py:516] Train [1/100](510/1618)  Time 10.8487(11.2434)  Loss 0.0241(0.0351)  Lr 0.00005599  Eta 3:27:37
[2024-03-21 03:47:20 pose_transfer_train.py:516] Train [1/100](520/1618)  Time 10.6656(11.2411)  Loss 0.0379(0.0351)  Lr 0.00005689  Eta 3:25:42
[2024-03-21 03:49:09 pose_transfer_train.py:516] Train [1/100](530/1618)  Time 10.7480(11.2344)  Loss 0.0230(0.0350)  Lr 0.00005779  Eta 3:23:43
[2024-03-21 03:51:00 pose_transfer_train.py:516] Train [1/100](540/1618)  Time 10.7003(11.2318)  Loss 0.0395(0.0350)  Lr 0.00005869  Eta 3:21:47
[2024-03-21 03:52:51 pose_transfer_train.py:516] Train [1/100](550/1618)  Time 10.4932(11.2296)  Loss 0.0441(0.0351)  Lr 0.00005959  Eta 3:19:53
[2024-03-21 03:54:43 pose_transfer_train.py:516] Train [1/100](560/1618)  Time 11.1135(11.2284)  Loss 0.0761(0.0351)  Lr 0.00006049  Eta 3:17:59
[2024-03-21 03:56:33 pose_transfer_train.py:516] Train [1/100](570/1618)  Time 10.8962(11.2252)  Loss 0.0342(0.0352)  Lr 0.00006139  Eta 3:16:03
[2024-03-21 03:58:24 pose_transfer_train.py:516] Train [1/100](580/1618)  Time 11.4898(11.2232)  Loss 0.0492(0.0352)  Lr 0.00006229  Eta 3:14:09
[2024-03-21 04:00:15 pose_transfer_train.py:516] Train [1/100](590/1618)  Time 10.4890(11.2209)  Loss 0.0382(0.0351)  Lr 0.00006319  Eta 3:12:15
[2024-03-21 04:02:05 pose_transfer_train.py:516] Train [1/100](600/1618)  Time 11.3790(11.2169)  Loss 0.0225(0.0352)  Lr 0.00006409  Eta 3:10:18
[2024-03-21 04:03:57 pose_transfer_train.py:516] Train [1/100](610/1618)  Time 11.3356(11.2159)  Loss 0.0201(0.0352)  Lr 0.00006499  Eta 3:08:25
[2024-03-21 04:05:47 pose_transfer_train.py:516] Train [1/100](620/1618)  Time 11.1808(11.2137)  Loss 0.0343(0.0352)  Lr 0.00006589  Eta 3:06:31
[2024-03-21 04:07:38 pose_transfer_train.py:516] Train [1/100](630/1618)  Time 10.4547(11.2106)  Loss 0.0266(0.0352)  Lr 0.00006679  Eta 3:04:36
[2024-03-21 04:09:29 pose_transfer_train.py:516] Train [1/100](640/1618)  Time 10.8997(11.2091)  Loss 0.0380(0.0352)  Lr 0.00006769  Eta 3:02:42
[2024-03-21 04:11:19 pose_transfer_train.py:516] Train [1/100](650/1618)  Time 10.2696(11.2058)  Loss 0.0365(0.0353)  Lr 0.00006859  Eta 3:00:47
[2024-03-21 04:13:08 pose_transfer_train.py:516] Train [1/100](660/1618)  Time 10.7889(11.2011)  Loss 0.0308(0.0353)  Lr 0.00006949  Eta 2:58:50
[2024-03-21 04:14:57 pose_transfer_train.py:516] Train [1/100](670/1618)  Time 10.9231(11.1967)  Loss 0.0329(0.0353)  Lr 0.00007039  Eta 2:56:54
[2024-03-21 04:16:46 pose_transfer_train.py:516] Train [1/100](680/1618)  Time 10.8377(11.1931)  Loss 0.0235(0.0352)  Lr 0.00007129  Eta 2:54:59
[2024-03-21 04:18:35 pose_transfer_train.py:516] Train [1/100](690/1618)  Time 11.1866(11.1887)  Loss 0.0241(0.0352)  Lr 0.00007219  Eta 2:53:03
[2024-03-21 04:20:27 pose_transfer_train.py:516] Train [1/100](700/1618)  Time 10.7417(11.1886)  Loss 0.0395(0.0352)  Lr 0.00007309  Eta 2:51:11
[2024-03-21 04:22:20 pose_transfer_train.py:516] Train [1/100](710/1618)  Time 12.0614(11.1902)  Loss 0.0741(0.0353)  Lr 0.00007399  Eta 2:49:20
[2024-03-21 04:24:10 pose_transfer_train.py:516] Train [1/100](720/1618)  Time 11.2426(11.1879)  Loss 0.0456(0.0354)  Lr 0.00007489  Eta 2:47:26
[2024-03-21 04:26:00 pose_transfer_train.py:516] Train [1/100](730/1618)  Time 11.1716(11.1852)  Loss 0.0312(0.0354)  Lr 0.00007579  Eta 2:45:32
[2024-03-21 04:27:50 pose_transfer_train.py:516] Train [1/100](740/1618)  Time 11.2357(11.1821)  Loss 0.0356(0.0354)  Lr 0.00007669  Eta 2:43:37
[2024-03-21 04:29:39 pose_transfer_train.py:516] Train [1/100](750/1618)  Time 11.2773(11.1789)  Loss 0.0287(0.0353)  Lr 0.00007759  Eta 2:41:43
[2024-03-21 04:31:30 pose_transfer_train.py:516] Train [1/100](760/1618)  Time 11.5000(11.1777)  Loss 0.0249(0.0353)  Lr 0.00007849  Eta 2:39:50
[2024-03-21 04:33:18 pose_transfer_train.py:516] Train [1/100](770/1618)  Time 10.8122(11.1733)  Loss 0.0409(0.0353)  Lr 0.00007939  Eta 2:37:54
[2024-03-21 04:35:07 pose_transfer_train.py:516] Train [1/100](780/1618)  Time 10.3999(11.1698)  Loss 0.0150(0.0352)  Lr 0.00008029  Eta 2:36:00
[2024-03-21 04:36:58 pose_transfer_train.py:516] Train [1/100](790/1618)  Time 9.9693(11.1690)  Loss 0.0424(0.0352)  Lr 0.00008119  Eta 2:34:07
[2024-03-21 04:38:49 pose_transfer_train.py:516] Train [1/100](800/1618)  Time 10.7643(11.1674)  Loss 0.0243(0.0352)  Lr 0.00008209  Eta 2:32:14
[2024-03-21 04:40:40 pose_transfer_train.py:516] Train [1/100](810/1618)  Time 11.3399(11.1670)  Loss 0.0470(0.0352)  Lr 0.00008299  Eta 2:30:22
[2024-03-21 04:42:28 pose_transfer_train.py:516] Train [1/100](820/1618)  Time 10.8437(11.1627)  Loss 0.0521(0.0353)  Lr 0.00008389  Eta 2:28:27
[2024-03-21 04:44:18 pose_transfer_train.py:516] Train [1/100](830/1618)  Time 11.4326(11.1601)  Loss 0.0539(0.0353)  Lr 0.00008479  Eta 2:26:34
[2024-03-21 04:46:08 pose_transfer_train.py:516] Train [1/100](840/1618)  Time 10.6877(11.1581)  Loss 0.0358(0.0352)  Lr 0.00008569  Eta 2:24:40
[2024-03-21 04:47:56 pose_transfer_train.py:516] Train [1/100](850/1618)  Time 11.4761(11.1539)  Loss 0.0343(0.0352)  Lr 0.00008659  Eta 2:22:46
[2024-03-21 04:49:46 pose_transfer_train.py:516] Train [1/100](860/1618)  Time 11.4770(11.1530)  Loss 0.0276(0.0353)  Lr 0.00008749  Eta 2:20:53
[2024-03-21 04:51:35 pose_transfer_train.py:516] Train [1/100](870/1618)  Time 10.8187(11.1499)  Loss 0.0377(0.0353)  Lr 0.00008839  Eta 2:19:00
[2024-03-21 04:53:24 pose_transfer_train.py:516] Train [1/100](880/1618)  Time 10.5785(11.1470)  Loss 0.0367(0.0353)  Lr 0.00008929  Eta 2:17:06
[2024-03-21 04:55:15 pose_transfer_train.py:516] Train [1/100](890/1618)  Time 10.7003(11.1460)  Loss 0.0427(0.0353)  Lr 0.00009019  Eta 2:15:14
[2024-03-21 04:57:07 pose_transfer_train.py:516] Train [1/100](900/1618)  Time 10.9691(11.1465)  Loss 0.0245(0.0352)  Lr 0.00009109  Eta 2:13:23
[2024-03-21 04:58:56 pose_transfer_train.py:516] Train [1/100](910/1618)  Time 10.7617(11.1441)  Loss 0.0329(0.0352)  Lr 0.00009199  Eta 2:11:30
[2024-03-21 05:00:46 pose_transfer_train.py:516] Train [1/100](920/1618)  Time 10.7420(11.1420)  Loss 0.0455(0.0353)  Lr 0.00009289  Eta 2:09:37
[2024-03-21 05:02:36 pose_transfer_train.py:516] Train [1/100](930/1618)  Time 11.1475(11.1412)  Loss 0.0309(0.0353)  Lr 0.00009379  Eta 2:07:45
[2024-03-21 05:04:28 pose_transfer_train.py:516] Train [1/100](940/1618)  Time 11.3173(11.1414)  Loss 0.0262(0.0353)  Lr 0.00009469  Eta 2:05:53
[2024-03-21 05:06:17 pose_transfer_train.py:516] Train [1/100](950/1618)  Time 11.1604(11.1391)  Loss 0.0383(0.0353)  Lr 0.00009559  Eta 2:04:00
[2024-03-21 05:08:06 pose_transfer_train.py:516] Train [1/100](960/1618)  Time 10.9954(11.1369)  Loss 0.0187(0.0353)  Lr 0.00009649  Eta 2:02:08
[2024-03-21 05:09:59 pose_transfer_train.py:516] Train [1/100](970/1618)  Time 11.0553(11.1378)  Loss 0.0464(0.0353)  Lr 0.00009739  Eta 2:00:17
[2024-03-21 05:12:07 pose_transfer_train.py:516] Train [1/100](980/1618)  Time 13.2919(11.1547)  Loss 0.0215(0.0353)  Lr 0.00009829  Eta 1:58:36
[2024-03-21 05:14:19 pose_transfer_train.py:516] Train [1/100](990/1618)  Time 13.3927(11.1759)  Loss 0.0285(0.0352)  Lr 0.00009919  Eta 1:56:58
[2024-03-21 05:16:30 pose_transfer_train.py:516] Train [1/100](1000/1618)  Time 13.2741(11.1952)  Loss 0.0362(0.0352)  Lr 0.00010000  Eta 1:55:18
[2024-03-21 05:18:43 pose_transfer_train.py:516] Train [1/100](1010/1618)  Time 12.7829(11.2158)  Loss 0.0175(0.0352)  Lr 0.00010000  Eta 1:53:39
[2024-03-21 05:20:55 pose_transfer_train.py:516] Train [1/100](1020/1618)  Time 13.7810(11.2352)  Loss 0.0249(0.0352)  Lr 0.00010000  Eta 1:51:58
[2024-03-21 05:23:09 pose_transfer_train.py:516] Train [1/100](1030/1618)  Time 13.5923(11.2560)  Loss 0.0804(0.0352)  Lr 0.00010000  Eta 1:50:18
[2024-03-21 05:25:22 pose_transfer_train.py:516] Train [1/100](1040/1618)  Time 14.3552(11.2757)  Loss 0.0310(0.0352)  Lr 0.00010000  Eta 1:48:37
[2024-03-21 05:27:33 pose_transfer_train.py:516] Train [1/100](1050/1618)  Time 12.9794(11.2934)  Loss 0.0294(0.0352)  Lr 0.00010000  Eta 1:46:54
[2024-03-21 05:29:44 pose_transfer_train.py:516] Train [1/100](1060/1618)  Time 12.4627(11.3103)  Loss 0.0157(0.0351)  Lr 0.00010000  Eta 1:45:11
[2024-03-21 05:31:55 pose_transfer_train.py:516] Train [1/100](1070/1618)  Time 13.1554(11.3276)  Loss 0.0541(0.0351)  Lr 0.00010000  Eta 1:43:27
[2024-03-21 05:34:04 pose_transfer_train.py:516] Train [1/100](1080/1618)  Time 12.2963(11.3421)  Loss 0.0218(0.0351)  Lr 0.00010000  Eta 1:41:42
[2024-03-21 05:36:20 pose_transfer_train.py:516] Train [1/100](1090/1618)  Time 12.6687(11.3627)  Loss 0.0427(0.0352)  Lr 0.00010000  Eta 1:39:59
[2024-03-21 05:38:32 pose_transfer_train.py:516] Train [1/100](1100/1618)  Time 12.9383(11.3790)  Loss 0.0255(0.0351)  Lr 0.00010000  Eta 1:38:14
[2024-03-21 05:40:45 pose_transfer_train.py:516] Train [1/100](1110/1618)  Time 13.0177(11.3963)  Loss 0.0542(0.0351)  Lr 0.00010000  Eta 1:36:29
[2024-03-21 05:42:56 pose_transfer_train.py:516] Train [1/100](1120/1618)  Time 13.4589(11.4117)  Loss 0.0422(0.0351)  Lr 0.00010000  Eta 1:34:43
[2024-03-21 05:45:09 pose_transfer_train.py:516] Train [1/100](1130/1618)  Time 13.0938(11.4283)  Loss 0.0248(0.0351)  Lr 0.00010000  Eta 1:32:57
[2024-03-21 05:47:19 pose_transfer_train.py:516] Train [1/100](1140/1618)  Time 12.2625(11.4424)  Loss 0.0535(0.0351)  Lr 0.00010000  Eta 1:31:09
[2024-03-21 05:49:33 pose_transfer_train.py:516] Train [1/100](1150/1618)  Time 13.2875(11.4594)  Loss 0.0454(0.0350)  Lr 0.00010000  Eta 1:29:23
[2024-03-21 05:51:47 pose_transfer_train.py:516] Train [1/100](1160/1618)  Time 14.3232(11.4761)  Loss 0.0322(0.0350)  Lr 0.00010000  Eta 1:27:36
[2024-03-21 05:54:01 pose_transfer_train.py:516] Train [1/100](1170/1618)  Time 13.6537(11.4923)  Loss 0.0311(0.0350)  Lr 0.00010000  Eta 1:25:48
[2024-03-21 05:56:11 pose_transfer_train.py:516] Train [1/100](1180/1618)  Time 13.6362(11.5050)  Loss 0.0794(0.0349)  Lr 0.00010000  Eta 1:23:59
[2024-03-21 05:58:20 pose_transfer_train.py:516] Train [1/100](1190/1618)  Time 13.3465(11.5166)  Loss 0.0254(0.0349)  Lr 0.00010000  Eta 1:22:09
[2024-03-21 06:00:30 pose_transfer_train.py:516] Train [1/100](1200/1618)  Time 12.6081(11.5294)  Loss 0.0228(0.0349)  Lr 0.00010000  Eta 1:20:19
[2024-03-21 06:02:41 pose_transfer_train.py:516] Train [1/100](1210/1618)  Time 12.5238(11.5420)  Loss 0.0251(0.0349)  Lr 0.00010000  Eta 1:18:29
[2024-03-21 06:04:50 pose_transfer_train.py:516] Train [1/100](1220/1618)  Time 12.9607(11.5537)  Loss 0.0271(0.0349)  Lr 0.00010000  Eta 1:16:38
[2024-03-21 06:07:01 pose_transfer_train.py:516] Train [1/100](1230/1618)  Time 13.1135(11.5660)  Loss 0.0394(0.0349)  Lr 0.00010000  Eta 1:14:47
[2024-03-21 06:09:09 pose_transfer_train.py:516] Train [1/100](1240/1618)  Time 13.1636(11.5760)  Loss 0.0377(0.0349)  Lr 0.00010000  Eta 1:12:55
[2024-03-21 06:11:21 pose_transfer_train.py:516] Train [1/100](1250/1618)  Time 11.8896(11.5889)  Loss 0.0388(0.0348)  Lr 0.00010000  Eta 1:11:04
[2024-03-21 06:13:22 pose_transfer_train.py:516] Train [1/100](1260/1618)  Time 12.3866(11.5927)  Loss 0.0226(0.0348)  Lr 0.00010000  Eta 1:09:10
[2024-03-21 06:15:25 pose_transfer_train.py:516] Train [1/100](1270/1618)  Time 12.7475(11.5984)  Loss 0.0347(0.0348)  Lr 0.00010000  Eta 1:07:16
[2024-03-21 06:17:32 pose_transfer_train.py:516] Train [1/100](1280/1618)  Time 12.6180(11.6070)  Loss 0.0270(0.0347)  Lr 0.00010000  Eta 1:05:23
[2024-03-21 06:19:39 pose_transfer_train.py:516] Train [1/100](1290/1618)  Time 12.4410(11.6152)  Loss 0.0363(0.0347)  Lr 0.00010000  Eta 1:03:29
[2024-03-21 06:21:45 pose_transfer_train.py:516] Train [1/100](1300/1618)  Time 12.3696(11.6233)  Loss 0.0224(0.0347)  Lr 0.00010000  Eta 1:01:36
[2024-03-21 06:23:52 pose_transfer_train.py:516] Train [1/100](1310/1618)  Time 12.4533(11.6315)  Loss 0.0296(0.0346)  Lr 0.00010000  Eta 0:59:42
[2024-03-21 06:25:59 pose_transfer_train.py:516] Train [1/100](1320/1618)  Time 12.6364(11.6391)  Loss 0.0444(0.0346)  Lr 0.00010000  Eta 0:57:48
[2024-03-21 06:28:04 pose_transfer_train.py:516] Train [1/100](1330/1618)  Time 12.6211(11.6463)  Loss 0.0376(0.0346)  Lr 0.00010000  Eta 0:55:54
[2024-03-21 06:30:09 pose_transfer_train.py:516] Train [1/100](1340/1618)  Time 12.8626(11.6526)  Loss 0.0382(0.0346)  Lr 0.00010000  Eta 0:53:59
[2024-03-21 06:32:19 pose_transfer_train.py:516] Train [1/100](1350/1618)  Time 12.3923(11.6619)  Loss 0.0261(0.0346)  Lr 0.00010000  Eta 0:52:05
[2024-03-21 06:34:26 pose_transfer_train.py:516] Train [1/100](1360/1618)  Time 11.8133(11.6699)  Loss 0.0299(0.0346)  Lr 0.00010000  Eta 0:50:10
[2024-03-21 06:36:33 pose_transfer_train.py:516] Train [1/100](1370/1618)  Time 12.6673(11.6774)  Loss 0.0197(0.0345)  Lr 0.00010000  Eta 0:48:15
[2024-03-21 06:38:41 pose_transfer_train.py:516] Train [1/100](1380/1618)  Time 12.4910(11.6857)  Loss 0.0376(0.0345)  Lr 0.00010000  Eta 0:46:21
[2024-03-21 06:40:49 pose_transfer_train.py:516] Train [1/100](1390/1618)  Time 12.8227(11.6938)  Loss 0.0562(0.0345)  Lr 0.00010000  Eta 0:44:26
[2024-03-21 06:42:54 pose_transfer_train.py:516] Train [1/100](1400/1618)  Time 12.3836(11.6990)  Loss 0.0140(0.0345)  Lr 0.00010000  Eta 0:42:30
[2024-03-21 06:45:01 pose_transfer_train.py:516] Train [1/100](1410/1618)  Time 12.5662(11.7067)  Loss 0.0276(0.0345)  Lr 0.00010000  Eta 0:40:34
[2024-03-21 06:47:09 pose_transfer_train.py:516] Train [1/100](1420/1618)  Time 13.0004(11.7145)  Loss 0.0320(0.0345)  Lr 0.00010000  Eta 0:38:39
[2024-03-21 06:49:19 pose_transfer_train.py:516] Train [1/100](1430/1618)  Time 13.4944(11.7229)  Loss 0.0121(0.0344)  Lr 0.00010000  Eta 0:36:43
[2024-03-21 06:51:26 pose_transfer_train.py:516] Train [1/100](1440/1618)  Time 13.2623(11.7301)  Loss 0.0252(0.0344)  Lr 0.00010000  Eta 0:34:47
[2024-03-21 06:53:36 pose_transfer_train.py:516] Train [1/100](1450/1618)  Time 12.2891(11.7385)  Loss 0.0191(0.0344)  Lr 0.00010000  Eta 0:32:52
[2024-03-21 06:55:44 pose_transfer_train.py:516] Train [1/100](1460/1618)  Time 12.7771(11.7463)  Loss 0.0358(0.0344)  Lr 0.00010000  Eta 0:30:55
[2024-03-21 06:57:52 pose_transfer_train.py:516] Train [1/100](1470/1618)  Time 11.7423(11.7529)  Loss 0.0424(0.0344)  Lr 0.00010000  Eta 0:28:59
[2024-03-21 07:00:00 pose_transfer_train.py:516] Train [1/100](1480/1618)  Time 12.6060(11.7600)  Loss 0.0540(0.0344)  Lr 0.00010000  Eta 0:27:02
[2024-03-21 07:02:05 pose_transfer_train.py:516] Train [1/100](1490/1618)  Time 12.3587(11.7655)  Loss 0.0274(0.0344)  Lr 0.00010000  Eta 0:25:05
[2024-03-21 07:04:12 pose_transfer_train.py:516] Train [1/100](1500/1618)  Time 12.4815(11.7716)  Loss 0.0195(0.0344)  Lr 0.00010000  Eta 0:23:09
[2024-03-21 07:06:17 pose_transfer_train.py:516] Train [1/100](1510/1618)  Time 13.0057(11.7759)  Loss 0.0278(0.0344)  Lr 0.00010000  Eta 0:21:11
[2024-03-21 07:08:23 pose_transfer_train.py:516] Train [1/100](1520/1618)  Time 12.7474(11.7817)  Loss 0.0349(0.0343)  Lr 0.00010000  Eta 0:19:14
[2024-03-21 07:10:29 pose_transfer_train.py:516] Train [1/100](1530/1618)  Time 12.5173(11.7872)  Loss 0.0242(0.0343)  Lr 0.00010000  Eta 0:17:17
[2024-03-21 07:12:35 pose_transfer_train.py:516] Train [1/100](1540/1618)  Time 12.7656(11.7926)  Loss 0.0457(0.0343)  Lr 0.00010000  Eta 0:15:19
[2024-03-21 07:14:43 pose_transfer_train.py:516] Train [1/100](1550/1618)  Time 13.5984(11.7986)  Loss 0.0206(0.0343)  Lr 0.00010000  Eta 0:13:22
[2024-03-21 07:16:49 pose_transfer_train.py:516] Train [1/100](1560/1618)  Time 13.0726(11.8040)  Loss 0.0526(0.0343)  Lr 0.00010000  Eta 0:11:24
[2024-03-21 07:18:54 pose_transfer_train.py:516] Train [1/100](1570/1618)  Time 12.2632(11.8085)  Loss 0.0399(0.0343)  Lr 0.00010000  Eta 0:09:26
[2024-03-21 07:21:00 pose_transfer_train.py:516] Train [1/100](1580/1618)  Time 13.6313(11.8135)  Loss 0.0316(0.0342)  Lr 0.00010000  Eta 0:07:28
[2024-03-21 07:23:09 pose_transfer_train.py:516] Train [1/100](1590/1618)  Time 13.0668(11.8199)  Loss 0.0280(0.0342)  Lr 0.00010000  Eta 0:05:30
[2024-03-21 07:25:15 pose_transfer_train.py:516] Train [1/100](1600/1618)  Time 12.9292(11.8253)  Loss 0.0135(0.0341)  Lr 0.00010000  Eta 0:03:32
[2024-03-21 07:27:23 pose_transfer_train.py:516] Train [1/100](1610/1618)  Time 12.9413(11.8311)  Loss 0.0573(0.0342)  Lr 0.00010000  Eta 0:01:34
[2024-03-21 07:29:01 pose_transfer_train.py:516] Train [1/100](1618/1618)  Time 13.1684(11.8333)  Loss 0.0336(0.0342)  Lr 0.00010000  Eta 0:00:00
[2024-03-21 07:29:01 pose_transfer_train.py:526] epoch 1 finished, running time 5:19:06
[2024-03-21 07:29:01 logging.py:60] Saving current state to outputs/CFLD/debug/epochs_001/checkpoints
[2024-03-21 07:29:08 logging.py:60] Model weights saved in outputs/CFLD/debug/epochs_001/checkpoints/pytorch_model.bin
