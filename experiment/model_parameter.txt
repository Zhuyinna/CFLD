========================model_CFG=======================:
build_model_CFG(
  (image_encoder_g): CLIPVisionModelWithProjection(
    (vision_model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
        (position_embedding): Embedding(50, 768)
      )
      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-11): 12 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (visual_projection): Linear(in_features=768, out_features=512, bias=False)
  )
  (backbone): SwinTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=128, input_resolution=(64, 64), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=128, input_resolution=(64, 64), num_heads=4, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=128, window_size=(16, 16), num_heads=4
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=128, input_resolution=(64, 64), num_heads=4, window_size=16, shift_size=8, mlp_ratio=4.0
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=128, window_size=(16, 16), num_heads=4
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=512, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(64, 64), dim=128
          (reduction): Linear(in_features=512, out_features=256, bias=False)
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): BasicLayer(
        dim=256, input_resolution=(32, 32), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=256, input_resolution=(32, 32), num_heads=8, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=256, window_size=(16, 16), num_heads=8
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=256, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1024, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=256, input_resolution=(32, 32), num_heads=8, window_size=16, shift_size=8, mlp_ratio=4.0
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=256, window_size=(16, 16), num_heads=8
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=256, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1024, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(32, 32), dim=256
          (reduction): Linear(in_features=1024, out_features=512, bias=False)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=512, input_resolution=(16, 16), depth=18
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.035)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.052)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.061)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.070)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.078)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.087)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.096)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.104)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.113)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.122)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.130)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.139)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.148)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.157)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.165)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.174)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SwinTransformerBlock(
            dim=512, input_resolution=(16, 16), num_heads=16, window_size=16, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=512, window_size=(16, 16), num_heads=16
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.183)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(16, 16), dim=512
          (reduction): Linear(in_features=2048, out_features=1024, bias=False)
          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=1024, input_resolution=(8, 8), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=1024, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=1024, window_size=(8, 8), num_heads=32
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.191)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=1024, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=1024, window_size=(8, 8), num_heads=32
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.200)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (norm): Identity()
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (appearance_encoder): AppearanceEncoder(
    (blocks): ModuleList(
      (0-2): 3 x Sequential(
        (0): BasicTransformerBlock(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=64, out_features=64, bias=False)
            (to_k): Linear(in_features=64, out_features=64, bias=False)
            (to_v): Linear(in_features=64, out_features=64, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=64, out_features=64, bias=False)
            (to_k): Linear(in_features=64, out_features=64, bias=False)
            (to_v): Linear(in_features=64, out_features=64, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=64, out_features=512, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=256, out_features=64, bias=True)
            )
          )
        )
        (1): BasicTransformerBlock(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=64, out_features=64, bias=False)
            (to_k): Linear(in_features=64, out_features=64, bias=False)
            (to_v): Linear(in_features=64, out_features=64, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=64, out_features=64, bias=False)
            (to_k): Linear(in_features=64, out_features=64, bias=False)
            (to_v): Linear(in_features=64, out_features=64, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=64, out_features=512, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=256, out_features=64, bias=True)
            )
          )
        )
        (2): BasicTransformerBlock(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=64, out_features=64, bias=False)
            (to_k): Linear(in_features=64, out_features=64, bias=False)
            (to_v): Linear(in_features=64, out_features=64, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=64, out_features=64, bias=False)
            (to_k): Linear(in_features=64, out_features=64, bias=False)
            (to_v): Linear(in_features=64, out_features=64, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=64, out_features=512, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=256, out_features=64, bias=True)
            )
          )
        )
        (3): BasicTransformerBlock(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=64, out_features=64, bias=False)
            (to_k): Linear(in_features=64, out_features=64, bias=False)
            (to_v): Linear(in_features=64, out_features=64, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=64, out_features=64, bias=False)
            (to_k): Linear(in_features=64, out_features=64, bias=False)
            (to_v): Linear(in_features=64, out_features=64, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=64, out_features=64, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=64, out_features=512, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=256, out_features=64, bias=True)
            )
          )
        )
      )
      (3-5): 3 x Sequential(
        (0): BasicTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=128, out_features=128, bias=False)
            (to_k): Linear(in_features=128, out_features=128, bias=False)
            (to_v): Linear(in_features=128, out_features=128, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=128, out_features=128, bias=False)
            (to_k): Linear(in_features=128, out_features=128, bias=False)
            (to_v): Linear(in_features=128, out_features=128, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=128, out_features=1024, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=512, out_features=128, bias=True)
            )
          )
        )
        (1): BasicTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=128, out_features=128, bias=False)
            (to_k): Linear(in_features=128, out_features=128, bias=False)
            (to_v): Linear(in_features=128, out_features=128, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=128, out_features=128, bias=False)
            (to_k): Linear(in_features=128, out_features=128, bias=False)
            (to_v): Linear(in_features=128, out_features=128, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=128, out_features=1024, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=512, out_features=128, bias=True)
            )
          )
        )
        (2): BasicTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=128, out_features=128, bias=False)
            (to_k): Linear(in_features=128, out_features=128, bias=False)
            (to_v): Linear(in_features=128, out_features=128, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=128, out_features=128, bias=False)
            (to_k): Linear(in_features=128, out_features=128, bias=False)
            (to_v): Linear(in_features=128, out_features=128, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=128, out_features=1024, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=512, out_features=128, bias=True)
            )
          )
        )
        (3): BasicTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=128, out_features=128, bias=False)
            (to_k): Linear(in_features=128, out_features=128, bias=False)
            (to_v): Linear(in_features=128, out_features=128, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=128, out_features=128, bias=False)
            (to_k): Linear(in_features=128, out_features=128, bias=False)
            (to_v): Linear(in_features=128, out_features=128, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=128, out_features=1024, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=512, out_features=128, bias=True)
            )
          )
        )
      )
      (6-8): 3 x Sequential(
        (0): BasicTransformerBlock(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=256, out_features=256, bias=False)
            (to_k): Linear(in_features=256, out_features=256, bias=False)
            (to_v): Linear(in_features=256, out_features=256, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=256, out_features=256, bias=False)
            (to_k): Linear(in_features=256, out_features=256, bias=False)
            (to_v): Linear(in_features=256, out_features=256, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=256, out_features=2048, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=1024, out_features=256, bias=True)
            )
          )
        )
        (1): BasicTransformerBlock(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=256, out_features=256, bias=False)
            (to_k): Linear(in_features=256, out_features=256, bias=False)
            (to_v): Linear(in_features=256, out_features=256, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=256, out_features=256, bias=False)
            (to_k): Linear(in_features=256, out_features=256, bias=False)
            (to_v): Linear(in_features=256, out_features=256, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=256, out_features=2048, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=1024, out_features=256, bias=True)
            )
          )
        )
        (2): BasicTransformerBlock(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=256, out_features=256, bias=False)
            (to_k): Linear(in_features=256, out_features=256, bias=False)
            (to_v): Linear(in_features=256, out_features=256, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=256, out_features=256, bias=False)
            (to_k): Linear(in_features=256, out_features=256, bias=False)
            (to_v): Linear(in_features=256, out_features=256, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=256, out_features=2048, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=1024, out_features=256, bias=True)
            )
          )
        )
        (3): BasicTransformerBlock(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn1): Attention(
            (to_q): Linear(in_features=256, out_features=256, bias=False)
            (to_k): Linear(in_features=256, out_features=256, bias=False)
            (to_v): Linear(in_features=256, out_features=256, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn2): Attention(
            (to_q): Linear(in_features=256, out_features=256, bias=False)
            (to_k): Linear(in_features=256, out_features=256, bias=False)
            (to_v): Linear(in_features=256, out_features=256, bias=False)
            (to_out): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Dropout(p=0.0, inplace=False)
            )
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ff): FeedForward(
            (net): ModuleList(
              (0): GEGLU(
                (proj): LoRACompatibleLinear(in_features=256, out_features=2048, bias=True)
              )
              (1): Dropout(p=0.0, inplace=False)
              (2): LoRACompatibleLinear(in_features=1024, out_features=256, bias=True)
            )
          )
        )
      )
    )
    (zero_conv_ins): ModuleList(
      (0-2): 3 x Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
      (3-5): 3 x Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
      (6-8): 3 x Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (zero_conv_outs): ModuleList(
      (0-2): 3 x Conv2d(64, 320, kernel_size=(1, 1), stride=(1, 1))
      (3-5): 3 x Conv2d(128, 640, kernel_size=(1, 1), stride=(1, 1))
      (6-8): 3 x Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (pose_encoder): PoseEncoder(
    (unshuffle): PixelUnshuffle(downscale_factor=4)
    (conv_in): Conv2d(336, 320, kernel_size=(1, 1), stride=(1, 1))
    (resnets): ModuleList(
      (0): ResnetBlock2D(
        (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)
        (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 320, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (nonlinearity): SiLU()
      )
      (1): ResnetBlock2D(
        (norm1): GroupNorm(32, 320, eps=1e-06, affine=True)
        (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 640, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (nonlinearity): SiLU()
        (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ResnetBlock2D(
        (norm1): GroupNorm(32, 640, eps=1e-06, affine=True)
        (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): GroupNorm(32, 1280, eps=1e-06, affine=True)
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (nonlinearity): SiLU()
        (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (downsamplers): ModuleList(
      (0-1): 2 x Downsample2D(
        (conv): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
      (2): Identity()
    )
  )
)
learnable_vector                                                                : False
image_encoder_g.vision_model.embeddings.class_embedding                         : False
image_encoder_g.vision_model.embeddings.patch_embedding.weight                  : False
image_encoder_g.vision_model.embeddings.position_embedding.weight               : False
image_encoder_g.vision_model.pre_layrnorm.weight                                : False
image_encoder_g.vision_model.pre_layrnorm.bias                                  : False
image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.0.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.0.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.0.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.0.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.0.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.0.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.0.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.0.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.0.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.0.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.1.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.1.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.1.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.1.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.1.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.1.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.1.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.1.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.1.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.1.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.2.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.2.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.2.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.2.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.2.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.2.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.2.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.2.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.2.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.2.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.3.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.3.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.3.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.3.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.3.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.3.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.3.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.3.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.3.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.3.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.4.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.4.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.4.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.4.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.4.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.4.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.4.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.4.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.4.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.4.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.5.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.5.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.5.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.5.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.5.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.5.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.5.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.5.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.5.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.5.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.6.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.6.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.6.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.6.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.6.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.6.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.6.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.6.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.6.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.6.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.7.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.7.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.7.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.7.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.7.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.7.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.7.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.7.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.7.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.7.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.8.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.8.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.8.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.8.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.8.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.8.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.8.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.8.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.8.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.8.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.9.self_attn.k_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.9.self_attn.v_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.weight           : False
image_encoder_g.vision_model.encoder.layers.9.self_attn.q_proj.bias             : False
image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.weight         : False
image_encoder_g.vision_model.encoder.layers.9.self_attn.out_proj.bias           : False
image_encoder_g.vision_model.encoder.layers.9.layer_norm1.weight                : False
image_encoder_g.vision_model.encoder.layers.9.layer_norm1.bias                  : False
image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.weight                    : False
image_encoder_g.vision_model.encoder.layers.9.mlp.fc1.bias                      : False
image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.weight                    : False
image_encoder_g.vision_model.encoder.layers.9.mlp.fc2.bias                      : False
image_encoder_g.vision_model.encoder.layers.9.layer_norm2.weight                : False
image_encoder_g.vision_model.encoder.layers.9.layer_norm2.bias                  : False
image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.weight          : False
image_encoder_g.vision_model.encoder.layers.10.self_attn.k_proj.bias            : False
image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.weight          : False
image_encoder_g.vision_model.encoder.layers.10.self_attn.v_proj.bias            : False
image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.weight          : False
image_encoder_g.vision_model.encoder.layers.10.self_attn.q_proj.bias            : False
image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.weight        : False
image_encoder_g.vision_model.encoder.layers.10.self_attn.out_proj.bias          : False
image_encoder_g.vision_model.encoder.layers.10.layer_norm1.weight               : False
image_encoder_g.vision_model.encoder.layers.10.layer_norm1.bias                 : False
image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.weight                   : False
image_encoder_g.vision_model.encoder.layers.10.mlp.fc1.bias                     : False
image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.weight                   : False
image_encoder_g.vision_model.encoder.layers.10.mlp.fc2.bias                     : False
image_encoder_g.vision_model.encoder.layers.10.layer_norm2.weight               : False
image_encoder_g.vision_model.encoder.layers.10.layer_norm2.bias                 : False
image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.weight          : False
image_encoder_g.vision_model.encoder.layers.11.self_attn.k_proj.bias            : False
image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.weight          : False
image_encoder_g.vision_model.encoder.layers.11.self_attn.v_proj.bias            : False
image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.weight          : False
image_encoder_g.vision_model.encoder.layers.11.self_attn.q_proj.bias            : False
image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.weight        : False
image_encoder_g.vision_model.encoder.layers.11.self_attn.out_proj.bias          : False
image_encoder_g.vision_model.encoder.layers.11.layer_norm1.weight               : False
image_encoder_g.vision_model.encoder.layers.11.layer_norm1.bias                 : False
image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.weight                   : False
image_encoder_g.vision_model.encoder.layers.11.mlp.fc1.bias                     : False
image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.weight                   : False
image_encoder_g.vision_model.encoder.layers.11.mlp.fc2.bias                     : False
image_encoder_g.vision_model.encoder.layers.11.layer_norm2.weight               : False
image_encoder_g.vision_model.encoder.layers.11.layer_norm2.bias                 : False
image_encoder_g.vision_model.post_layernorm.weight                              : False
image_encoder_g.vision_model.post_layernorm.bias                                : False
image_encoder_g.visual_projection.weight                                        : False
backbone.patch_embed.proj.weight                                                : False
backbone.patch_embed.proj.bias                                                  : False
backbone.patch_embed.norm.weight                                                : False
backbone.patch_embed.norm.bias                                                  : False
backbone.layers.0.blocks.0.norm1.weight                                         : False
backbone.layers.0.blocks.0.norm1.bias                                           : False
backbone.layers.0.blocks.0.attn.relative_position_bias_table                    : False
backbone.layers.0.blocks.0.attn.qkv.weight                                      : False
backbone.layers.0.blocks.0.attn.qkv.bias                                        : False
backbone.layers.0.blocks.0.attn.proj.weight                                     : False
backbone.layers.0.blocks.0.attn.proj.bias                                       : False
backbone.layers.0.blocks.0.norm2.weight                                         : False
backbone.layers.0.blocks.0.norm2.bias                                           : False
backbone.layers.0.blocks.0.mlp.fc1.weight                                       : False
backbone.layers.0.blocks.0.mlp.fc1.bias                                         : False
backbone.layers.0.blocks.0.mlp.fc2.weight                                       : False
backbone.layers.0.blocks.0.mlp.fc2.bias                                         : False
backbone.layers.0.blocks.1.norm1.weight                                         : False
backbone.layers.0.blocks.1.norm1.bias                                           : False
backbone.layers.0.blocks.1.attn.relative_position_bias_table                    : False
backbone.layers.0.blocks.1.attn.qkv.weight                                      : False
backbone.layers.0.blocks.1.attn.qkv.bias                                        : False
backbone.layers.0.blocks.1.attn.proj.weight                                     : False
backbone.layers.0.blocks.1.attn.proj.bias                                       : False
backbone.layers.0.blocks.1.norm2.weight                                         : False
backbone.layers.0.blocks.1.norm2.bias                                           : False
backbone.layers.0.blocks.1.mlp.fc1.weight                                       : False
backbone.layers.0.blocks.1.mlp.fc1.bias                                         : False
backbone.layers.0.blocks.1.mlp.fc2.weight                                       : False
backbone.layers.0.blocks.1.mlp.fc2.bias                                         : False
backbone.layers.0.downsample.reduction.weight                                   : False
backbone.layers.0.downsample.norm.weight                                        : False
backbone.layers.0.downsample.norm.bias                                          : False
backbone.layers.1.blocks.0.norm1.weight                                         : False
backbone.layers.1.blocks.0.norm1.bias                                           : False
backbone.layers.1.blocks.0.attn.relative_position_bias_table                    : False
backbone.layers.1.blocks.0.attn.qkv.weight                                      : False
backbone.layers.1.blocks.0.attn.qkv.bias                                        : False
backbone.layers.1.blocks.0.attn.proj.weight                                     : False
backbone.layers.1.blocks.0.attn.proj.bias                                       : False
backbone.layers.1.blocks.0.norm2.weight                                         : False
backbone.layers.1.blocks.0.norm2.bias                                           : False
backbone.layers.1.blocks.0.mlp.fc1.weight                                       : False
backbone.layers.1.blocks.0.mlp.fc1.bias                                         : False
backbone.layers.1.blocks.0.mlp.fc2.weight                                       : False
backbone.layers.1.blocks.0.mlp.fc2.bias                                         : False
backbone.layers.1.blocks.1.norm1.weight                                         : False
backbone.layers.1.blocks.1.norm1.bias                                           : False
backbone.layers.1.blocks.1.attn.relative_position_bias_table                    : False
backbone.layers.1.blocks.1.attn.qkv.weight                                      : False
backbone.layers.1.blocks.1.attn.qkv.bias                                        : False
backbone.layers.1.blocks.1.attn.proj.weight                                     : False
backbone.layers.1.blocks.1.attn.proj.bias                                       : False
backbone.layers.1.blocks.1.norm2.weight                                         : False
backbone.layers.1.blocks.1.norm2.bias                                           : False
backbone.layers.1.blocks.1.mlp.fc1.weight                                       : False
backbone.layers.1.blocks.1.mlp.fc1.bias                                         : False
backbone.layers.1.blocks.1.mlp.fc2.weight                                       : False
backbone.layers.1.blocks.1.mlp.fc2.bias                                         : False
backbone.layers.1.downsample.reduction.weight                                   : False
backbone.layers.1.downsample.norm.weight                                        : False
backbone.layers.1.downsample.norm.bias                                          : False
backbone.layers.2.blocks.0.norm1.weight                                         : False
backbone.layers.2.blocks.0.norm1.bias                                           : False
backbone.layers.2.blocks.0.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.0.attn.qkv.weight                                      : False
backbone.layers.2.blocks.0.attn.qkv.bias                                        : False
backbone.layers.2.blocks.0.attn.proj.weight                                     : False
backbone.layers.2.blocks.0.attn.proj.bias                                       : False
backbone.layers.2.blocks.0.norm2.weight                                         : False
backbone.layers.2.blocks.0.norm2.bias                                           : False
backbone.layers.2.blocks.0.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.0.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.0.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.0.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.1.norm1.weight                                         : False
backbone.layers.2.blocks.1.norm1.bias                                           : False
backbone.layers.2.blocks.1.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.1.attn.qkv.weight                                      : False
backbone.layers.2.blocks.1.attn.qkv.bias                                        : False
backbone.layers.2.blocks.1.attn.proj.weight                                     : False
backbone.layers.2.blocks.1.attn.proj.bias                                       : False
backbone.layers.2.blocks.1.norm2.weight                                         : False
backbone.layers.2.blocks.1.norm2.bias                                           : False
backbone.layers.2.blocks.1.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.1.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.1.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.1.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.2.norm1.weight                                         : False
backbone.layers.2.blocks.2.norm1.bias                                           : False
backbone.layers.2.blocks.2.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.2.attn.qkv.weight                                      : False
backbone.layers.2.blocks.2.attn.qkv.bias                                        : False
backbone.layers.2.blocks.2.attn.proj.weight                                     : False
backbone.layers.2.blocks.2.attn.proj.bias                                       : False
backbone.layers.2.blocks.2.norm2.weight                                         : False
backbone.layers.2.blocks.2.norm2.bias                                           : False
backbone.layers.2.blocks.2.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.2.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.2.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.2.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.3.norm1.weight                                         : False
backbone.layers.2.blocks.3.norm1.bias                                           : False
backbone.layers.2.blocks.3.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.3.attn.qkv.weight                                      : False
backbone.layers.2.blocks.3.attn.qkv.bias                                        : False
backbone.layers.2.blocks.3.attn.proj.weight                                     : False
backbone.layers.2.blocks.3.attn.proj.bias                                       : False
backbone.layers.2.blocks.3.norm2.weight                                         : False
backbone.layers.2.blocks.3.norm2.bias                                           : False
backbone.layers.2.blocks.3.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.3.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.3.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.3.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.4.norm1.weight                                         : False
backbone.layers.2.blocks.4.norm1.bias                                           : False
backbone.layers.2.blocks.4.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.4.attn.qkv.weight                                      : False
backbone.layers.2.blocks.4.attn.qkv.bias                                        : False
backbone.layers.2.blocks.4.attn.proj.weight                                     : False
backbone.layers.2.blocks.4.attn.proj.bias                                       : False
backbone.layers.2.blocks.4.norm2.weight                                         : False
backbone.layers.2.blocks.4.norm2.bias                                           : False
backbone.layers.2.blocks.4.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.4.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.4.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.4.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.5.norm1.weight                                         : False
backbone.layers.2.blocks.5.norm1.bias                                           : False
backbone.layers.2.blocks.5.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.5.attn.qkv.weight                                      : False
backbone.layers.2.blocks.5.attn.qkv.bias                                        : False
backbone.layers.2.blocks.5.attn.proj.weight                                     : False
backbone.layers.2.blocks.5.attn.proj.bias                                       : False
backbone.layers.2.blocks.5.norm2.weight                                         : False
backbone.layers.2.blocks.5.norm2.bias                                           : False
backbone.layers.2.blocks.5.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.5.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.5.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.5.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.6.norm1.weight                                         : False
backbone.layers.2.blocks.6.norm1.bias                                           : False
backbone.layers.2.blocks.6.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.6.attn.qkv.weight                                      : False
backbone.layers.2.blocks.6.attn.qkv.bias                                        : False
backbone.layers.2.blocks.6.attn.proj.weight                                     : False
backbone.layers.2.blocks.6.attn.proj.bias                                       : False
backbone.layers.2.blocks.6.norm2.weight                                         : False
backbone.layers.2.blocks.6.norm2.bias                                           : False
backbone.layers.2.blocks.6.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.6.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.6.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.6.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.7.norm1.weight                                         : False
backbone.layers.2.blocks.7.norm1.bias                                           : False
backbone.layers.2.blocks.7.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.7.attn.qkv.weight                                      : False
backbone.layers.2.blocks.7.attn.qkv.bias                                        : False
backbone.layers.2.blocks.7.attn.proj.weight                                     : False
backbone.layers.2.blocks.7.attn.proj.bias                                       : False
backbone.layers.2.blocks.7.norm2.weight                                         : False
backbone.layers.2.blocks.7.norm2.bias                                           : False
backbone.layers.2.blocks.7.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.7.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.7.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.7.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.8.norm1.weight                                         : False
backbone.layers.2.blocks.8.norm1.bias                                           : False
backbone.layers.2.blocks.8.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.8.attn.qkv.weight                                      : False
backbone.layers.2.blocks.8.attn.qkv.bias                                        : False
backbone.layers.2.blocks.8.attn.proj.weight                                     : False
backbone.layers.2.blocks.8.attn.proj.bias                                       : False
backbone.layers.2.blocks.8.norm2.weight                                         : False
backbone.layers.2.blocks.8.norm2.bias                                           : False
backbone.layers.2.blocks.8.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.8.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.8.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.8.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.9.norm1.weight                                         : False
backbone.layers.2.blocks.9.norm1.bias                                           : False
backbone.layers.2.blocks.9.attn.relative_position_bias_table                    : False
backbone.layers.2.blocks.9.attn.qkv.weight                                      : False
backbone.layers.2.blocks.9.attn.qkv.bias                                        : False
backbone.layers.2.blocks.9.attn.proj.weight                                     : False
backbone.layers.2.blocks.9.attn.proj.bias                                       : False
backbone.layers.2.blocks.9.norm2.weight                                         : False
backbone.layers.2.blocks.9.norm2.bias                                           : False
backbone.layers.2.blocks.9.mlp.fc1.weight                                       : False
backbone.layers.2.blocks.9.mlp.fc1.bias                                         : False
backbone.layers.2.blocks.9.mlp.fc2.weight                                       : False
backbone.layers.2.blocks.9.mlp.fc2.bias                                         : False
backbone.layers.2.blocks.10.norm1.weight                                        : False
backbone.layers.2.blocks.10.norm1.bias                                          : False
backbone.layers.2.blocks.10.attn.relative_position_bias_table                   : False
backbone.layers.2.blocks.10.attn.qkv.weight                                     : False
backbone.layers.2.blocks.10.attn.qkv.bias                                       : False
backbone.layers.2.blocks.10.attn.proj.weight                                    : False
backbone.layers.2.blocks.10.attn.proj.bias                                      : False
backbone.layers.2.blocks.10.norm2.weight                                        : False
backbone.layers.2.blocks.10.norm2.bias                                          : False
backbone.layers.2.blocks.10.mlp.fc1.weight                                      : False
backbone.layers.2.blocks.10.mlp.fc1.bias                                        : False
backbone.layers.2.blocks.10.mlp.fc2.weight                                      : False
backbone.layers.2.blocks.10.mlp.fc2.bias                                        : False
backbone.layers.2.blocks.11.norm1.weight                                        : False
backbone.layers.2.blocks.11.norm1.bias                                          : False
backbone.layers.2.blocks.11.attn.relative_position_bias_table                   : False
backbone.layers.2.blocks.11.attn.qkv.weight                                     : False
backbone.layers.2.blocks.11.attn.qkv.bias                                       : False
backbone.layers.2.blocks.11.attn.proj.weight                                    : False
backbone.layers.2.blocks.11.attn.proj.bias                                      : False
backbone.layers.2.blocks.11.norm2.weight                                        : False
backbone.layers.2.blocks.11.norm2.bias                                          : False
backbone.layers.2.blocks.11.mlp.fc1.weight                                      : False
backbone.layers.2.blocks.11.mlp.fc1.bias                                        : False
backbone.layers.2.blocks.11.mlp.fc2.weight                                      : False
backbone.layers.2.blocks.11.mlp.fc2.bias                                        : False
backbone.layers.2.blocks.12.norm1.weight                                        : False
backbone.layers.2.blocks.12.norm1.bias                                          : False
backbone.layers.2.blocks.12.attn.relative_position_bias_table                   : False
backbone.layers.2.blocks.12.attn.qkv.weight                                     : False
backbone.layers.2.blocks.12.attn.qkv.bias                                       : False
backbone.layers.2.blocks.12.attn.proj.weight                                    : False
backbone.layers.2.blocks.12.attn.proj.bias                                      : False
backbone.layers.2.blocks.12.norm2.weight                                        : False
backbone.layers.2.blocks.12.norm2.bias                                          : False
backbone.layers.2.blocks.12.mlp.fc1.weight                                      : False
backbone.layers.2.blocks.12.mlp.fc1.bias                                        : False
backbone.layers.2.blocks.12.mlp.fc2.weight                                      : False
backbone.layers.2.blocks.12.mlp.fc2.bias                                        : False
backbone.layers.2.blocks.13.norm1.weight                                        : False
backbone.layers.2.blocks.13.norm1.bias                                          : False
backbone.layers.2.blocks.13.attn.relative_position_bias_table                   : False
backbone.layers.2.blocks.13.attn.qkv.weight                                     : False
backbone.layers.2.blocks.13.attn.qkv.bias                                       : False
backbone.layers.2.blocks.13.attn.proj.weight                                    : False
backbone.layers.2.blocks.13.attn.proj.bias                                      : False
backbone.layers.2.blocks.13.norm2.weight                                        : False
backbone.layers.2.blocks.13.norm2.bias                                          : False
backbone.layers.2.blocks.13.mlp.fc1.weight                                      : False
backbone.layers.2.blocks.13.mlp.fc1.bias                                        : False
backbone.layers.2.blocks.13.mlp.fc2.weight                                      : False
backbone.layers.2.blocks.13.mlp.fc2.bias                                        : False
backbone.layers.2.blocks.14.norm1.weight                                        : False
backbone.layers.2.blocks.14.norm1.bias                                          : False
backbone.layers.2.blocks.14.attn.relative_position_bias_table                   : False
backbone.layers.2.blocks.14.attn.qkv.weight                                     : False
backbone.layers.2.blocks.14.attn.qkv.bias                                       : False
backbone.layers.2.blocks.14.attn.proj.weight                                    : False
backbone.layers.2.blocks.14.attn.proj.bias                                      : False
backbone.layers.2.blocks.14.norm2.weight                                        : False
backbone.layers.2.blocks.14.norm2.bias                                          : False
backbone.layers.2.blocks.14.mlp.fc1.weight                                      : False
backbone.layers.2.blocks.14.mlp.fc1.bias                                        : False
backbone.layers.2.blocks.14.mlp.fc2.weight                                      : False
backbone.layers.2.blocks.14.mlp.fc2.bias                                        : False
backbone.layers.2.blocks.15.norm1.weight                                        : False
backbone.layers.2.blocks.15.norm1.bias                                          : False
backbone.layers.2.blocks.15.attn.relative_position_bias_table                   : False
backbone.layers.2.blocks.15.attn.qkv.weight                                     : False
backbone.layers.2.blocks.15.attn.qkv.bias                                       : False
backbone.layers.2.blocks.15.attn.proj.weight                                    : False
backbone.layers.2.blocks.15.attn.proj.bias                                      : False
backbone.layers.2.blocks.15.norm2.weight                                        : False
backbone.layers.2.blocks.15.norm2.bias                                          : False
backbone.layers.2.blocks.15.mlp.fc1.weight                                      : False
backbone.layers.2.blocks.15.mlp.fc1.bias                                        : False
backbone.layers.2.blocks.15.mlp.fc2.weight                                      : False
backbone.layers.2.blocks.15.mlp.fc2.bias                                        : False
backbone.layers.2.blocks.16.norm1.weight                                        : False
backbone.layers.2.blocks.16.norm1.bias                                          : False
backbone.layers.2.blocks.16.attn.relative_position_bias_table                   : False
backbone.layers.2.blocks.16.attn.qkv.weight                                     : False
backbone.layers.2.blocks.16.attn.qkv.bias                                       : False
backbone.layers.2.blocks.16.attn.proj.weight                                    : False
backbone.layers.2.blocks.16.attn.proj.bias                                      : False
backbone.layers.2.blocks.16.norm2.weight                                        : False
backbone.layers.2.blocks.16.norm2.bias                                          : False
backbone.layers.2.blocks.16.mlp.fc1.weight                                      : False
backbone.layers.2.blocks.16.mlp.fc1.bias                                        : False
backbone.layers.2.blocks.16.mlp.fc2.weight                                      : False
backbone.layers.2.blocks.16.mlp.fc2.bias                                        : False
backbone.layers.2.blocks.17.norm1.weight                                        : False
backbone.layers.2.blocks.17.norm1.bias                                          : False
backbone.layers.2.blocks.17.attn.relative_position_bias_table                   : False
backbone.layers.2.blocks.17.attn.qkv.weight                                     : False
backbone.layers.2.blocks.17.attn.qkv.bias                                       : False
backbone.layers.2.blocks.17.attn.proj.weight                                    : False
backbone.layers.2.blocks.17.attn.proj.bias                                      : False
backbone.layers.2.blocks.17.norm2.weight                                        : False
backbone.layers.2.blocks.17.norm2.bias                                          : False
backbone.layers.2.blocks.17.mlp.fc1.weight                                      : False
backbone.layers.2.blocks.17.mlp.fc1.bias                                        : False
backbone.layers.2.blocks.17.mlp.fc2.weight                                      : False
backbone.layers.2.blocks.17.mlp.fc2.bias                                        : False
backbone.layers.2.downsample.reduction.weight                                   : False
backbone.layers.2.downsample.norm.weight                                        : False
backbone.layers.2.downsample.norm.bias                                          : False
backbone.layers.3.blocks.0.norm1.weight                                         : False
backbone.layers.3.blocks.0.norm1.bias                                           : False
backbone.layers.3.blocks.0.attn.relative_position_bias_table                    : False
backbone.layers.3.blocks.0.attn.qkv.weight                                      : False
backbone.layers.3.blocks.0.attn.qkv.bias                                        : False
backbone.layers.3.blocks.0.attn.proj.weight                                     : False
backbone.layers.3.blocks.0.attn.proj.bias                                       : False
backbone.layers.3.blocks.0.norm2.weight                                         : False
backbone.layers.3.blocks.0.norm2.bias                                           : False
backbone.layers.3.blocks.0.mlp.fc1.weight                                       : False
backbone.layers.3.blocks.0.mlp.fc1.bias                                         : False
backbone.layers.3.blocks.0.mlp.fc2.weight                                       : False
backbone.layers.3.blocks.0.mlp.fc2.bias                                         : False
backbone.layers.3.blocks.1.norm1.weight                                         : False
backbone.layers.3.blocks.1.norm1.bias                                           : False
backbone.layers.3.blocks.1.attn.relative_position_bias_table                    : False
backbone.layers.3.blocks.1.attn.qkv.weight                                      : False
backbone.layers.3.blocks.1.attn.qkv.bias                                        : False
backbone.layers.3.blocks.1.attn.proj.weight                                     : False
backbone.layers.3.blocks.1.attn.proj.bias                                       : False
backbone.layers.3.blocks.1.norm2.weight                                         : False
backbone.layers.3.blocks.1.norm2.bias                                           : False
backbone.layers.3.blocks.1.mlp.fc1.weight                                       : False
backbone.layers.3.blocks.1.mlp.fc1.bias                                         : False
backbone.layers.3.blocks.1.mlp.fc2.weight                                       : False
backbone.layers.3.blocks.1.mlp.fc2.bias                                         : False
appearance_encoder.blocks.0.0.norm1.weight                                      : False
appearance_encoder.blocks.0.0.norm1.bias                                        : False
appearance_encoder.blocks.0.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.0.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.0.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.0.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.0.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.0.0.norm2.weight                                      : False
appearance_encoder.blocks.0.0.norm2.bias                                        : False
appearance_encoder.blocks.0.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.0.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.0.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.0.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.0.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.0.0.norm3.weight                                      : False
appearance_encoder.blocks.0.0.norm3.bias                                        : False
appearance_encoder.blocks.0.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.0.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.0.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.0.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.0.1.norm1.weight                                      : False
appearance_encoder.blocks.0.1.norm1.bias                                        : False
appearance_encoder.blocks.0.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.0.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.0.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.0.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.0.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.0.1.norm2.weight                                      : False
appearance_encoder.blocks.0.1.norm2.bias                                        : False
appearance_encoder.blocks.0.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.0.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.0.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.0.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.0.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.0.1.norm3.weight                                      : False
appearance_encoder.blocks.0.1.norm3.bias                                        : False
appearance_encoder.blocks.0.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.0.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.0.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.0.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.0.2.norm1.weight                                      : False
appearance_encoder.blocks.0.2.norm1.bias                                        : False
appearance_encoder.blocks.0.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.0.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.0.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.0.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.0.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.0.2.norm2.weight                                      : False
appearance_encoder.blocks.0.2.norm2.bias                                        : False
appearance_encoder.blocks.0.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.0.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.0.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.0.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.0.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.0.2.norm3.weight                                      : False
appearance_encoder.blocks.0.2.norm3.bias                                        : False
appearance_encoder.blocks.0.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.0.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.0.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.0.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.0.3.norm1.weight                                      : False
appearance_encoder.blocks.0.3.norm1.bias                                        : False
appearance_encoder.blocks.0.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.0.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.0.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.0.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.0.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.0.3.norm2.weight                                      : False
appearance_encoder.blocks.0.3.norm2.bias                                        : False
appearance_encoder.blocks.0.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.0.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.0.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.0.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.0.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.0.3.norm3.weight                                      : False
appearance_encoder.blocks.0.3.norm3.bias                                        : False
appearance_encoder.blocks.0.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.0.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.0.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.0.3.ff.net.2.bias                                     : False
appearance_encoder.blocks.1.0.norm1.weight                                      : False
appearance_encoder.blocks.1.0.norm1.bias                                        : False
appearance_encoder.blocks.1.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.1.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.1.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.1.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.1.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.1.0.norm2.weight                                      : False
appearance_encoder.blocks.1.0.norm2.bias                                        : False
appearance_encoder.blocks.1.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.1.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.1.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.1.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.1.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.1.0.norm3.weight                                      : False
appearance_encoder.blocks.1.0.norm3.bias                                        : False
appearance_encoder.blocks.1.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.1.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.1.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.1.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.1.1.norm1.weight                                      : False
appearance_encoder.blocks.1.1.norm1.bias                                        : False
appearance_encoder.blocks.1.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.1.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.1.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.1.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.1.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.1.1.norm2.weight                                      : False
appearance_encoder.blocks.1.1.norm2.bias                                        : False
appearance_encoder.blocks.1.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.1.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.1.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.1.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.1.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.1.1.norm3.weight                                      : False
appearance_encoder.blocks.1.1.norm3.bias                                        : False
appearance_encoder.blocks.1.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.1.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.1.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.1.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.1.2.norm1.weight                                      : False
appearance_encoder.blocks.1.2.norm1.bias                                        : False
appearance_encoder.blocks.1.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.1.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.1.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.1.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.1.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.1.2.norm2.weight                                      : False
appearance_encoder.blocks.1.2.norm2.bias                                        : False
appearance_encoder.blocks.1.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.1.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.1.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.1.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.1.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.1.2.norm3.weight                                      : False
appearance_encoder.blocks.1.2.norm3.bias                                        : False
appearance_encoder.blocks.1.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.1.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.1.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.1.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.1.3.norm1.weight                                      : False
appearance_encoder.blocks.1.3.norm1.bias                                        : False
appearance_encoder.blocks.1.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.1.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.1.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.1.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.1.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.1.3.norm2.weight                                      : False
appearance_encoder.blocks.1.3.norm2.bias                                        : False
appearance_encoder.blocks.1.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.1.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.1.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.1.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.1.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.1.3.norm3.weight                                      : False
appearance_encoder.blocks.1.3.norm3.bias                                        : False
appearance_encoder.blocks.1.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.1.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.1.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.1.3.ff.net.2.bias                                     : False
appearance_encoder.blocks.2.0.norm1.weight                                      : False
appearance_encoder.blocks.2.0.norm1.bias                                        : False
appearance_encoder.blocks.2.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.2.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.2.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.2.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.2.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.2.0.norm2.weight                                      : False
appearance_encoder.blocks.2.0.norm2.bias                                        : False
appearance_encoder.blocks.2.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.2.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.2.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.2.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.2.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.2.0.norm3.weight                                      : False
appearance_encoder.blocks.2.0.norm3.bias                                        : False
appearance_encoder.blocks.2.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.2.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.2.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.2.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.2.1.norm1.weight                                      : False
appearance_encoder.blocks.2.1.norm1.bias                                        : False
appearance_encoder.blocks.2.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.2.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.2.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.2.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.2.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.2.1.norm2.weight                                      : False
appearance_encoder.blocks.2.1.norm2.bias                                        : False
appearance_encoder.blocks.2.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.2.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.2.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.2.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.2.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.2.1.norm3.weight                                      : False
appearance_encoder.blocks.2.1.norm3.bias                                        : False
appearance_encoder.blocks.2.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.2.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.2.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.2.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.2.2.norm1.weight                                      : False
appearance_encoder.blocks.2.2.norm1.bias                                        : False
appearance_encoder.blocks.2.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.2.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.2.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.2.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.2.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.2.2.norm2.weight                                      : False
appearance_encoder.blocks.2.2.norm2.bias                                        : False
appearance_encoder.blocks.2.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.2.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.2.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.2.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.2.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.2.2.norm3.weight                                      : False
appearance_encoder.blocks.2.2.norm3.bias                                        : False
appearance_encoder.blocks.2.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.2.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.2.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.2.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.2.3.norm1.weight                                      : False
appearance_encoder.blocks.2.3.norm1.bias                                        : False
appearance_encoder.blocks.2.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.2.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.2.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.2.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.2.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.2.3.norm2.weight                                      : False
appearance_encoder.blocks.2.3.norm2.bias                                        : False
appearance_encoder.blocks.2.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.2.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.2.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.2.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.2.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.2.3.norm3.weight                                      : False
appearance_encoder.blocks.2.3.norm3.bias                                        : False
appearance_encoder.blocks.2.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.2.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.2.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.2.3.ff.net.2.bias                                     : False
appearance_encoder.blocks.3.0.norm1.weight                                      : False
appearance_encoder.blocks.3.0.norm1.bias                                        : False
appearance_encoder.blocks.3.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.3.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.3.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.3.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.3.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.3.0.norm2.weight                                      : False
appearance_encoder.blocks.3.0.norm2.bias                                        : False
appearance_encoder.blocks.3.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.3.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.3.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.3.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.3.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.3.0.norm3.weight                                      : False
appearance_encoder.blocks.3.0.norm3.bias                                        : False
appearance_encoder.blocks.3.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.3.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.3.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.3.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.3.1.norm1.weight                                      : False
appearance_encoder.blocks.3.1.norm1.bias                                        : False
appearance_encoder.blocks.3.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.3.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.3.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.3.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.3.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.3.1.norm2.weight                                      : False
appearance_encoder.blocks.3.1.norm2.bias                                        : False
appearance_encoder.blocks.3.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.3.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.3.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.3.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.3.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.3.1.norm3.weight                                      : False
appearance_encoder.blocks.3.1.norm3.bias                                        : False
appearance_encoder.blocks.3.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.3.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.3.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.3.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.3.2.norm1.weight                                      : False
appearance_encoder.blocks.3.2.norm1.bias                                        : False
appearance_encoder.blocks.3.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.3.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.3.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.3.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.3.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.3.2.norm2.weight                                      : False
appearance_encoder.blocks.3.2.norm2.bias                                        : False
appearance_encoder.blocks.3.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.3.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.3.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.3.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.3.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.3.2.norm3.weight                                      : False
appearance_encoder.blocks.3.2.norm3.bias                                        : False
appearance_encoder.blocks.3.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.3.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.3.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.3.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.3.3.norm1.weight                                      : False
appearance_encoder.blocks.3.3.norm1.bias                                        : False
appearance_encoder.blocks.3.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.3.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.3.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.3.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.3.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.3.3.norm2.weight                                      : False
appearance_encoder.blocks.3.3.norm2.bias                                        : False
appearance_encoder.blocks.3.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.3.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.3.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.3.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.3.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.3.3.norm3.weight                                      : False
appearance_encoder.blocks.3.3.norm3.bias                                        : False
appearance_encoder.blocks.3.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.3.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.3.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.3.3.ff.net.2.bias                                     : False
appearance_encoder.blocks.4.0.norm1.weight                                      : False
appearance_encoder.blocks.4.0.norm1.bias                                        : False
appearance_encoder.blocks.4.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.4.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.4.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.4.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.4.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.4.0.norm2.weight                                      : False
appearance_encoder.blocks.4.0.norm2.bias                                        : False
appearance_encoder.blocks.4.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.4.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.4.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.4.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.4.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.4.0.norm3.weight                                      : False
appearance_encoder.blocks.4.0.norm3.bias                                        : False
appearance_encoder.blocks.4.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.4.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.4.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.4.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.4.1.norm1.weight                                      : False
appearance_encoder.blocks.4.1.norm1.bias                                        : False
appearance_encoder.blocks.4.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.4.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.4.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.4.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.4.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.4.1.norm2.weight                                      : False
appearance_encoder.blocks.4.1.norm2.bias                                        : False
appearance_encoder.blocks.4.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.4.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.4.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.4.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.4.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.4.1.norm3.weight                                      : False
appearance_encoder.blocks.4.1.norm3.bias                                        : False
appearance_encoder.blocks.4.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.4.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.4.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.4.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.4.2.norm1.weight                                      : False
appearance_encoder.blocks.4.2.norm1.bias                                        : False
appearance_encoder.blocks.4.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.4.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.4.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.4.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.4.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.4.2.norm2.weight                                      : False
appearance_encoder.blocks.4.2.norm2.bias                                        : False
appearance_encoder.blocks.4.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.4.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.4.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.4.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.4.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.4.2.norm3.weight                                      : False
appearance_encoder.blocks.4.2.norm3.bias                                        : False
appearance_encoder.blocks.4.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.4.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.4.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.4.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.4.3.norm1.weight                                      : False
appearance_encoder.blocks.4.3.norm1.bias                                        : False
appearance_encoder.blocks.4.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.4.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.4.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.4.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.4.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.4.3.norm2.weight                                      : False
appearance_encoder.blocks.4.3.norm2.bias                                        : False
appearance_encoder.blocks.4.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.4.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.4.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.4.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.4.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.4.3.norm3.weight                                      : False
appearance_encoder.blocks.4.3.norm3.bias                                        : False
appearance_encoder.blocks.4.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.4.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.4.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.4.3.ff.net.2.bias                                     : False
appearance_encoder.blocks.5.0.norm1.weight                                      : False
appearance_encoder.blocks.5.0.norm1.bias                                        : False
appearance_encoder.blocks.5.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.5.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.5.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.5.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.5.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.5.0.norm2.weight                                      : False
appearance_encoder.blocks.5.0.norm2.bias                                        : False
appearance_encoder.blocks.5.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.5.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.5.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.5.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.5.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.5.0.norm3.weight                                      : False
appearance_encoder.blocks.5.0.norm3.bias                                        : False
appearance_encoder.blocks.5.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.5.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.5.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.5.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.5.1.norm1.weight                                      : False
appearance_encoder.blocks.5.1.norm1.bias                                        : False
appearance_encoder.blocks.5.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.5.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.5.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.5.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.5.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.5.1.norm2.weight                                      : False
appearance_encoder.blocks.5.1.norm2.bias                                        : False
appearance_encoder.blocks.5.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.5.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.5.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.5.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.5.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.5.1.norm3.weight                                      : False
appearance_encoder.blocks.5.1.norm3.bias                                        : False
appearance_encoder.blocks.5.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.5.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.5.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.5.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.5.2.norm1.weight                                      : False
appearance_encoder.blocks.5.2.norm1.bias                                        : False
appearance_encoder.blocks.5.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.5.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.5.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.5.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.5.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.5.2.norm2.weight                                      : False
appearance_encoder.blocks.5.2.norm2.bias                                        : False
appearance_encoder.blocks.5.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.5.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.5.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.5.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.5.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.5.2.norm3.weight                                      : False
appearance_encoder.blocks.5.2.norm3.bias                                        : False
appearance_encoder.blocks.5.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.5.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.5.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.5.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.5.3.norm1.weight                                      : False
appearance_encoder.blocks.5.3.norm1.bias                                        : False
appearance_encoder.blocks.5.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.5.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.5.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.5.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.5.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.5.3.norm2.weight                                      : False
appearance_encoder.blocks.5.3.norm2.bias                                        : False
appearance_encoder.blocks.5.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.5.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.5.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.5.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.5.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.5.3.norm3.weight                                      : False
appearance_encoder.blocks.5.3.norm3.bias                                        : False
appearance_encoder.blocks.5.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.5.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.5.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.5.3.ff.net.2.bias                                     : False
appearance_encoder.blocks.6.0.norm1.weight                                      : False
appearance_encoder.blocks.6.0.norm1.bias                                        : False
appearance_encoder.blocks.6.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.6.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.6.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.6.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.6.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.6.0.norm2.weight                                      : False
appearance_encoder.blocks.6.0.norm2.bias                                        : False
appearance_encoder.blocks.6.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.6.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.6.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.6.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.6.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.6.0.norm3.weight                                      : False
appearance_encoder.blocks.6.0.norm3.bias                                        : False
appearance_encoder.blocks.6.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.6.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.6.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.6.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.6.1.norm1.weight                                      : False
appearance_encoder.blocks.6.1.norm1.bias                                        : False
appearance_encoder.blocks.6.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.6.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.6.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.6.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.6.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.6.1.norm2.weight                                      : False
appearance_encoder.blocks.6.1.norm2.bias                                        : False
appearance_encoder.blocks.6.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.6.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.6.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.6.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.6.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.6.1.norm3.weight                                      : False
appearance_encoder.blocks.6.1.norm3.bias                                        : False
appearance_encoder.blocks.6.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.6.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.6.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.6.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.6.2.norm1.weight                                      : False
appearance_encoder.blocks.6.2.norm1.bias                                        : False
appearance_encoder.blocks.6.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.6.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.6.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.6.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.6.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.6.2.norm2.weight                                      : False
appearance_encoder.blocks.6.2.norm2.bias                                        : False
appearance_encoder.blocks.6.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.6.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.6.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.6.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.6.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.6.2.norm3.weight                                      : False
appearance_encoder.blocks.6.2.norm3.bias                                        : False
appearance_encoder.blocks.6.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.6.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.6.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.6.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.6.3.norm1.weight                                      : False
appearance_encoder.blocks.6.3.norm1.bias                                        : False
appearance_encoder.blocks.6.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.6.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.6.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.6.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.6.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.6.3.norm2.weight                                      : False
appearance_encoder.blocks.6.3.norm2.bias                                        : False
appearance_encoder.blocks.6.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.6.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.6.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.6.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.6.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.6.3.norm3.weight                                      : False
appearance_encoder.blocks.6.3.norm3.bias                                        : False
appearance_encoder.blocks.6.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.6.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.6.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.6.3.ff.net.2.bias                                     : False
appearance_encoder.blocks.7.0.norm1.weight                                      : False
appearance_encoder.blocks.7.0.norm1.bias                                        : False
appearance_encoder.blocks.7.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.7.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.7.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.7.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.7.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.7.0.norm2.weight                                      : False
appearance_encoder.blocks.7.0.norm2.bias                                        : False
appearance_encoder.blocks.7.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.7.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.7.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.7.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.7.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.7.0.norm3.weight                                      : False
appearance_encoder.blocks.7.0.norm3.bias                                        : False
appearance_encoder.blocks.7.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.7.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.7.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.7.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.7.1.norm1.weight                                      : False
appearance_encoder.blocks.7.1.norm1.bias                                        : False
appearance_encoder.blocks.7.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.7.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.7.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.7.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.7.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.7.1.norm2.weight                                      : False
appearance_encoder.blocks.7.1.norm2.bias                                        : False
appearance_encoder.blocks.7.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.7.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.7.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.7.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.7.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.7.1.norm3.weight                                      : False
appearance_encoder.blocks.7.1.norm3.bias                                        : False
appearance_encoder.blocks.7.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.7.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.7.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.7.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.7.2.norm1.weight                                      : False
appearance_encoder.blocks.7.2.norm1.bias                                        : False
appearance_encoder.blocks.7.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.7.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.7.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.7.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.7.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.7.2.norm2.weight                                      : False
appearance_encoder.blocks.7.2.norm2.bias                                        : False
appearance_encoder.blocks.7.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.7.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.7.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.7.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.7.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.7.2.norm3.weight                                      : False
appearance_encoder.blocks.7.2.norm3.bias                                        : False
appearance_encoder.blocks.7.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.7.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.7.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.7.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.7.3.norm1.weight                                      : False
appearance_encoder.blocks.7.3.norm1.bias                                        : False
appearance_encoder.blocks.7.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.7.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.7.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.7.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.7.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.7.3.norm2.weight                                      : False
appearance_encoder.blocks.7.3.norm2.bias                                        : False
appearance_encoder.blocks.7.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.7.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.7.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.7.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.7.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.7.3.norm3.weight                                      : False
appearance_encoder.blocks.7.3.norm3.bias                                        : False
appearance_encoder.blocks.7.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.7.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.7.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.7.3.ff.net.2.bias                                     : False
appearance_encoder.blocks.8.0.norm1.weight                                      : False
appearance_encoder.blocks.8.0.norm1.bias                                        : False
appearance_encoder.blocks.8.0.attn1.to_q.weight                                 : False
appearance_encoder.blocks.8.0.attn1.to_k.weight                                 : False
appearance_encoder.blocks.8.0.attn1.to_v.weight                                 : False
appearance_encoder.blocks.8.0.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.8.0.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.8.0.norm2.weight                                      : False
appearance_encoder.blocks.8.0.norm2.bias                                        : False
appearance_encoder.blocks.8.0.attn2.to_q.weight                                 : False
appearance_encoder.blocks.8.0.attn2.to_k.weight                                 : False
appearance_encoder.blocks.8.0.attn2.to_v.weight                                 : False
appearance_encoder.blocks.8.0.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.8.0.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.8.0.norm3.weight                                      : False
appearance_encoder.blocks.8.0.norm3.bias                                        : False
appearance_encoder.blocks.8.0.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.8.0.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.8.0.ff.net.2.weight                                   : False
appearance_encoder.blocks.8.0.ff.net.2.bias                                     : False
appearance_encoder.blocks.8.1.norm1.weight                                      : False
appearance_encoder.blocks.8.1.norm1.bias                                        : False
appearance_encoder.blocks.8.1.attn1.to_q.weight                                 : False
appearance_encoder.blocks.8.1.attn1.to_k.weight                                 : False
appearance_encoder.blocks.8.1.attn1.to_v.weight                                 : False
appearance_encoder.blocks.8.1.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.8.1.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.8.1.norm2.weight                                      : False
appearance_encoder.blocks.8.1.norm2.bias                                        : False
appearance_encoder.blocks.8.1.attn2.to_q.weight                                 : False
appearance_encoder.blocks.8.1.attn2.to_k.weight                                 : False
appearance_encoder.blocks.8.1.attn2.to_v.weight                                 : False
appearance_encoder.blocks.8.1.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.8.1.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.8.1.norm3.weight                                      : False
appearance_encoder.blocks.8.1.norm3.bias                                        : False
appearance_encoder.blocks.8.1.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.8.1.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.8.1.ff.net.2.weight                                   : False
appearance_encoder.blocks.8.1.ff.net.2.bias                                     : False
appearance_encoder.blocks.8.2.norm1.weight                                      : False
appearance_encoder.blocks.8.2.norm1.bias                                        : False
appearance_encoder.blocks.8.2.attn1.to_q.weight                                 : False
appearance_encoder.blocks.8.2.attn1.to_k.weight                                 : False
appearance_encoder.blocks.8.2.attn1.to_v.weight                                 : False
appearance_encoder.blocks.8.2.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.8.2.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.8.2.norm2.weight                                      : False
appearance_encoder.blocks.8.2.norm2.bias                                        : False
appearance_encoder.blocks.8.2.attn2.to_q.weight                                 : False
appearance_encoder.blocks.8.2.attn2.to_k.weight                                 : False
appearance_encoder.blocks.8.2.attn2.to_v.weight                                 : False
appearance_encoder.blocks.8.2.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.8.2.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.8.2.norm3.weight                                      : False
appearance_encoder.blocks.8.2.norm3.bias                                        : False
appearance_encoder.blocks.8.2.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.8.2.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.8.2.ff.net.2.weight                                   : False
appearance_encoder.blocks.8.2.ff.net.2.bias                                     : False
appearance_encoder.blocks.8.3.norm1.weight                                      : False
appearance_encoder.blocks.8.3.norm1.bias                                        : False
appearance_encoder.blocks.8.3.attn1.to_q.weight                                 : False
appearance_encoder.blocks.8.3.attn1.to_k.weight                                 : False
appearance_encoder.blocks.8.3.attn1.to_v.weight                                 : False
appearance_encoder.blocks.8.3.attn1.to_out.0.weight                             : False
appearance_encoder.blocks.8.3.attn1.to_out.0.bias                               : False
appearance_encoder.blocks.8.3.norm2.weight                                      : False
appearance_encoder.blocks.8.3.norm2.bias                                        : False
appearance_encoder.blocks.8.3.attn2.to_q.weight                                 : False
appearance_encoder.blocks.8.3.attn2.to_k.weight                                 : False
appearance_encoder.blocks.8.3.attn2.to_v.weight                                 : False
appearance_encoder.blocks.8.3.attn2.to_out.0.weight                             : False
appearance_encoder.blocks.8.3.attn2.to_out.0.bias                               : False
appearance_encoder.blocks.8.3.norm3.weight                                      : False
appearance_encoder.blocks.8.3.norm3.bias                                        : False
appearance_encoder.blocks.8.3.ff.net.0.proj.weight                              : False
appearance_encoder.blocks.8.3.ff.net.0.proj.bias                                : False
appearance_encoder.blocks.8.3.ff.net.2.weight                                   : False
appearance_encoder.blocks.8.3.ff.net.2.bias                                     : False
appearance_encoder.zero_conv_ins.0.weight                                       : False
appearance_encoder.zero_conv_ins.0.bias                                         : False
appearance_encoder.zero_conv_ins.1.weight                                       : False
appearance_encoder.zero_conv_ins.1.bias                                         : False
appearance_encoder.zero_conv_ins.2.weight                                       : False
appearance_encoder.zero_conv_ins.2.bias                                         : False
appearance_encoder.zero_conv_ins.3.weight                                       : False
appearance_encoder.zero_conv_ins.3.bias                                         : False
appearance_encoder.zero_conv_ins.4.weight                                       : False
appearance_encoder.zero_conv_ins.4.bias                                         : False
appearance_encoder.zero_conv_ins.5.weight                                       : False
appearance_encoder.zero_conv_ins.5.bias                                         : False
appearance_encoder.zero_conv_ins.6.weight                                       : False
appearance_encoder.zero_conv_ins.6.bias                                         : False
appearance_encoder.zero_conv_ins.7.weight                                       : False
appearance_encoder.zero_conv_ins.7.bias                                         : False
appearance_encoder.zero_conv_ins.8.weight                                       : False
appearance_encoder.zero_conv_ins.8.bias                                         : False
appearance_encoder.zero_conv_outs.0.weight                                      : False
appearance_encoder.zero_conv_outs.0.bias                                        : False
appearance_encoder.zero_conv_outs.1.weight                                      : False
appearance_encoder.zero_conv_outs.1.bias                                        : False
appearance_encoder.zero_conv_outs.2.weight                                      : False
appearance_encoder.zero_conv_outs.2.bias                                        : False
appearance_encoder.zero_conv_outs.3.weight                                      : False
appearance_encoder.zero_conv_outs.3.bias                                        : False
appearance_encoder.zero_conv_outs.4.weight                                      : False
appearance_encoder.zero_conv_outs.4.bias                                        : False
appearance_encoder.zero_conv_outs.5.weight                                      : False
appearance_encoder.zero_conv_outs.5.bias                                        : False
appearance_encoder.zero_conv_outs.6.weight                                      : False
appearance_encoder.zero_conv_outs.6.bias                                        : False
appearance_encoder.zero_conv_outs.7.weight                                      : False
appearance_encoder.zero_conv_outs.7.bias                                        : False
appearance_encoder.zero_conv_outs.8.weight                                      : False
appearance_encoder.zero_conv_outs.8.bias                                        : False
pose_encoder.conv_in.weight                                                     : False
pose_encoder.conv_in.bias                                                       : False
pose_encoder.resnets.0.norm1.weight                                             : False
pose_encoder.resnets.0.norm1.bias                                               : False
pose_encoder.resnets.0.conv1.weight                                             : False
pose_encoder.resnets.0.conv1.bias                                               : False
pose_encoder.resnets.0.norm2.weight                                             : False
pose_encoder.resnets.0.norm2.bias                                               : False
pose_encoder.resnets.0.conv2.weight                                             : False
pose_encoder.resnets.0.conv2.bias                                               : False
pose_encoder.resnets.1.norm1.weight                                             : False
pose_encoder.resnets.1.norm1.bias                                               : False
pose_encoder.resnets.1.conv1.weight                                             : False
pose_encoder.resnets.1.conv1.bias                                               : False
pose_encoder.resnets.1.norm2.weight                                             : False
pose_encoder.resnets.1.norm2.bias                                               : False
pose_encoder.resnets.1.conv2.weight                                             : False
pose_encoder.resnets.1.conv2.bias                                               : False
pose_encoder.resnets.1.conv_shortcut.weight                                     : False
pose_encoder.resnets.1.conv_shortcut.bias                                       : False
pose_encoder.resnets.2.norm1.weight                                             : False
pose_encoder.resnets.2.norm1.bias                                               : False
pose_encoder.resnets.2.conv1.weight                                             : False
pose_encoder.resnets.2.conv1.bias                                               : False
pose_encoder.resnets.2.norm2.weight                                             : False
pose_encoder.resnets.2.norm2.bias                                               : False
pose_encoder.resnets.2.conv2.weight                                             : False
pose_encoder.resnets.2.conv2.bias                                               : False
pose_encoder.resnets.2.conv_shortcut.weight                                     : False
pose_encoder.resnets.2.conv_shortcut.bias                                       : False
========================unet=======================:
ExtendedUNet(
  (model): ExtendedResidualUNet2DConditionModel(
    (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (time_proj): Timesteps()
    (time_embedding): TimestepEmbedding(
      (linear_1): Linear(in_features=320, out_features=1280, bias=True)
      (act): SiLU()
      (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
    )
    (down_blocks): ModuleList(
      (0): CrossAttnDownBlock2D(
        (attentions): ModuleList(
          (0-1): 2 x Transformer2DModel(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (attn1): Attention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (attn2): Attention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=768, out_features=320, bias=False)
                  (to_v): Linear(in_features=768, out_features=320, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (ff): FeedForward(
                  (net): ModuleList(
                    (0): GEGLU(
                      (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)
                  )
                )
              )
            )
            (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (resnets): ModuleList(
          (0-1): 2 x ResnetBlock2D(
            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)
            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
          )
        )
        (downsamplers): ModuleList(
          (0): Downsample2D(
            (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
      )
      (1): CrossAttnDownBlock2D(
        (attentions): ModuleList(
          (0-1): 2 x Transformer2DModel(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (attn1): Attention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (attn2): Attention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=768, out_features=640, bias=False)
                  (to_v): Linear(in_features=768, out_features=640, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (ff): FeedForward(
                  (net): ModuleList(
                    (0): GEGLU(
                      (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)
                  )
                )
              )
            )
            (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (resnets): ModuleList(
          (0): ResnetBlock2D(
            (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)
            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock2D(
            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)
            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
          )
        )
        (downsamplers): ModuleList(
          (0): Downsample2D(
            (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
      )
      (2): CrossAttnDownBlock2D(
        (attentions): ModuleList(
          (0-1): 2 x Transformer2DModel(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): BasicTransformerBlock(
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (attn1): Attention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (attn2): Attention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=768, out_features=1280, bias=False)
                  (to_v): Linear(in_features=768, out_features=1280, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (ff): FeedForward(
                  (net): ModuleList(
                    (0): GEGLU(
                      (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
                  )
                )
              )
            )
            (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (resnets): ModuleList(
          (0): ResnetBlock2D(
            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)
            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock2D(
            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)
            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
          )
        )
        (downsamplers): ModuleList(
          (0): Downsample2D(
            (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          )
        )
      )
      (3): DownBlock2D(
        (resnets): ModuleList(
          (0-1): 2 x ResnetBlock2D(
            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)
            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
          )
        )
      )
    )
    (up_blocks): ModuleList(
      (0): ResidualUpBlock2D(
        (resnets): ModuleList(
          (0-2): 3 x ResidualResnetBlock2D(
            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)
            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (upsamplers): ModuleList(
          (0): Upsample2D(
            (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (1): ExtendedResidualCrossAttnUpBlock2D(
        (attentions): ModuleList(
          (0-2): 3 x ResidualTransformer2DModel(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): ResidualTransformerBlock(
                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (attn1): ResidualAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (attn2): ResidualAttention(
                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                  (to_k): Linear(in_features=768, out_features=1280, bias=False)
                  (to_v): Linear(in_features=768, out_features=1280, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=1280, out_features=1280, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
                (ff): FeedForward(
                  (net): ModuleList(
                    (0): GEGLU(
                      (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
                  )
                )
              )
            )
            (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (resnets): ModuleList(
          (0-1): 2 x ResidualResnetBlock2D(
            (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)
            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
          (2): ResidualResnetBlock2D(
            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)
            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (warpflows): ModuleList(
          (0-2): 3 x CustomSpatialTransformer(
            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
            (proj_in): Conv2d(1280, 512, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): CustomBasicTransformerBlock(
                (attn1): MemoryEfficientCrossAttentionWithMask(
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_k): Linear(in_features=512, out_features=512, bias=False)
                  (to_v): Linear(in_features=512, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=512, out_features=4096, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (attn2): MemoryEfficientCrossAttentionWithMask(
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_k): Linear(in_features=1280, out_features=512, bias=False)
                  (to_v): Linear(in_features=1280, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Conv2d(512, 1280, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (warpzc): ModuleList(
          (0-2): 3 x Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
        (upsamplers): ModuleList(
          (0): Upsample2D(
            (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (2): ExtendedResidualCrossAttnUpBlock2D(
        (attentions): ModuleList(
          (0-2): 3 x ResidualTransformer2DModel(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): ResidualTransformerBlock(
                (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (attn1): ResidualAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=640, out_features=640, bias=False)
                  (to_v): Linear(in_features=640, out_features=640, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (attn2): ResidualAttention(
                  (to_q): Linear(in_features=640, out_features=640, bias=False)
                  (to_k): Linear(in_features=768, out_features=640, bias=False)
                  (to_v): Linear(in_features=768, out_features=640, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=640, out_features=640, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)
                (ff): FeedForward(
                  (net): ModuleList(
                    (0): GEGLU(
                      (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)
                  )
                )
              )
            )
            (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (resnets): ModuleList(
          (0): ResidualResnetBlock2D(
            (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)
            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResidualResnetBlock2D(
            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)
            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))
          )
          (2): ResidualResnetBlock2D(
            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)
            (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (warpflows): ModuleList(
          (0-2): 3 x CustomSpatialTransformer(
            (norm): GroupNorm(32, 640, eps=1e-06, affine=True)
            (proj_in): Conv2d(640, 512, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): CustomBasicTransformerBlock(
                (attn1): MemoryEfficientCrossAttentionWithMask(
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_k): Linear(in_features=512, out_features=512, bias=False)
                  (to_v): Linear(in_features=512, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=512, out_features=4096, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (attn2): MemoryEfficientCrossAttentionWithMask(
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_k): Linear(in_features=640, out_features=512, bias=False)
                  (to_v): Linear(in_features=640, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Conv2d(512, 640, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (warpzc): ModuleList(
          (0-2): 3 x Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))
        )
        (upsamplers): ModuleList(
          (0): Upsample2D(
            (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (3): ExtendedResidualCrossAttnUpBlock2D(
        (attentions): ModuleList(
          (0-2): 3 x ResidualTransformer2DModel(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): ResidualTransformerBlock(
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (attn1): ResidualAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=320, out_features=320, bias=False)
                  (to_v): Linear(in_features=320, out_features=320, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (attn2): ResidualAttention(
                  (to_q): Linear(in_features=320, out_features=320, bias=False)
                  (to_k): Linear(in_features=768, out_features=320, bias=False)
                  (to_v): Linear(in_features=768, out_features=320, bias=False)
                  (to_out): ModuleList(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (ff): FeedForward(
                  (net): ModuleList(
                    (0): GEGLU(
                      (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)
                  )
                )
              )
            )
            (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (resnets): ModuleList(
          (0): ResidualResnetBlock2D(
            (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)
            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))
          )
          (1-2): 2 x ResidualResnetBlock2D(
            (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)
            (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)
            (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nonlinearity): SiLU()
            (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (warpflows): ModuleList(
          (0-2): 3 x CustomSpatialTransformer(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Conv2d(320, 512, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
              (0): CustomBasicTransformerBlock(
                (attn1): MemoryEfficientCrossAttentionWithMask(
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_k): Linear(in_features=512, out_features=512, bias=False)
                  (to_v): Linear(in_features=512, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (ff): FeedForward(
                  (net): Sequential(
                    (0): GEGLU(
                      (proj): Linear(in_features=512, out_features=4096, bias=True)
                    )
                    (1): Dropout(p=0.0, inplace=False)
                    (2): Linear(in_features=2048, out_features=512, bias=True)
                  )
                )
                (attn2): MemoryEfficientCrossAttentionWithMask(
                  (to_q): Linear(in_features=512, out_features=512, bias=False)
                  (to_k): Linear(in_features=320, out_features=512, bias=False)
                  (to_v): Linear(in_features=320, out_features=512, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=512, out_features=512, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
                (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              )
            )
            (proj_out): Conv2d(512, 320, kernel_size=(1, 1), stride=(1, 1))
          )
        )
        (warpzc): ModuleList(
          (0-2): 3 x Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
    (mid_block): UNetMidBlock2DCrossAttn(
      (attentions): ModuleList(
        (0): Transformer2DModel(
          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)
          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
          (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn1): Attention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=1280, out_features=1280, bias=False)
                (to_v): Linear(in_features=1280, out_features=1280, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (attn2): Attention(
                (to_q): Linear(in_features=1280, out_features=1280, bias=False)
                (to_k): Linear(in_features=768, out_features=1280, bias=False)
                (to_v): Linear(in_features=768, out_features=1280, bias=False)
                (to_out): ModuleList(
                  (0): Linear(in_features=1280, out_features=1280, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
              (ff): FeedForward(
                (net): ModuleList(
                  (0): GEGLU(
                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)
                  )
                  (1): Dropout(p=0.0, inplace=False)
                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)
                )
              )
            )
          )
          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (resnets): ModuleList(
        (0-1): 2 x ResnetBlock2D(
          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)
          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (nonlinearity): SiLU()
        )
      )
    )
    (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)
    (conv_act): SiLU()
    (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
)
model.conv_in.weight                                                            : False
model.conv_in.bias                                                              : False
model.time_embedding.linear_1.weight                                            : False
model.time_embedding.linear_1.bias                                              : False
model.time_embedding.linear_2.weight                                            : False
model.time_embedding.linear_2.bias                                              : False
model.down_blocks.0.attentions.0.norm.weight                                    : False
model.down_blocks.0.attentions.0.norm.bias                                      : False
model.down_blocks.0.attentions.0.proj_in.weight                                 : False
model.down_blocks.0.attentions.0.proj_in.bias                                   : False
model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight              : False
model.down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias                : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight         : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight         : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight         : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight     : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias       : False
model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight              : False
model.down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias                : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight         : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight         : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight         : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight     : False
model.down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias       : False
model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight              : False
model.down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias                : False
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight      : False
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias        : False
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight           : False
model.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias             : False
model.down_blocks.0.attentions.0.proj_out.weight                                : False
model.down_blocks.0.attentions.0.proj_out.bias                                  : False
model.down_blocks.0.attentions.1.norm.weight                                    : False
model.down_blocks.0.attentions.1.norm.bias                                      : False
model.down_blocks.0.attentions.1.proj_in.weight                                 : False
model.down_blocks.0.attentions.1.proj_in.bias                                   : False
model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight              : False
model.down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias                : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight         : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight         : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight         : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight     : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias       : False
model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight              : False
model.down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias                : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight         : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight         : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight         : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight     : False
model.down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias       : False
model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight              : False
model.down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias                : False
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight      : False
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias        : False
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight           : False
model.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias             : False
model.down_blocks.0.attentions.1.proj_out.weight                                : False
model.down_blocks.0.attentions.1.proj_out.bias                                  : False
model.down_blocks.0.resnets.0.norm1.weight                                      : False
model.down_blocks.0.resnets.0.norm1.bias                                        : False
model.down_blocks.0.resnets.0.conv1.weight                                      : False
model.down_blocks.0.resnets.0.conv1.bias                                        : False
model.down_blocks.0.resnets.0.time_emb_proj.weight                              : False
model.down_blocks.0.resnets.0.time_emb_proj.bias                                : False
model.down_blocks.0.resnets.0.norm2.weight                                      : False
model.down_blocks.0.resnets.0.norm2.bias                                        : False
model.down_blocks.0.resnets.0.conv2.weight                                      : False
model.down_blocks.0.resnets.0.conv2.bias                                        : False
model.down_blocks.0.resnets.1.norm1.weight                                      : False
model.down_blocks.0.resnets.1.norm1.bias                                        : False
model.down_blocks.0.resnets.1.conv1.weight                                      : False
model.down_blocks.0.resnets.1.conv1.bias                                        : False
model.down_blocks.0.resnets.1.time_emb_proj.weight                              : False
model.down_blocks.0.resnets.1.time_emb_proj.bias                                : False
model.down_blocks.0.resnets.1.norm2.weight                                      : False
model.down_blocks.0.resnets.1.norm2.bias                                        : False
model.down_blocks.0.resnets.1.conv2.weight                                      : False
model.down_blocks.0.resnets.1.conv2.bias                                        : False
model.down_blocks.0.downsamplers.0.conv.weight                                  : False
model.down_blocks.0.downsamplers.0.conv.bias                                    : False
model.down_blocks.1.attentions.0.norm.weight                                    : False
model.down_blocks.1.attentions.0.norm.bias                                      : False
model.down_blocks.1.attentions.0.proj_in.weight                                 : False
model.down_blocks.1.attentions.0.proj_in.bias                                   : False
model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight              : False
model.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias                : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight         : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight         : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight         : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight     : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias       : False
model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight              : False
model.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias                : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight         : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight         : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight         : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight     : False
model.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias       : False
model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight              : False
model.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias                : False
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight      : False
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias        : False
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight           : False
model.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias             : False
model.down_blocks.1.attentions.0.proj_out.weight                                : False
model.down_blocks.1.attentions.0.proj_out.bias                                  : False
model.down_blocks.1.attentions.1.norm.weight                                    : False
model.down_blocks.1.attentions.1.norm.bias                                      : False
model.down_blocks.1.attentions.1.proj_in.weight                                 : False
model.down_blocks.1.attentions.1.proj_in.bias                                   : False
model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight              : False
model.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias                : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight         : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight         : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight         : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight     : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias       : False
model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight              : False
model.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias                : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight         : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight         : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight         : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight     : False
model.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias       : False
model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight              : False
model.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias                : False
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight      : False
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias        : False
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight           : False
model.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias             : False
model.down_blocks.1.attentions.1.proj_out.weight                                : False
model.down_blocks.1.attentions.1.proj_out.bias                                  : False
model.down_blocks.1.resnets.0.norm1.weight                                      : False
model.down_blocks.1.resnets.0.norm1.bias                                        : False
model.down_blocks.1.resnets.0.conv1.weight                                      : False
model.down_blocks.1.resnets.0.conv1.bias                                        : False
model.down_blocks.1.resnets.0.time_emb_proj.weight                              : False
model.down_blocks.1.resnets.0.time_emb_proj.bias                                : False
model.down_blocks.1.resnets.0.norm2.weight                                      : False
model.down_blocks.1.resnets.0.norm2.bias                                        : False
model.down_blocks.1.resnets.0.conv2.weight                                      : False
model.down_blocks.1.resnets.0.conv2.bias                                        : False
model.down_blocks.1.resnets.0.conv_shortcut.weight                              : False
model.down_blocks.1.resnets.0.conv_shortcut.bias                                : False
model.down_blocks.1.resnets.1.norm1.weight                                      : False
model.down_blocks.1.resnets.1.norm1.bias                                        : False
model.down_blocks.1.resnets.1.conv1.weight                                      : False
model.down_blocks.1.resnets.1.conv1.bias                                        : False
model.down_blocks.1.resnets.1.time_emb_proj.weight                              : False
model.down_blocks.1.resnets.1.time_emb_proj.bias                                : False
model.down_blocks.1.resnets.1.norm2.weight                                      : False
model.down_blocks.1.resnets.1.norm2.bias                                        : False
model.down_blocks.1.resnets.1.conv2.weight                                      : False
model.down_blocks.1.resnets.1.conv2.bias                                        : False
model.down_blocks.1.downsamplers.0.conv.weight                                  : False
model.down_blocks.1.downsamplers.0.conv.bias                                    : False
model.down_blocks.2.attentions.0.norm.weight                                    : False
model.down_blocks.2.attentions.0.norm.bias                                      : False
model.down_blocks.2.attentions.0.proj_in.weight                                 : False
model.down_blocks.2.attentions.0.proj_in.bias                                   : False
model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight              : False
model.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias                : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight         : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight         : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight         : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight     : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias       : False
model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight              : False
model.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias                : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight         : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight         : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight         : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight     : False
model.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias       : False
model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight              : False
model.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias                : False
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight      : False
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias        : False
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight           : False
model.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias             : False
model.down_blocks.2.attentions.0.proj_out.weight                                : False
model.down_blocks.2.attentions.0.proj_out.bias                                  : False
model.down_blocks.2.attentions.1.norm.weight                                    : False
model.down_blocks.2.attentions.1.norm.bias                                      : False
model.down_blocks.2.attentions.1.proj_in.weight                                 : False
model.down_blocks.2.attentions.1.proj_in.bias                                   : False
model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight              : False
model.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias                : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight         : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight         : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight         : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight     : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias       : False
model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight              : False
model.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias                : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight         : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight         : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight         : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight     : False
model.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias       : False
model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight              : False
model.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias                : False
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight      : False
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias        : False
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight           : False
model.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias             : False
model.down_blocks.2.attentions.1.proj_out.weight                                : False
model.down_blocks.2.attentions.1.proj_out.bias                                  : False
model.down_blocks.2.resnets.0.norm1.weight                                      : False
model.down_blocks.2.resnets.0.norm1.bias                                        : False
model.down_blocks.2.resnets.0.conv1.weight                                      : False
model.down_blocks.2.resnets.0.conv1.bias                                        : False
model.down_blocks.2.resnets.0.time_emb_proj.weight                              : False
model.down_blocks.2.resnets.0.time_emb_proj.bias                                : False
model.down_blocks.2.resnets.0.norm2.weight                                      : False
model.down_blocks.2.resnets.0.norm2.bias                                        : False
model.down_blocks.2.resnets.0.conv2.weight                                      : False
model.down_blocks.2.resnets.0.conv2.bias                                        : False
model.down_blocks.2.resnets.0.conv_shortcut.weight                              : False
model.down_blocks.2.resnets.0.conv_shortcut.bias                                : False
model.down_blocks.2.resnets.1.norm1.weight                                      : False
model.down_blocks.2.resnets.1.norm1.bias                                        : False
model.down_blocks.2.resnets.1.conv1.weight                                      : False
model.down_blocks.2.resnets.1.conv1.bias                                        : False
model.down_blocks.2.resnets.1.time_emb_proj.weight                              : False
model.down_blocks.2.resnets.1.time_emb_proj.bias                                : False
model.down_blocks.2.resnets.1.norm2.weight                                      : False
model.down_blocks.2.resnets.1.norm2.bias                                        : False
model.down_blocks.2.resnets.1.conv2.weight                                      : False
model.down_blocks.2.resnets.1.conv2.bias                                        : False
model.down_blocks.2.downsamplers.0.conv.weight                                  : False
model.down_blocks.2.downsamplers.0.conv.bias                                    : False
model.down_blocks.3.resnets.0.norm1.weight                                      : False
model.down_blocks.3.resnets.0.norm1.bias                                        : False
model.down_blocks.3.resnets.0.conv1.weight                                      : False
model.down_blocks.3.resnets.0.conv1.bias                                        : False
model.down_blocks.3.resnets.0.time_emb_proj.weight                              : False
model.down_blocks.3.resnets.0.time_emb_proj.bias                                : False
model.down_blocks.3.resnets.0.norm2.weight                                      : False
model.down_blocks.3.resnets.0.norm2.bias                                        : False
model.down_blocks.3.resnets.0.conv2.weight                                      : False
model.down_blocks.3.resnets.0.conv2.bias                                        : False
model.down_blocks.3.resnets.1.norm1.weight                                      : False
model.down_blocks.3.resnets.1.norm1.bias                                        : False
model.down_blocks.3.resnets.1.conv1.weight                                      : False
model.down_blocks.3.resnets.1.conv1.bias                                        : False
model.down_blocks.3.resnets.1.time_emb_proj.weight                              : False
model.down_blocks.3.resnets.1.time_emb_proj.bias                                : False
model.down_blocks.3.resnets.1.norm2.weight                                      : False
model.down_blocks.3.resnets.1.norm2.bias                                        : False
model.down_blocks.3.resnets.1.conv2.weight                                      : False
model.down_blocks.3.resnets.1.conv2.bias                                        : False
model.up_blocks.0.resnets.0.norm1.weight                                        : False
model.up_blocks.0.resnets.0.norm1.bias                                          : False
model.up_blocks.0.resnets.0.conv1.weight                                        : False
model.up_blocks.0.resnets.0.conv1.bias                                          : False
model.up_blocks.0.resnets.0.time_emb_proj.weight                                : False
model.up_blocks.0.resnets.0.time_emb_proj.bias                                  : False
model.up_blocks.0.resnets.0.norm2.weight                                        : False
model.up_blocks.0.resnets.0.norm2.bias                                          : False
model.up_blocks.0.resnets.0.conv2.weight                                        : False
model.up_blocks.0.resnets.0.conv2.bias                                          : False
model.up_blocks.0.resnets.0.conv_shortcut.weight                                : False
model.up_blocks.0.resnets.0.conv_shortcut.bias                                  : False
model.up_blocks.0.resnets.1.norm1.weight                                        : False
model.up_blocks.0.resnets.1.norm1.bias                                          : False
model.up_blocks.0.resnets.1.conv1.weight                                        : False
model.up_blocks.0.resnets.1.conv1.bias                                          : False
model.up_blocks.0.resnets.1.time_emb_proj.weight                                : False
model.up_blocks.0.resnets.1.time_emb_proj.bias                                  : False
model.up_blocks.0.resnets.1.norm2.weight                                        : False
model.up_blocks.0.resnets.1.norm2.bias                                          : False
model.up_blocks.0.resnets.1.conv2.weight                                        : False
model.up_blocks.0.resnets.1.conv2.bias                                          : False
model.up_blocks.0.resnets.1.conv_shortcut.weight                                : False
model.up_blocks.0.resnets.1.conv_shortcut.bias                                  : False
model.up_blocks.0.resnets.2.norm1.weight                                        : False
model.up_blocks.0.resnets.2.norm1.bias                                          : False
model.up_blocks.0.resnets.2.conv1.weight                                        : False
model.up_blocks.0.resnets.2.conv1.bias                                          : False
model.up_blocks.0.resnets.2.time_emb_proj.weight                                : False
model.up_blocks.0.resnets.2.time_emb_proj.bias                                  : False
model.up_blocks.0.resnets.2.norm2.weight                                        : False
model.up_blocks.0.resnets.2.norm2.bias                                          : False
model.up_blocks.0.resnets.2.conv2.weight                                        : False
model.up_blocks.0.resnets.2.conv2.bias                                          : False
model.up_blocks.0.resnets.2.conv_shortcut.weight                                : False
model.up_blocks.0.resnets.2.conv_shortcut.bias                                  : False
model.up_blocks.0.upsamplers.0.conv.weight                                      : False
model.up_blocks.0.upsamplers.0.conv.bias                                        : False
model.up_blocks.1.attentions.0.norm.weight                                      : False
model.up_blocks.1.attentions.0.norm.bias                                        : False
model.up_blocks.1.attentions.0.proj_in.weight                                   : False
model.up_blocks.1.attentions.0.proj_in.bias                                     : False
model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight                : False
model.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight                : False
model.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight                : False
model.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.1.attentions.0.proj_out.weight                                  : False
model.up_blocks.1.attentions.0.proj_out.bias                                    : False
model.up_blocks.1.attentions.1.norm.weight                                      : False
model.up_blocks.1.attentions.1.norm.bias                                        : False
model.up_blocks.1.attentions.1.proj_in.weight                                   : False
model.up_blocks.1.attentions.1.proj_in.bias                                     : False
model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight                : False
model.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight                : False
model.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight                : False
model.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.1.attentions.1.proj_out.weight                                  : False
model.up_blocks.1.attentions.1.proj_out.bias                                    : False
model.up_blocks.1.attentions.2.norm.weight                                      : False
model.up_blocks.1.attentions.2.norm.bias                                        : False
model.up_blocks.1.attentions.2.proj_in.weight                                   : False
model.up_blocks.1.attentions.2.proj_in.bias                                     : False
model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight                : False
model.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight                : False
model.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight                : False
model.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.1.attentions.2.proj_out.weight                                  : False
model.up_blocks.1.attentions.2.proj_out.bias                                    : False
model.up_blocks.1.resnets.0.norm1.weight                                        : False
model.up_blocks.1.resnets.0.norm1.bias                                          : False
model.up_blocks.1.resnets.0.conv1.weight                                        : False
model.up_blocks.1.resnets.0.conv1.bias                                          : False
model.up_blocks.1.resnets.0.time_emb_proj.weight                                : False
model.up_blocks.1.resnets.0.time_emb_proj.bias                                  : False
model.up_blocks.1.resnets.0.norm2.weight                                        : False
model.up_blocks.1.resnets.0.norm2.bias                                          : False
model.up_blocks.1.resnets.0.conv2.weight                                        : False
model.up_blocks.1.resnets.0.conv2.bias                                          : False
model.up_blocks.1.resnets.0.conv_shortcut.weight                                : False
model.up_blocks.1.resnets.0.conv_shortcut.bias                                  : False
model.up_blocks.1.resnets.1.norm1.weight                                        : False
model.up_blocks.1.resnets.1.norm1.bias                                          : False
model.up_blocks.1.resnets.1.conv1.weight                                        : False
model.up_blocks.1.resnets.1.conv1.bias                                          : False
model.up_blocks.1.resnets.1.time_emb_proj.weight                                : False
model.up_blocks.1.resnets.1.time_emb_proj.bias                                  : False
model.up_blocks.1.resnets.1.norm2.weight                                        : False
model.up_blocks.1.resnets.1.norm2.bias                                          : False
model.up_blocks.1.resnets.1.conv2.weight                                        : False
model.up_blocks.1.resnets.1.conv2.bias                                          : False
model.up_blocks.1.resnets.1.conv_shortcut.weight                                : False
model.up_blocks.1.resnets.1.conv_shortcut.bias                                  : False
model.up_blocks.1.resnets.2.norm1.weight                                        : False
model.up_blocks.1.resnets.2.norm1.bias                                          : False
model.up_blocks.1.resnets.2.conv1.weight                                        : False
model.up_blocks.1.resnets.2.conv1.bias                                          : False
model.up_blocks.1.resnets.2.time_emb_proj.weight                                : False
model.up_blocks.1.resnets.2.time_emb_proj.bias                                  : False
model.up_blocks.1.resnets.2.norm2.weight                                        : False
model.up_blocks.1.resnets.2.norm2.bias                                          : False
model.up_blocks.1.resnets.2.conv2.weight                                        : False
model.up_blocks.1.resnets.2.conv2.bias                                          : False
model.up_blocks.1.resnets.2.conv_shortcut.weight                                : False
model.up_blocks.1.resnets.2.conv_shortcut.bias                                  : False
model.up_blocks.1.warpflows.0.norm.weight                                       : True
model.up_blocks.1.warpflows.0.norm.bias                                         : True
model.up_blocks.1.warpflows.0.proj_in.weight                                    : True
model.up_blocks.1.warpflows.0.proj_in.bias                                      : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.1.warpflows.0.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.1.warpflows.0.proj_out.weight                                   : True
model.up_blocks.1.warpflows.0.proj_out.bias                                     : True
model.up_blocks.1.warpflows.1.norm.weight                                       : True
model.up_blocks.1.warpflows.1.norm.bias                                         : True
model.up_blocks.1.warpflows.1.proj_in.weight                                    : True
model.up_blocks.1.warpflows.1.proj_in.bias                                      : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.1.warpflows.1.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.1.warpflows.1.proj_out.weight                                   : True
model.up_blocks.1.warpflows.1.proj_out.bias                                     : True
model.up_blocks.1.warpflows.2.norm.weight                                       : True
model.up_blocks.1.warpflows.2.norm.bias                                         : True
model.up_blocks.1.warpflows.2.proj_in.weight                                    : True
model.up_blocks.1.warpflows.2.proj_in.bias                                      : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.1.warpflows.2.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.1.warpflows.2.proj_out.weight                                   : True
model.up_blocks.1.warpflows.2.proj_out.bias                                     : True
model.up_blocks.1.warpzc.0.weight                                               : True
model.up_blocks.1.warpzc.0.bias                                                 : True
model.up_blocks.1.warpzc.1.weight                                               : True
model.up_blocks.1.warpzc.1.bias                                                 : True
model.up_blocks.1.warpzc.2.weight                                               : True
model.up_blocks.1.warpzc.2.bias                                                 : True
model.up_blocks.1.upsamplers.0.conv.weight                                      : False
model.up_blocks.1.upsamplers.0.conv.bias                                        : False
model.up_blocks.2.attentions.0.norm.weight                                      : False
model.up_blocks.2.attentions.0.norm.bias                                        : False
model.up_blocks.2.attentions.0.proj_in.weight                                   : False
model.up_blocks.2.attentions.0.proj_in.bias                                     : False
model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight                : False
model.up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight                : False
model.up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight                : False
model.up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.2.attentions.0.proj_out.weight                                  : False
model.up_blocks.2.attentions.0.proj_out.bias                                    : False
model.up_blocks.2.attentions.1.norm.weight                                      : False
model.up_blocks.2.attentions.1.norm.bias                                        : False
model.up_blocks.2.attentions.1.proj_in.weight                                   : False
model.up_blocks.2.attentions.1.proj_in.bias                                     : False
model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight                : False
model.up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight                : False
model.up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight                : False
model.up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.2.attentions.1.proj_out.weight                                  : False
model.up_blocks.2.attentions.1.proj_out.bias                                    : False
model.up_blocks.2.attentions.2.norm.weight                                      : False
model.up_blocks.2.attentions.2.norm.bias                                        : False
model.up_blocks.2.attentions.2.proj_in.weight                                   : False
model.up_blocks.2.attentions.2.proj_in.bias                                     : False
model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight                : False
model.up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight                : False
model.up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight                : False
model.up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.2.attentions.2.proj_out.weight                                  : False
model.up_blocks.2.attentions.2.proj_out.bias                                    : False
model.up_blocks.2.resnets.0.norm1.weight                                        : False
model.up_blocks.2.resnets.0.norm1.bias                                          : False
model.up_blocks.2.resnets.0.conv1.weight                                        : False
model.up_blocks.2.resnets.0.conv1.bias                                          : False
model.up_blocks.2.resnets.0.time_emb_proj.weight                                : False
model.up_blocks.2.resnets.0.time_emb_proj.bias                                  : False
model.up_blocks.2.resnets.0.norm2.weight                                        : False
model.up_blocks.2.resnets.0.norm2.bias                                          : False
model.up_blocks.2.resnets.0.conv2.weight                                        : False
model.up_blocks.2.resnets.0.conv2.bias                                          : False
model.up_blocks.2.resnets.0.conv_shortcut.weight                                : False
model.up_blocks.2.resnets.0.conv_shortcut.bias                                  : False
model.up_blocks.2.resnets.1.norm1.weight                                        : False
model.up_blocks.2.resnets.1.norm1.bias                                          : False
model.up_blocks.2.resnets.1.conv1.weight                                        : False
model.up_blocks.2.resnets.1.conv1.bias                                          : False
model.up_blocks.2.resnets.1.time_emb_proj.weight                                : False
model.up_blocks.2.resnets.1.time_emb_proj.bias                                  : False
model.up_blocks.2.resnets.1.norm2.weight                                        : False
model.up_blocks.2.resnets.1.norm2.bias                                          : False
model.up_blocks.2.resnets.1.conv2.weight                                        : False
model.up_blocks.2.resnets.1.conv2.bias                                          : False
model.up_blocks.2.resnets.1.conv_shortcut.weight                                : False
model.up_blocks.2.resnets.1.conv_shortcut.bias                                  : False
model.up_blocks.2.resnets.2.norm1.weight                                        : False
model.up_blocks.2.resnets.2.norm1.bias                                          : False
model.up_blocks.2.resnets.2.conv1.weight                                        : False
model.up_blocks.2.resnets.2.conv1.bias                                          : False
model.up_blocks.2.resnets.2.time_emb_proj.weight                                : False
model.up_blocks.2.resnets.2.time_emb_proj.bias                                  : False
model.up_blocks.2.resnets.2.norm2.weight                                        : False
model.up_blocks.2.resnets.2.norm2.bias                                          : False
model.up_blocks.2.resnets.2.conv2.weight                                        : False
model.up_blocks.2.resnets.2.conv2.bias                                          : False
model.up_blocks.2.resnets.2.conv_shortcut.weight                                : False
model.up_blocks.2.resnets.2.conv_shortcut.bias                                  : False
model.up_blocks.2.warpflows.0.norm.weight                                       : True
model.up_blocks.2.warpflows.0.norm.bias                                         : True
model.up_blocks.2.warpflows.0.proj_in.weight                                    : True
model.up_blocks.2.warpflows.0.proj_in.bias                                      : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.2.warpflows.0.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.2.warpflows.0.proj_out.weight                                   : True
model.up_blocks.2.warpflows.0.proj_out.bias                                     : True
model.up_blocks.2.warpflows.1.norm.weight                                       : True
model.up_blocks.2.warpflows.1.norm.bias                                         : True
model.up_blocks.2.warpflows.1.proj_in.weight                                    : True
model.up_blocks.2.warpflows.1.proj_in.bias                                      : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.2.warpflows.1.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.2.warpflows.1.proj_out.weight                                   : True
model.up_blocks.2.warpflows.1.proj_out.bias                                     : True
model.up_blocks.2.warpflows.2.norm.weight                                       : True
model.up_blocks.2.warpflows.2.norm.bias                                         : True
model.up_blocks.2.warpflows.2.proj_in.weight                                    : True
model.up_blocks.2.warpflows.2.proj_in.bias                                      : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.2.warpflows.2.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.2.warpflows.2.proj_out.weight                                   : True
model.up_blocks.2.warpflows.2.proj_out.bias                                     : True
model.up_blocks.2.warpzc.0.weight                                               : True
model.up_blocks.2.warpzc.0.bias                                                 : True
model.up_blocks.2.warpzc.1.weight                                               : True
model.up_blocks.2.warpzc.1.bias                                                 : True
model.up_blocks.2.warpzc.2.weight                                               : True
model.up_blocks.2.warpzc.2.bias                                                 : True
model.up_blocks.2.upsamplers.0.conv.weight                                      : False
model.up_blocks.2.upsamplers.0.conv.bias                                        : False
model.up_blocks.3.attentions.0.norm.weight                                      : False
model.up_blocks.3.attentions.0.norm.bias                                        : False
model.up_blocks.3.attentions.0.proj_in.weight                                   : False
model.up_blocks.3.attentions.0.proj_in.bias                                     : False
model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight                : False
model.up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight                : False
model.up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight                : False
model.up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.3.attentions.0.proj_out.weight                                  : False
model.up_blocks.3.attentions.0.proj_out.bias                                    : False
model.up_blocks.3.attentions.1.norm.weight                                      : False
model.up_blocks.3.attentions.1.norm.bias                                        : False
model.up_blocks.3.attentions.1.proj_in.weight                                   : False
model.up_blocks.3.attentions.1.proj_in.bias                                     : False
model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight                : False
model.up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight                : False
model.up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight                : False
model.up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.3.attentions.1.proj_out.weight                                  : False
model.up_blocks.3.attentions.1.proj_out.bias                                    : False
model.up_blocks.3.attentions.2.norm.weight                                      : False
model.up_blocks.3.attentions.2.norm.bias                                        : False
model.up_blocks.3.attentions.2.proj_in.weight                                   : False
model.up_blocks.3.attentions.2.proj_in.bias                                     : False
model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight                : False
model.up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias                  : False
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight           : False
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight           : False
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight           : False
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight       : False
model.up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias         : False
model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight                : False
model.up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias                  : False
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight           : False
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight           : True
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight           : True
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight       : False
model.up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias         : False
model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight                : False
model.up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias                  : False
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight        : False
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias          : False
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight             : False
model.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias               : False
model.up_blocks.3.attentions.2.proj_out.weight                                  : False
model.up_blocks.3.attentions.2.proj_out.bias                                    : False
model.up_blocks.3.resnets.0.norm1.weight                                        : False
model.up_blocks.3.resnets.0.norm1.bias                                          : False
model.up_blocks.3.resnets.0.conv1.weight                                        : False
model.up_blocks.3.resnets.0.conv1.bias                                          : False
model.up_blocks.3.resnets.0.time_emb_proj.weight                                : False
model.up_blocks.3.resnets.0.time_emb_proj.bias                                  : False
model.up_blocks.3.resnets.0.norm2.weight                                        : False
model.up_blocks.3.resnets.0.norm2.bias                                          : False
model.up_blocks.3.resnets.0.conv2.weight                                        : False
model.up_blocks.3.resnets.0.conv2.bias                                          : False
model.up_blocks.3.resnets.0.conv_shortcut.weight                                : False
model.up_blocks.3.resnets.0.conv_shortcut.bias                                  : False
model.up_blocks.3.resnets.1.norm1.weight                                        : False
model.up_blocks.3.resnets.1.norm1.bias                                          : False
model.up_blocks.3.resnets.1.conv1.weight                                        : False
model.up_blocks.3.resnets.1.conv1.bias                                          : False
model.up_blocks.3.resnets.1.time_emb_proj.weight                                : False
model.up_blocks.3.resnets.1.time_emb_proj.bias                                  : False
model.up_blocks.3.resnets.1.norm2.weight                                        : False
model.up_blocks.3.resnets.1.norm2.bias                                          : False
model.up_blocks.3.resnets.1.conv2.weight                                        : False
model.up_blocks.3.resnets.1.conv2.bias                                          : False
model.up_blocks.3.resnets.1.conv_shortcut.weight                                : False
model.up_blocks.3.resnets.1.conv_shortcut.bias                                  : False
model.up_blocks.3.resnets.2.norm1.weight                                        : False
model.up_blocks.3.resnets.2.norm1.bias                                          : False
model.up_blocks.3.resnets.2.conv1.weight                                        : False
model.up_blocks.3.resnets.2.conv1.bias                                          : False
model.up_blocks.3.resnets.2.time_emb_proj.weight                                : False
model.up_blocks.3.resnets.2.time_emb_proj.bias                                  : False
model.up_blocks.3.resnets.2.norm2.weight                                        : False
model.up_blocks.3.resnets.2.norm2.bias                                          : False
model.up_blocks.3.resnets.2.conv2.weight                                        : False
model.up_blocks.3.resnets.2.conv2.bias                                          : False
model.up_blocks.3.resnets.2.conv_shortcut.weight                                : False
model.up_blocks.3.resnets.2.conv_shortcut.bias                                  : False
model.up_blocks.3.warpflows.0.norm.weight                                       : True
model.up_blocks.3.warpflows.0.norm.bias                                         : True
model.up_blocks.3.warpflows.0.proj_in.weight                                    : True
model.up_blocks.3.warpflows.0.proj_in.bias                                      : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.3.warpflows.0.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.3.warpflows.0.proj_out.weight                                   : True
model.up_blocks.3.warpflows.0.proj_out.bias                                     : True
model.up_blocks.3.warpflows.1.norm.weight                                       : True
model.up_blocks.3.warpflows.1.norm.bias                                         : True
model.up_blocks.3.warpflows.1.proj_in.weight                                    : True
model.up_blocks.3.warpflows.1.proj_in.bias                                      : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.3.warpflows.1.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.3.warpflows.1.proj_out.weight                                   : True
model.up_blocks.3.warpflows.1.proj_out.bias                                     : True
model.up_blocks.3.warpflows.2.norm.weight                                       : True
model.up_blocks.3.warpflows.2.norm.bias                                         : True
model.up_blocks.3.warpflows.2.proj_in.weight                                    : True
model.up_blocks.3.warpflows.2.proj_in.bias                                      : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_q.weight            : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_k.weight            : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_v.weight            : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.weight        : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn1.to_out.0.bias          : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.weight         : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.0.proj.bias           : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.weight              : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.ff.net.2.bias                : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_q.weight            : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_k.weight            : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_v.weight            : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.weight        : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.attn2.to_out.0.bias          : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.weight                 : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.norm1.bias                   : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.weight                 : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.norm2.bias                   : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.weight                 : True
model.up_blocks.3.warpflows.2.transformer_blocks.0.norm3.bias                   : True
model.up_blocks.3.warpflows.2.proj_out.weight                                   : True
model.up_blocks.3.warpflows.2.proj_out.bias                                     : True
model.up_blocks.3.warpzc.0.weight                                               : True
model.up_blocks.3.warpzc.0.bias                                                 : True
model.up_blocks.3.warpzc.1.weight                                               : True
model.up_blocks.3.warpzc.1.bias                                                 : True
model.up_blocks.3.warpzc.2.weight                                               : True
model.up_blocks.3.warpzc.2.bias                                                 : True
model.mid_block.attentions.0.norm.weight                                        : False
model.mid_block.attentions.0.norm.bias                                          : False
model.mid_block.attentions.0.proj_in.weight                                     : False
model.mid_block.attentions.0.proj_in.bias                                       : False
model.mid_block.attentions.0.transformer_blocks.0.norm1.weight                  : False
model.mid_block.attentions.0.transformer_blocks.0.norm1.bias                    : False
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight             : False
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight             : False
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight             : False
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight         : False
model.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias           : False
model.mid_block.attentions.0.transformer_blocks.0.norm2.weight                  : False
model.mid_block.attentions.0.transformer_blocks.0.norm2.bias                    : False
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight             : False
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight             : False
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight             : False
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight         : False
model.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias           : False
model.mid_block.attentions.0.transformer_blocks.0.norm3.weight                  : False
model.mid_block.attentions.0.transformer_blocks.0.norm3.bias                    : False
model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight          : False
model.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias            : False
model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight               : False
model.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias                 : False
model.mid_block.attentions.0.proj_out.weight                                    : False
model.mid_block.attentions.0.proj_out.bias                                      : False
model.mid_block.resnets.0.norm1.weight                                          : False
model.mid_block.resnets.0.norm1.bias                                            : False
model.mid_block.resnets.0.conv1.weight                                          : False
model.mid_block.resnets.0.conv1.bias                                            : False
model.mid_block.resnets.0.time_emb_proj.weight                                  : False
model.mid_block.resnets.0.time_emb_proj.bias                                    : False
model.mid_block.resnets.0.norm2.weight                                          : False
model.mid_block.resnets.0.norm2.bias                                            : False
model.mid_block.resnets.0.conv2.weight                                          : False
model.mid_block.resnets.0.conv2.bias                                            : False
model.mid_block.resnets.1.norm1.weight                                          : False
model.mid_block.resnets.1.norm1.bias                                            : False
model.mid_block.resnets.1.conv1.weight                                          : False
model.mid_block.resnets.1.conv1.bias                                            : False
model.mid_block.resnets.1.time_emb_proj.weight                                  : False
model.mid_block.resnets.1.time_emb_proj.bias                                    : False
model.mid_block.resnets.1.norm2.weight                                          : False
model.mid_block.resnets.1.norm2.bias                                            : False
model.mid_block.resnets.1.conv2.weight                                          : False
model.mid_block.resnets.1.conv2.bias                                            : False
model.conv_norm_out.weight                                                      : False
model.conv_norm_out.bias                                                        : False
model.conv_out.weight                                                           : False
model.conv_out.bias                                                             : False
